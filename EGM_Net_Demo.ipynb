{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a590fe",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678d1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/QuocKhanhLuong/FourierNetwork.git\n",
    "%cd FourierNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f537f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567d48f",
   "metadata": {},
   "source": [
    "## 2. Import Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our models\n",
    "from monogenic import EnergyMap, MonogenicSignal, BoundaryDetector\n",
    "from gabor_implicit import GaborBasis, GaborNet, ImplicitSegmentationHead\n",
    "from egm_net import EGMNet, EGMNetLite\n",
    "from spectral_mamba import SpectralVMUNet\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1eae7",
   "metadata": {},
   "source": [
    "## üè• REAL DATA TRAINING (Synapse Multi-Organ Dataset)\n",
    "\n",
    "This section downloads and trains on the **Synapse Multi-Organ CT Dataset** - a standard benchmark for medical image segmentation.\n",
    "\n",
    "**Dataset Info:**\n",
    "- 30 abdominal CT scans with 3779 axial contrast-enhanced clinical CT images\n",
    "- 8 abdominal organs: Aorta, Gallbladder, Spleen, Left Kidney, Right Kidney, Liver, Pancreas, Stomach\n",
    "- Training: 18 cases, Testing: 12 cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03af621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Synapse dataset from Google Drive or alternative sources\n",
    "import os\n",
    "import zipfile\n",
    "from google.colab import drive\n",
    "\n",
    "# Option 1: Mount Google Drive (if you have the dataset there)\n",
    "# drive.mount('/content/drive')\n",
    "# DATA_PATH = '/content/drive/MyDrive/Synapse'\n",
    "\n",
    "# Option 2: Download from public source (preprocessed npz format)\n",
    "# Note: You may need to request access from https://www.synapse.org/\n",
    "# Here we provide a preprocessed version structure\n",
    "\n",
    "# Create data directory\n",
    "os.makedirs('data/Synapse', exist_ok=True)\n",
    "\n",
    "print(\"\"\"\n",
    "üì• DATASET DOWNLOAD OPTIONS:\n",
    "\n",
    "Option 1: Download preprocessed Synapse dataset\n",
    "-------------------------------------------------\n",
    "!gdown --id <your_google_drive_file_id> -O data/synapse.zip\n",
    "!unzip -q data/synapse.zip -d data/\n",
    "\n",
    "Option 2: Use your own dataset\n",
    "-------------------------------\n",
    "Upload your data to: data/Synapse/\n",
    "Structure:\n",
    "  data/Synapse/\n",
    "    ‚îú‚îÄ‚îÄ train_npz/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ case0001_slice001.npz\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ case0001_slice002.npz\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "    ‚îî‚îÄ‚îÄ test_vol_h5/\n",
    "        ‚îú‚îÄ‚îÄ case0001.npy.h5\n",
    "        ‚îî‚îÄ‚îÄ ...\n",
    "\n",
    "Option 3: Use ISIC Skin Lesion dataset (easier to download)\n",
    "------------------------------------------------------------\n",
    "We'll download ISIC 2018 dataset for demonstration.\n",
    "\"\"\")\n",
    "\n",
    "USE_ISIC = True  # Set to False if you have Synapse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ISIC 2018 Skin Lesion Dataset (easy to access)\n",
    "if USE_ISIC:\n",
    "    print(\"üì• Downloading ISIC 2018 Dataset...\")\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs('data/ISIC2018/images', exist_ok=True)\n",
    "    os.makedirs('data/ISIC2018/masks', exist_ok=True)\n",
    "    \n",
    "    # Download training images and masks\n",
    "    !pip install -q gdown\n",
    "    \n",
    "    # ISIC 2018 Training Data (subset for demo - 500 images)\n",
    "    # Full dataset: https://challenge.isic-archive.com/data/\n",
    "    !gdown --fuzzy \"https://drive.google.com/file/d/1E2xHt5jqXLxWCjWwIZ9lNj9lE4kZ8vIz/view?usp=sharing\" -O data/ISIC2018_subset.zip 2>/dev/null || echo \"Using backup download...\"\n",
    "    \n",
    "    # If gdown fails, use wget from alternative source\n",
    "    if not os.path.exists('data/ISIC2018_subset.zip'):\n",
    "        print(\"Downloading from alternative source...\")\n",
    "        # Create synthetic data if download fails (for demo purposes)\n",
    "        print(\"‚ö†Ô∏è Could not download dataset. Creating synthetic medical data for demo...\")\n",
    "        CREATE_SYNTHETIC = True\n",
    "    else:\n",
    "        !unzip -q data/ISIC2018_subset.zip -d data/ISIC2018/\n",
    "        CREATE_SYNTHETIC = False\n",
    "else:\n",
    "    CREATE_SYNTHETIC = False\n",
    "\n",
    "print(\"‚úÖ Data setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e088d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic medical imaging dataset (fallback if download fails)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "class SyntheticMedicalDataset(Dataset):\n",
    "    \"\"\"Generate synthetic medical-like images with organs for demo.\"\"\"\n",
    "    def __init__(self, num_samples=500, img_size=256, num_classes=2, split='train'):\n",
    "        self.num_samples = num_samples\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "        self.split = split\n",
    "        np.random.seed(42 if split == 'train' else 123)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        size = self.img_size\n",
    "        img = np.zeros((size, size), dtype=np.float32)\n",
    "        mask = np.zeros((size, size), dtype=np.int64)\n",
    "        \n",
    "        # Background texture\n",
    "        img += 0.1 * np.random.randn(size, size)\n",
    "        \n",
    "        # Add random elliptical organs\n",
    "        num_organs = np.random.randint(1, 4)\n",
    "        y, x = np.meshgrid(np.arange(size), np.arange(size), indexing='ij')\n",
    "        \n",
    "        for i in range(num_organs):\n",
    "            # Random ellipse parameters\n",
    "            cx = np.random.randint(size//4, 3*size//4)\n",
    "            cy = np.random.randint(size//4, 3*size//4)\n",
    "            rx = np.random.randint(size//8, size//3)\n",
    "            ry = np.random.randint(size//8, size//3)\n",
    "            angle = np.random.rand() * np.pi\n",
    "            \n",
    "            # Rotated ellipse\n",
    "            cos_a, sin_a = np.cos(angle), np.sin(angle)\n",
    "            x_rot = cos_a * (x - cx) + sin_a * (y - cy)\n",
    "            y_rot = -sin_a * (x - cx) + cos_a * (y - cy)\n",
    "            ellipse_mask = (x_rot/rx)**2 + (y_rot/ry)**2 < 1\n",
    "            \n",
    "            intensity = 0.3 + 0.5 * np.random.rand()\n",
    "            img[ellipse_mask] = intensity\n",
    "            mask[ellipse_mask] = 1  # Foreground class\n",
    "        \n",
    "        # Add noise\n",
    "        img = img + 0.05 * np.random.randn(size, size)\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        img = torch.from_numpy(img).unsqueeze(0).float()\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        # Data augmentation for training\n",
    "        if self.split == 'train' and np.random.rand() > 0.5:\n",
    "            img = torch.flip(img, dims=[2])\n",
    "            mask = torch.flip(mask, dims=[1])\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "\n",
    "class ISICDataset(Dataset):\n",
    "    \"\"\"ISIC Skin Lesion Dataset.\"\"\"\n",
    "    def __init__(self, data_dir, img_size=256, split='train'):\n",
    "        self.img_size = img_size\n",
    "        self.split = split\n",
    "        \n",
    "        img_dir = os.path.join(data_dir, 'images')\n",
    "        mask_dir = os.path.join(data_dir, 'masks')\n",
    "        \n",
    "        self.images = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))\n",
    "        self.masks = sorted(glob.glob(os.path.join(mask_dir, '*.png')))\n",
    "        \n",
    "        # Split 80/20\n",
    "        split_idx = int(0.8 * len(self.images))\n",
    "        if split == 'train':\n",
    "            self.images = self.images[:split_idx]\n",
    "            self.masks = self.masks[:split_idx]\n",
    "        else:\n",
    "            self.images = self.images[split_idx:]\n",
    "            self.masks = self.masks[split_idx:]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img = Image.open(self.images[idx]).convert('L')  # Grayscale\n",
    "        img = img.resize((self.img_size, self.img_size), Image.BILINEAR)\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        # Load mask\n",
    "        mask = Image.open(self.masks[idx]).convert('L')\n",
    "        mask = mask.resize((self.img_size, self.img_size), Image.NEAREST)\n",
    "        mask = (np.array(mask) > 127).astype(np.int64)\n",
    "        \n",
    "        img = torch.from_numpy(img).unsqueeze(0).float()\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "\n",
    "class SynapseDataset(Dataset):\n",
    "    \"\"\"Synapse Multi-Organ Dataset (NPZ format).\"\"\"\n",
    "    def __init__(self, data_dir, img_size=256, split='train'):\n",
    "        self.img_size = img_size\n",
    "        self.split = split\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.data_files = sorted(glob.glob(os.path.join(data_dir, 'train_npz', '*.npz')))\n",
    "        else:\n",
    "            self.data_files = sorted(glob.glob(os.path.join(data_dir, 'test_npz', '*.npz')))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.data_files[idx])\n",
    "        img = data['image']\n",
    "        mask = data['label']\n",
    "        \n",
    "        # Resize if needed\n",
    "        if img.shape != (self.img_size, self.img_size):\n",
    "            img = np.array(Image.fromarray(img).resize((self.img_size, self.img_size), Image.BILINEAR))\n",
    "            mask = np.array(Image.fromarray(mask.astype(np.uint8)).resize((self.img_size, self.img_size), Image.NEAREST))\n",
    "        \n",
    "        img = torch.from_numpy(img).unsqueeze(0).float()\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "IMG_SIZE = 256\n",
    "NUM_CLASSES = 2  # Background + Foreground (or 9 for Synapse)\n",
    "\n",
    "if 'CREATE_SYNTHETIC' in dir() and CREATE_SYNTHETIC:\n",
    "    print(\"üìä Creating synthetic medical dataset...\")\n",
    "    train_dataset = SyntheticMedicalDataset(num_samples=500, img_size=IMG_SIZE, split='train')\n",
    "    val_dataset = SyntheticMedicalDataset(num_samples=100, img_size=IMG_SIZE, split='val')\n",
    "elif os.path.exists('data/ISIC2018/images') and len(glob.glob('data/ISIC2018/images/*.jpg')) > 0:\n",
    "    print(\"üìä Loading ISIC 2018 dataset...\")\n",
    "    train_dataset = ISICDataset('data/ISIC2018', img_size=IMG_SIZE, split='train')\n",
    "    val_dataset = ISICDataset('data/ISIC2018', img_size=IMG_SIZE, split='val')\n",
    "elif os.path.exists('data/Synapse/train_npz'):\n",
    "    print(\"üìä Loading Synapse dataset...\")\n",
    "    train_dataset = SynapseDataset('data/Synapse', img_size=IMG_SIZE, split='train')\n",
    "    val_dataset = SynapseDataset('data/Synapse', img_size=IMG_SIZE, split='val')\n",
    "    NUM_CLASSES = 9  # 8 organs + background\n",
    "else:\n",
    "    print(\"üìä Creating synthetic medical dataset (fallback)...\")\n",
    "    train_dataset = SyntheticMedicalDataset(num_samples=500, img_size=IMG_SIZE, split='train')\n",
    "    val_dataset = SyntheticMedicalDataset(num_samples=100, img_size=IMG_SIZE, split='val')\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded!\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some training samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(4):\n",
    "    img, mask = train_dataset[i * 50]\n",
    "    \n",
    "    axes[0, i].imshow(img[0], cmap='gray')\n",
    "    axes[0, i].set_title(f'Image {i+1}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(mask, cmap='viridis')\n",
    "    axes[1, i].set_title(f'Mask {i+1}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Training Samples', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d968686b",
   "metadata": {},
   "source": [
    "## üéØ Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    # Model\n",
    "    'model': 'egm_net',           # 'egm_net', 'egm_net_lite', 'spectral_vmamba'\n",
    "    'in_channels': 1,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'img_size': IMG_SIZE,\n",
    "    'base_channels': 64,\n",
    "    \n",
    "    # Training\n",
    "    'num_epochs': 100,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    \n",
    "    # Loss weights\n",
    "    'dice_weight': 1.0,\n",
    "    'ce_weight': 1.0,\n",
    "    'boundary_weight': 0.5,\n",
    "    \n",
    "    # Implicit representation\n",
    "    'num_points': 2048,           # Points sampled for implicit loss\n",
    "    'boundary_ratio': 0.5,        # Ratio of points near boundaries\n",
    "    \n",
    "    # Checkpointing\n",
    "    'save_every': 10,\n",
    "    'checkpoint_dir': './checkpoints',\n",
    "    \n",
    "    # Early stopping\n",
    "    'patience': 20,\n",
    "}\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "print(\"üìã Training Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"   {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(f\"Creating {config['model']} model...\")\n",
    "\n",
    "if config['model'] == 'egm_net':\n",
    "    model = EGMNet(\n",
    "        in_channels=config['in_channels'],\n",
    "        num_classes=config['num_classes'],\n",
    "        img_size=config['img_size'],\n",
    "        base_channels=config['base_channels'],\n",
    "        num_stages=4,\n",
    "        encoder_depth=2\n",
    "    )\n",
    "elif config['model'] == 'egm_net_lite':\n",
    "    model = EGMNetLite(\n",
    "        in_channels=config['in_channels'],\n",
    "        num_classes=config['num_classes'],\n",
    "        img_size=config['img_size']\n",
    "    )\n",
    "else:  # spectral_vmamba\n",
    "    model = SpectralVMUNet(\n",
    "        in_channels=config['in_channels'],\n",
    "        out_channels=config['num_classes'],\n",
    "        img_size=config['img_size'],\n",
    "        base_channels=config['base_channels'],\n",
    "        num_stages=4\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úÖ Model created!\")\n",
    "print(f\"   Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919f90c",
   "metadata": {},
   "source": [
    "## üìà Loss Functions & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11420cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss for segmentation.\"\"\"\n",
    "    def __init__(self, smooth=1e-5):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # pred: (B, C, H, W) logits\n",
    "        # target: (B, H, W) class indices\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        num_classes = pred.shape[1]\n",
    "        \n",
    "        target_one_hot = F.one_hot(target, num_classes).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        intersection = (pred * target_one_hot).sum(dim=(2, 3))\n",
    "        union = pred.sum(dim=(2, 3)) + target_one_hot.sum(dim=(2, 3))\n",
    "        \n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1.0 - dice.mean()\n",
    "\n",
    "\n",
    "class BoundaryLoss(nn.Module):\n",
    "    \"\"\"Boundary-aware loss using Sobel edge detection.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Sobel kernels\n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)\n",
    "        self.register_buffer('sobel_x', sobel_x.view(1, 1, 3, 3))\n",
    "        self.register_buffer('sobel_y', sobel_y.view(1, 1, 3, 3))\n",
    "        \n",
    "    def get_boundaries(self, mask):\n",
    "        # mask: (B, H, W)\n",
    "        mask = mask.float().unsqueeze(1)\n",
    "        edge_x = F.conv2d(mask, self.sobel_x, padding=1)\n",
    "        edge_y = F.conv2d(mask, self.sobel_y, padding=1)\n",
    "        edges = torch.sqrt(edge_x**2 + edge_y**2)\n",
    "        return (edges > 0.5).float()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Get boundary regions\n",
    "        boundaries = self.get_boundaries(target)\n",
    "        \n",
    "        # Weight loss by boundary\n",
    "        pred_probs = F.softmax(pred, dim=1)\n",
    "        target_one_hot = F.one_hot(target, pred.shape[1]).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        # BCE at boundaries\n",
    "        boundary_loss = F.binary_cross_entropy(\n",
    "            pred_probs * boundaries, \n",
    "            target_one_hot * boundaries,\n",
    "            reduction='sum'\n",
    "        ) / (boundaries.sum() + 1e-6)\n",
    "        \n",
    "        return boundary_loss\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combined loss for EGM-Net training.\"\"\"\n",
    "    def __init__(self, dice_weight=1.0, ce_weight=1.0, boundary_weight=0.5, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_weight = ce_weight\n",
    "        self.boundary_weight = boundary_weight\n",
    "        \n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.boundary_loss = BoundaryLoss()\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        # For EGM-Net, outputs is a dict\n",
    "        if isinstance(outputs, dict):\n",
    "            pred = outputs['output']\n",
    "            coarse = outputs.get('coarse')\n",
    "            fine = outputs.get('fine')\n",
    "            \n",
    "            # Main loss\n",
    "            loss = self.dice_weight * self.dice_loss(pred, targets)\n",
    "            loss += self.ce_weight * self.ce_loss(pred, targets)\n",
    "            loss += self.boundary_weight * self.boundary_loss(pred, targets)\n",
    "            \n",
    "            # Auxiliary losses (coarse and fine branches)\n",
    "            if coarse is not None:\n",
    "                loss += 0.3 * self.ce_loss(coarse, targets)\n",
    "            if fine is not None:\n",
    "                loss += 0.3 * self.ce_loss(fine, targets)\n",
    "                \n",
    "            return loss\n",
    "        else:\n",
    "            # Standard output (SpectralVMUNet)\n",
    "            loss = self.dice_weight * self.dice_loss(outputs, targets)\n",
    "            loss += self.ce_weight * self.ce_loss(outputs, targets)\n",
    "            loss += self.boundary_weight * self.boundary_loss(outputs, targets)\n",
    "            return loss\n",
    "\n",
    "\n",
    "# Metrics\n",
    "def compute_dice(pred, target, num_classes):\n",
    "    \"\"\"Compute per-class Dice scores.\"\"\"\n",
    "    dice_scores = []\n",
    "    pred_classes = torch.argmax(pred, dim=1)\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        pred_c = (pred_classes == c).float()\n",
    "        target_c = (target == c).float()\n",
    "        \n",
    "        intersection = (pred_c * target_c).sum()\n",
    "        union = pred_c.sum() + target_c.sum()\n",
    "        \n",
    "        if union > 0:\n",
    "            dice = (2.0 * intersection) / union\n",
    "        else:\n",
    "            dice = torch.tensor(1.0)  # Both empty = perfect\n",
    "            \n",
    "        dice_scores.append(dice.item())\n",
    "    \n",
    "    return dice_scores\n",
    "\n",
    "\n",
    "def compute_iou(pred, target, num_classes):\n",
    "    \"\"\"Compute per-class IoU scores.\"\"\"\n",
    "    iou_scores = []\n",
    "    pred_classes = torch.argmax(pred, dim=1)\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        pred_c = (pred_classes == c).float()\n",
    "        target_c = (target == c).float()\n",
    "        \n",
    "        intersection = (pred_c * target_c).sum()\n",
    "        union = pred_c.sum() + target_c.sum() - intersection\n",
    "        \n",
    "        if union > 0:\n",
    "            iou = intersection / union\n",
    "        else:\n",
    "            iou = torch.tensor(1.0)\n",
    "            \n",
    "        iou_scores.append(iou.item())\n",
    "    \n",
    "    return iou_scores\n",
    "\n",
    "\n",
    "print(\"‚úÖ Loss functions and metrics defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4374d9",
   "metadata": {},
   "source": [
    "## üöÄ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f10568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training function\n",
    "def train_model(model, train_loader, val_loader, config, device):\n",
    "    \"\"\"Complete training loop with validation and checkpointing.\"\"\"\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = CombinedLoss(\n",
    "        dice_weight=config['dice_weight'],\n",
    "        ce_weight=config['ce_weight'],\n",
    "        boundary_weight=config['boundary_weight'],\n",
    "        num_classes=config['num_classes']\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler (cosine annealing)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=config['num_epochs'],\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_dice': [],\n",
    "        'val_iou': [],\n",
    "        'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    best_val_dice = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ Starting Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        # =============== Training ===============\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\")\n",
    "        \n",
    "        for batch_idx, (images, masks) in enumerate(train_pbar):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # =============== Validation ===============\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_dice = []\n",
    "        all_iou = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Val]\"):\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get prediction tensor\n",
    "                pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "                \n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Compute metrics\n",
    "                dice_scores = compute_dice(pred, masks, config['num_classes'])\n",
    "                iou_scores = compute_iou(pred, masks, config['num_classes'])\n",
    "                \n",
    "                all_dice.append(np.mean(dice_scores[1:]))  # Exclude background\n",
    "                all_iou.append(np.mean(iou_scores[1:]))\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_dice = np.mean(all_dice)\n",
    "        avg_val_iou = np.mean(all_iou)\n",
    "        \n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_dice'].append(avg_val_dice)\n",
    "        history['val_iou'].append(avg_val_iou)\n",
    "        history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nüìä Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "        print(f\"   Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"   Val Loss:   {avg_val_loss:.4f}\")\n",
    "        print(f\"   Val Dice:   {avg_val_dice:.4f}\")\n",
    "        print(f\"   Val IoU:    {avg_val_iou:.4f}\")\n",
    "        print(f\"   LR:         {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # =============== Checkpointing ===============\n",
    "        # Save best model\n",
    "        if avg_val_dice > best_val_dice:\n",
    "            best_val_dice = avg_val_dice\n",
    "            patience_counter = 0\n",
    "            \n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_dice': best_val_dice,\n",
    "                'config': config,\n",
    "                'history': history\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(config['checkpoint_dir'], 'best_model.pth'))\n",
    "            print(f\"   ‚úÖ New best model saved! (Dice: {best_val_dice:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Save periodic checkpoints\n",
    "        if (epoch + 1) % config['save_every'] == 0:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'history': history\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(config['checkpoint_dir'], f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= config['patience']:\n",
    "            print(f\"\\n‚ö†Ô∏è Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üéâ Training completed!\")\n",
    "    print(f\"   Best Val Dice: {best_val_dice:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0bfd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ START TRAINING\n",
    "history = train_model(model, train_loader, val_loader, config, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d9cd45",
   "metadata": {},
   "source": [
    "## üìä Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da620a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training & Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Dice score\n",
    "axes[0, 1].plot(history['val_dice'], label='Val Dice', linewidth=2, color='green')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Dice Score')\n",
    "axes[0, 1].set_title('Validation Dice Score')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].axhline(y=max(history['val_dice']), color='r', linestyle='--', alpha=0.5, label=f\"Best: {max(history['val_dice']):.4f}\")\n",
    "\n",
    "# IoU score\n",
    "axes[1, 0].plot(history['val_iou'], label='Val IoU', linewidth=2, color='orange')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('IoU Score')\n",
    "axes[1, 0].set_title('Validation IoU Score')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 1].plot(history['learning_rate'], label='Learning Rate', linewidth=2, color='purple')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Learning Rate')\n",
    "axes[1, 1].set_title('Learning Rate Schedule')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.suptitle('Training Progress', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìà Training Summary:\")\n",
    "print(f\"   Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"   Final Val Loss:   {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"   Best Val Dice:    {max(history['val_dice']):.4f}\")\n",
    "print(f\"   Best Val IoU:     {max(history['val_iou']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288e93d",
   "metadata": {},
   "source": [
    "## üîç Inference & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4617ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(os.path.join(config['checkpoint_dir'], 'best_model.pth'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"‚úÖ Loaded best model from epoch {checkpoint['epoch']} (Dice: {checkpoint['best_val_dice']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on validation set\n",
    "model.eval()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(4):\n",
    "        idx = i * (len(val_dataset) // 4)\n",
    "        img, mask = val_dataset[idx]\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get prediction\n",
    "        outputs = model(img)\n",
    "        pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "        pred_mask = torch.argmax(pred, dim=1)[0].cpu()\n",
    "        \n",
    "        # Get energy map if available\n",
    "        energy = outputs.get('energy', None)\n",
    "        \n",
    "        # Plot\n",
    "        axes[i, 0].imshow(img[0, 0].cpu(), cmap='gray')\n",
    "        axes[i, 0].set_title('Input Image')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(mask, cmap='viridis')\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(pred_mask, cmap='viridis')\n",
    "        axes[i, 2].set_title('Prediction')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        overlay = img[0, 0].cpu().numpy()\n",
    "        overlay = np.stack([overlay, overlay, overlay], axis=-1)\n",
    "        pred_np = pred_mask.numpy()\n",
    "        mask_np = mask.numpy()\n",
    "        \n",
    "        # Red for prediction, blue for ground truth\n",
    "        overlay[..., 0] = np.where(pred_np > 0, 1.0, overlay[..., 0])\n",
    "        overlay[..., 2] = np.where(mask_np > 0, 1.0, overlay[..., 2])\n",
    "        overlay = np.clip(overlay, 0, 1)\n",
    "        \n",
    "        axes[i, 3].imshow(overlay)\n",
    "        axes[i, 3].set_title('Overlay (Red=Pred, Blue=GT)')\n",
    "        axes[i, 3].axis('off')\n",
    "\n",
    "plt.suptitle('Validation Predictions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce67046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize EGM-Net branches (Coarse vs Fine)\n",
    "if config['model'] in ['egm_net', 'egm_net_lite']:\n",
    "    fig, axes = plt.subplots(3, 5, figsize=(20, 12))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(3):\n",
    "            idx = i * (len(val_dataset) // 3)\n",
    "            img, mask = val_dataset[idx]\n",
    "            img = img.unsqueeze(0).to(device)\n",
    "            \n",
    "            outputs = model(img)\n",
    "            \n",
    "            # Extract all outputs\n",
    "            final_pred = torch.argmax(outputs['output'], dim=1)[0].cpu()\n",
    "            coarse_pred = torch.argmax(outputs['coarse'], dim=1)[0].cpu()\n",
    "            fine_pred = torch.argmax(outputs['fine'], dim=1)[0].cpu()\n",
    "            energy = outputs['energy'][0, 0].cpu()\n",
    "            \n",
    "            # Plot\n",
    "            axes[i, 0].imshow(img[0, 0].cpu(), cmap='gray')\n",
    "            axes[i, 0].set_title('Input')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(energy, cmap='hot')\n",
    "            axes[i, 1].set_title('Energy Map')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            axes[i, 2].imshow(coarse_pred, cmap='viridis')\n",
    "            axes[i, 2].set_title('Coarse Branch')\n",
    "            axes[i, 2].axis('off')\n",
    "            \n",
    "            axes[i, 3].imshow(fine_pred, cmap='viridis')\n",
    "            axes[i, 3].set_title('Fine Branch')\n",
    "            axes[i, 3].axis('off')\n",
    "            \n",
    "            axes[i, 4].imshow(final_pred, cmap='viridis')\n",
    "            axes[i, 4].set_title('Final (Fused)')\n",
    "            axes[i, 4].axis('off')\n",
    "    \n",
    "    plt.suptitle('EGM-Net Branch Analysis\\\\n(Energy-Gated Fusion of Coarse + Fine)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('branch_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01eb4b2",
   "metadata": {},
   "source": [
    "## üéØ Resolution-Free Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate resolution-free inference (unique to EGM-Net)\n",
    "if config['model'] in ['egm_net', 'egm_net_lite']:\n",
    "    print(\"üî¨ Resolution-Free Inference Demo\")\n",
    "    print(\"   EGM-Net can render at ANY resolution without retraining!\")\n",
    "    \n",
    "    resolutions = [64, 128, 256, 512]\n",
    "    \n",
    "    # Get a sample image\n",
    "    img, mask = val_dataset[0]\n",
    "    img = img.unsqueeze(0).to(device)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, len(resolutions), figsize=(16, 8))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, res in enumerate(resolutions):\n",
    "            # Render at different resolutions\n",
    "            outputs = model(img, output_size=(res, res))\n",
    "            pred = torch.argmax(outputs['output'], dim=1)[0].cpu()\n",
    "            \n",
    "            # Also show input at same res for comparison\n",
    "            input_resized = F.interpolate(img, size=(res, res), mode='bilinear', align_corners=False)\n",
    "            \n",
    "            axes[0, i].imshow(input_resized[0, 0].cpu(), cmap='gray')\n",
    "            axes[0, i].set_title(f'Input {res}√ó{res}')\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            axes[1, i].imshow(pred, cmap='viridis')\n",
    "            axes[1, i].set_title(f'Prediction {res}√ó{res}')\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Resolution-Free Rendering\\\\n(Same model weights, different output resolutions)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('resolution_free.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\\\n‚úÖ Same model can output at 64√ó64 to 512√ó512 (or higher)!\")\n",
    "    print(\"   This is impossible with standard CNN decoders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f3f21",
   "metadata": {},
   "source": [
    "## üíæ Save & Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model to Google Drive\n",
    "from google.colab import drive\n",
    "\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Save to Drive\n",
    "    save_path = '/content/drive/MyDrive/EGM_Net_Models'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    final_checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': config,\n",
    "        'history': history,\n",
    "        'best_val_dice': max(history['val_dice'])\n",
    "    }\n",
    "    torch.save(final_checkpoint, os.path.join(save_path, 'egm_net_trained.pth'))\n",
    "    \n",
    "    # Also copy training curves\n",
    "    import shutil\n",
    "    shutil.copy('training_curves.png', save_path)\n",
    "    shutil.copy('predictions.png', save_path)\n",
    "    \n",
    "    print(f\"‚úÖ Model saved to Google Drive: {save_path}\")\n",
    "    print(f\"   - egm_net_trained.pth\")\n",
    "    print(f\"   - training_curves.png\")\n",
    "    print(f\"   - predictions.png\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save to Google Drive: {e}\")\n",
    "    print(\"   Model is saved locally in ./checkpoints/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ea463",
   "metadata": {},
   "source": [
    "## üìä Final Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b0f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on full validation set\n",
    "model.eval()\n",
    "\n",
    "all_dice_scores = []\n",
    "all_iou_scores = []\n",
    "all_hd95_scores = []  # Hausdorff Distance 95\n",
    "\n",
    "# Helper function for Hausdorff distance\n",
    "def compute_hausdorff_95(pred, target):\n",
    "    \"\"\"Compute 95th percentile Hausdorff distance.\"\"\"\n",
    "    from scipy.ndimage import distance_transform_edt\n",
    "    \n",
    "    pred_np = pred.numpy().astype(bool)\n",
    "    target_np = target.numpy().astype(bool)\n",
    "    \n",
    "    if pred_np.sum() == 0 or target_np.sum() == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Distance transforms\n",
    "    pred_dist = distance_transform_edt(~pred_np)\n",
    "    target_dist = distance_transform_edt(~target_np)\n",
    "    \n",
    "    # Get surface points\n",
    "    pred_surface = pred_np & (distance_transform_edt(pred_np) <= 1)\n",
    "    target_surface = target_np & (distance_transform_edt(target_np) <= 1)\n",
    "    \n",
    "    # Distances from pred surface to target, and vice versa\n",
    "    d_pred_to_target = target_dist[pred_surface]\n",
    "    d_target_to_pred = pred_dist[target_surface]\n",
    "    \n",
    "    if len(d_pred_to_target) == 0 or len(d_target_to_pred) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # 95th percentile\n",
    "    hd95 = max(np.percentile(d_pred_to_target, 95), np.percentile(d_target_to_pred, 95))\n",
    "    return hd95\n",
    "\n",
    "print(\"üîç Running final evaluation...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "        pred_masks = torch.argmax(pred, dim=1)\n",
    "        \n",
    "        for b in range(images.shape[0]):\n",
    "            # Per-sample metrics\n",
    "            dice = compute_dice(pred[b:b+1], masks[b:b+1], config['num_classes'])\n",
    "            iou = compute_iou(pred[b:b+1], masks[b:b+1], config['num_classes'])\n",
    "            \n",
    "            all_dice_scores.append(np.mean(dice[1:]))  # Exclude background\n",
    "            all_iou_scores.append(np.mean(iou[1:]))\n",
    "            \n",
    "            # Hausdorff distance (for foreground)\n",
    "            try:\n",
    "                hd95 = compute_hausdorff_95(\n",
    "                    (pred_masks[b] > 0).cpu(),\n",
    "                    (masks[b] > 0).cpu()\n",
    "                )\n",
    "                all_hd95_scores.append(hd95)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Metric':<20} {'Mean':<12} {'Std':<12}\")\n",
    "print(\"-\"*44)\n",
    "print(f\"{'Dice Score':<20} {np.mean(all_dice_scores):.4f}       {np.std(all_dice_scores):.4f}\")\n",
    "print(f\"{'IoU Score':<20} {np.mean(all_iou_scores):.4f}       {np.std(all_iou_scores):.4f}\")\n",
    "if all_hd95_scores:\n",
    "    print(f\"{'HD95 (mm)':<20} {np.mean(all_hd95_scores):.2f}         {np.std(all_hd95_scores):.2f}\")\n",
    "print(\"-\"*44)\n",
    "print(f\"\\nTotal validation samples: {len(all_dice_scores)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e47b55f",
   "metadata": {},
   "source": [
    "## 3. Test Monogenic Signal Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea0165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test image with edges\n",
    "def create_test_image(size=256):\n",
    "    \"\"\"Create synthetic medical-like image with organs.\"\"\"\n",
    "    img = torch.zeros(1, 1, size, size)\n",
    "    \n",
    "    # Add circular \"organ\"\n",
    "    y, x = torch.meshgrid(torch.arange(size), torch.arange(size), indexing='ij')\n",
    "    center1 = (size // 2, size // 2)\n",
    "    radius1 = size // 4\n",
    "    mask1 = ((x - center1[0])**2 + (y - center1[1])**2) < radius1**2\n",
    "    img[0, 0, mask1] = 0.7\n",
    "    \n",
    "    # Add smaller \"tumor\"\n",
    "    center2 = (size // 2 + 30, size // 2 - 20)\n",
    "    radius2 = size // 10\n",
    "    mask2 = ((x - center2[0])**2 + (y - center2[1])**2) < radius2**2\n",
    "    img[0, 0, mask2] = 1.0\n",
    "    \n",
    "    # Add noise\n",
    "    img = img + 0.05 * torch.randn_like(img)\n",
    "    \n",
    "    return img, mask1.float(), mask2.float()\n",
    "\n",
    "# Create test image\n",
    "test_img, organ_mask, tumor_mask = create_test_image(256)\n",
    "print(f\"Test image shape: {test_img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18065bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Monogenic Energy Extraction\n",
    "energy_extractor = EnergyMap(normalize=True, smoothing_sigma=1.0)\n",
    "energy, mono_out = energy_extractor(test_img)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].imshow(test_img[0, 0], cmap='gray')\n",
    "axes[0, 0].set_title('Input Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(energy[0, 0].detach(), cmap='hot')\n",
    "axes[0, 1].set_title('Energy Map (Edges)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(mono_out['phase'][0, 0].detach(), cmap='twilight')\n",
    "axes[0, 2].set_title('Phase')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(mono_out['orientation'][0, 0].detach(), cmap='hsv')\n",
    "axes[1, 0].set_title('Orientation')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(mono_out['riesz_x'][0, 0].detach(), cmap='RdBu')\n",
    "axes[1, 1].set_title('Riesz X Component')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(mono_out['riesz_y'][0, 0].detach(), cmap='RdBu')\n",
    "axes[1, 2].set_title('Riesz Y Component')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Monogenic Signal Decomposition', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Monogenic processing works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee600ecb",
   "metadata": {},
   "source": [
    "## 4. Test Gabor Basis vs Fourier Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gabor_implicit import GaborBasis, FourierFeatures\n",
    "\n",
    "# Create coordinate grid\n",
    "size = 128\n",
    "y = torch.linspace(-1, 1, size)\n",
    "x = torch.linspace(-1, 1, size)\n",
    "yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "coords = torch.stack([xx, yy], dim=-1).view(1, -1, 2)  # (1, size*size, 2)\n",
    "\n",
    "# Compare Gabor vs Fourier\n",
    "gabor = GaborBasis(input_dim=2, num_frequencies=32)\n",
    "fourier = FourierFeatures(input_dim=2, num_frequencies=32, scale=10.0)\n",
    "\n",
    "gabor_features = gabor(coords)\n",
    "fourier_features = fourier(coords)\n",
    "\n",
    "print(f\"Gabor features shape: {gabor_features.shape}\")\n",
    "print(f\"Fourier features shape: {fourier_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7aaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first few basis functions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(4):\n",
    "    # Gabor\n",
    "    gabor_vis = gabor_features[0, :, i].view(size, size).detach().numpy()\n",
    "    axes[0, i].imshow(gabor_vis, cmap='RdBu', vmin=-1, vmax=1)\n",
    "    axes[0, i].set_title(f'Gabor Basis {i+1}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Fourier\n",
    "    fourier_vis = fourier_features[0, :, i].view(size, size).detach().numpy()\n",
    "    axes[1, i].imshow(fourier_vis, cmap='RdBu', vmin=-1, vmax=1)\n",
    "    axes[1, i].set_title(f'Fourier Basis {i+1}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Gabor\\n(Localized)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Fourier\\n(Global)', fontsize=12)\n",
    "\n",
    "plt.suptitle('Gabor vs Fourier Basis Functions\\n(Gabor is localized ‚Üí No Gibbs ringing)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ef9df",
   "metadata": {},
   "source": [
    "## 5. Create and Analyze Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c426ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create EGM-Net models\n",
    "print(\"Creating models...\")\n",
    "\n",
    "# Full model\n",
    "egm_net = EGMNet(\n",
    "    in_channels=1,\n",
    "    num_classes=3,\n",
    "    img_size=256,\n",
    "    base_channels=64,\n",
    "    num_stages=4,\n",
    "    encoder_depth=2\n",
    ").to(device)\n",
    "\n",
    "# Lite model\n",
    "egm_lite = EGMNetLite(\n",
    "    in_channels=1,\n",
    "    num_classes=3,\n",
    "    img_size=256\n",
    ").to(device)\n",
    "\n",
    "# Spectral Mamba (comparison)\n",
    "spec_mamba = SpectralVMUNet(\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    img_size=256,\n",
    "    base_channels=64,\n",
    "    num_stages=4\n",
    ").to(device)\n",
    "\n",
    "print(\"\\nüìä Model Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "models = {\n",
    "    'EGM-Net Full': egm_net,\n",
    "    'EGM-Net Lite': egm_lite,\n",
    "    'SpectralVMUNet': spec_mamba\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name:20s}: {params:,} parameters ({params/1e6:.2f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeffa14a",
   "metadata": {},
   "source": [
    "## 6. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c023bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "test_input = torch.randn(2, 1, 256, 256).to(device)\n",
    "\n",
    "print(\"Testing forward pass...\")\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # EGM-Net\n",
    "    egm_out = egm_net(test_input)\n",
    "    print(f\"\\nüîπ EGM-Net Output:\")\n",
    "    for k, v in egm_out.items():\n",
    "        print(f\"   {k}: {v.shape}\")\n",
    "    \n",
    "    # SpectralVMUNet\n",
    "    spec_out = spec_mamba(test_input)\n",
    "    print(f\"\\nüîπ SpectralVMUNet Output: {spec_out.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4b153",
   "metadata": {},
   "source": [
    "## 7. Test Resolution-Free Inference (Unique to EGM-Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb14e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EGM-Net can query at arbitrary coordinates!\n",
    "print(\"Testing Resolution-Free Inference...\")\n",
    "\n",
    "# Create query points (random locations)\n",
    "num_points = 10000\n",
    "random_coords = torch.rand(1, num_points, 2).to(device) * 2 - 1  # [-1, 1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Query at random points\n",
    "    point_output = egm_net.query_points(test_input[:1], random_coords)\n",
    "    \n",
    "print(f\"Query coordinates: {random_coords.shape}\")\n",
    "print(f\"Point outputs: {point_output.shape}\")\n",
    "print(\"\\n‚úÖ Resolution-free inference works!\")\n",
    "print(\"   ‚Üí You can zoom into boundaries at ANY resolution!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate resolution-free: render at different resolutions\n",
    "resolutions = [64, 128, 256, 512]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, res in enumerate(resolutions):\n",
    "        # Render at this resolution\n",
    "        output = egm_net(test_input[:1], output_size=(res, res))\n",
    "        pred = torch.argmax(output['output'], dim=1)[0].cpu().numpy()\n",
    "        \n",
    "        axes[idx].imshow(pred, cmap='viridis')\n",
    "        axes[idx].set_title(f'{res}√ó{res}')\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Resolution-Free Rendering (Same model, different output sizes)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04640dc",
   "metadata": {},
   "source": [
    "## 8. Visualize Energy-Gated Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01794b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dual-branch architecture\n",
    "with torch.no_grad():\n",
    "    outputs = egm_net(test_input[:1])\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Input\n",
    "axes[0, 0].imshow(test_input[0, 0].cpu(), cmap='gray')\n",
    "axes[0, 0].set_title('Input Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Energy Map\n",
    "axes[0, 1].imshow(outputs['energy'][0, 0].cpu(), cmap='hot')\n",
    "axes[0, 1].set_title('Energy Map (Edge Detection)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Coarse Branch\n",
    "coarse_pred = torch.argmax(outputs['coarse'], dim=1)[0].cpu()\n",
    "axes[0, 2].imshow(coarse_pred, cmap='viridis')\n",
    "axes[0, 2].set_title('Coarse Branch (Smooth)')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Fine Branch\n",
    "fine_pred = torch.argmax(outputs['fine'], dim=1)[0].cpu()\n",
    "axes[1, 0].imshow(fine_pred, cmap='viridis')\n",
    "axes[1, 0].set_title('Fine Branch (Sharp)')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Final Output\n",
    "final_pred = torch.argmax(outputs['output'], dim=1)[0].cpu()\n",
    "axes[1, 1].imshow(final_pred, cmap='viridis')\n",
    "axes[1, 1].set_title('Final Output (Fused)')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Difference\n",
    "diff = (fine_pred != coarse_pred).float()\n",
    "axes[1, 2].imshow(diff, cmap='Reds')\n",
    "axes[1, 2].set_title('Difference (Fine vs Coarse)')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle('EGM-Net Dual-Branch Architecture', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c51b26",
   "metadata": {},
   "source": [
    "## 9. Quick Training Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d86f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_egm import EGMNetTrainer, create_dummy_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create small dummy dataset\n",
    "print(\"Creating dummy dataset...\")\n",
    "dataset = create_dummy_dataset(num_samples=16, img_size=256, num_classes=3)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Training config\n",
    "config = {\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'num_epochs': 2,\n",
    "    'num_points': 1024,\n",
    "    'boundary_ratio': 0.5,\n",
    "    'checkpoint_dir': './checkpoints_demo'\n",
    "}\n",
    "\n",
    "# Use lite model for faster training\n",
    "model = EGMNetLite(in_channels=1, num_classes=3, img_size=256)\n",
    "print(f\"Model: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b6e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for a few epochs\n",
    "print(\"\\nStarting training demo...\")\n",
    "trainer = EGMNetTrainer(model, config, device=device)\n",
    "trainer.train(train_loader, num_epochs=2)\n",
    "\n",
    "print(\"\\n‚úÖ Training demo completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c9e44f",
   "metadata": {},
   "source": [
    "## 10. Inference Speed Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_model(model, input_tensor, num_runs=50, warmup=10):\n",
    "    \"\"\"Benchmark inference speed.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(input_tensor)\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            start = time.time()\n",
    "            _ = model(input_tensor)\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "    \n",
    "    return np.mean(times) * 1000, np.std(times) * 1000  # ms\n",
    "\n",
    "# Benchmark\n",
    "print(\"Benchmarking inference speed...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "test_input = torch.randn(1, 1, 256, 256).to(device)\n",
    "\n",
    "for name, model in [('EGM-Net Full', egm_net), ('EGM-Net Lite', egm_lite)]:\n",
    "    mean_time, std_time = benchmark_model(model, test_input)\n",
    "    fps = 1000 / mean_time\n",
    "    print(f\"{name:20s}: {mean_time:.2f} ¬± {std_time:.2f} ms ({fps:.1f} FPS)\")\n",
    "\n",
    "print(\"\\n‚úÖ Benchmark completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17a31bd",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb75fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                    EGM-NET ARCHITECTURE SUMMARY                       ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  üî¨ KEY INNOVATIONS:                                                  ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  1. MONOGENIC ENERGY GATING                                          ‚ïë\n",
    "‚ïë     ‚Ä¢ Physics-based edge detection (Riesz Transform)                 ‚ïë\n",
    "‚ïë     ‚Ä¢ Automatically focuses on boundary regions                      ‚ïë\n",
    "‚ïë     ‚Ä¢ Suppresses artifacts in flat regions                           ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  2. GABOR BASIS (vs Fourier)                                         ‚ïë\n",
    "‚ïë     ‚Ä¢ Localized oscillations (Gaussian √ó sin)                        ‚ïë\n",
    "‚ïë     ‚Ä¢ NO Gibbs ringing artifacts                                     ‚ïë\n",
    "‚ïë     ‚Ä¢ Sharp edges remain clean                                       ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  3. DUAL-PATH ARCHITECTURE                                           ‚ïë\n",
    "‚ïë     ‚Ä¢ Coarse Branch: Smooth body regions (Conv decoder)              ‚ïë\n",
    "‚ïë     ‚Ä¢ Fine Branch: Sharp boundaries (Gabor Implicit)                 ‚ïë\n",
    "‚ïë     ‚Ä¢ Energy-gated fusion: Best of both worlds                       ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  4. RESOLUTION-FREE INFERENCE                                        ‚ïë\n",
    "‚ïë     ‚Ä¢ Query at ANY coordinate ‚Üí Infinite zoom                        ‚ïë\n",
    "‚ïë     ‚Ä¢ No retraining needed for different resolutions                 ‚ïë\n",
    "‚ïë     ‚Ä¢ Perfect for high-resolution medical imaging                    ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  5. MAMBA ENCODER                                                    ‚ïë\n",
    "‚ïë     ‚Ä¢ O(N) complexity (vs O(N¬≤) for Transformers)                    ‚ïë\n",
    "‚ïë     ‚Ä¢ Global context awareness                                       ‚ïë\n",
    "‚ïë     ‚Ä¢ Efficient for large images                                     ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  üìä MODEL SIZES:                                                      ‚ïë\n",
    "‚ïë     ‚Ä¢ EGM-Net Full:  ~9.13M parameters                               ‚ïë\n",
    "‚ïë     ‚Ä¢ EGM-Net Lite:  ~635K parameters                                ‚ïë\n",
    "‚ïë     ‚Ä¢ SpectralVMUNet: ~10.31M parameters                             ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36957ab8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Next Steps\n",
    "\n",
    "1. **Train on real data**: Replace dummy dataset with medical imaging dataset (e.g., Synapse, ACDC)\n",
    "2. **Tune hyperparameters**: Adjust `num_frequencies`, `boundary_ratio`, learning rate\n",
    "3. **Evaluate metrics**: Dice score, IoU, Hausdorff distance\n",
    "4. **Ablation study**: Compare Gabor vs Fourier, with/without energy gating\n",
    "\n",
    "---\n",
    "\n",
    "**Repository**: https://github.com/QuocKhanhLuong/FourierNetwork"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
