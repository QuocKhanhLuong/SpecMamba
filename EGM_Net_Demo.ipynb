{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a590fe",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678d1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/QuocKhanhLuong/FourierNetwork.git\n",
    "%cd FourierNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f537f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ðŸ–¥ï¸ Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567d48f",
   "metadata": {},
   "source": [
    "## 2. Import Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our models\n",
    "from monogenic import EnergyMap, MonogenicSignal, BoundaryDetector\n",
    "from gabor_implicit import GaborBasis, GaborNet, ImplicitSegmentationHead\n",
    "from egm_net import EGMNet, EGMNetLite\n",
    "from spectral_mamba import SpectralVMUNet\n",
    "\n",
    "print(\"âœ… All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1eae7",
   "metadata": {},
   "source": [
    "## ðŸ¥ REAL DATA TRAINING\n",
    "\n",
    "This section trains on real medical imaging datasets. Choose from:\n",
    "\n",
    "| Dataset | Modality | Classes | Description |\n",
    "|---------|----------|---------|-------------|\n",
    "| **ACDC** | Cardiac MRI | 4 | LV, RV, Myocardium + Background |\n",
    "| **BraTS21** | Brain MRI | 4 | Tumor core, Enhancing, Edema + Background |\n",
    "| **M&Ms** | Cardiac MRI | 4 | Multi-center, Multi-vendor cardiac segmentation |\n",
    "\n",
    "**Your Google Drive Links:**\n",
    "- ACDC: `1EelzBVjIoDQ4uzt0_2JzmF_PuUHsD93e`\n",
    "- BraTS21: `1m7b5u_6cEj9PbqzZQDNoDonnXIowBCY2`\n",
    "- M&Ms: `1DpW8ucYE17Tj8iMlsAev_yM6TaZZmWYj`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03af621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and select dataset\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”§ CONFIGURE YOUR DATASET HERE\n",
    "# ============================================================\n",
    "DATASET = 'ACDC'  # Options: 'ACDC', 'BraTS21', 'MnM'\n",
    "\n",
    "# Google Drive folder IDs (from your links)\n",
    "DRIVE_FOLDERS = {\n",
    "    'ACDC': '1EelzBVjIoDQ4uzt0_2JzmF_PuUHsD93e',\n",
    "    'BraTS21': '1m7b5u_6cEj9PbqzZQDNoDonnXIowBCY2',\n",
    "    'MnM': '1DpW8ucYE17Tj8iMlsAev_yM6TaZZmWYj'\n",
    "}\n",
    "\n",
    "# Dataset configurations\n",
    "DATASET_CONFIG = {\n",
    "    'ACDC': {\n",
    "        'num_classes': 4,  # Background, RV, Myo, LV\n",
    "        'class_names': ['Background', 'RV', 'Myocardium', 'LV'],\n",
    "        'in_channels': 1,\n",
    "        'img_size': 256,\n",
    "        'drive_path': '/content/drive/MyDrive/ACDC'  # Adjust if different\n",
    "    },\n",
    "    'BraTS21': {\n",
    "        'num_classes': 4,  # Background, NCR/NET, ED, ET\n",
    "        'class_names': ['Background', 'NCR/NET', 'Edema', 'Enhancing'],\n",
    "        'in_channels': 4,  # T1, T1ce, T2, FLAIR\n",
    "        'img_size': 256,\n",
    "        'drive_path': '/content/drive/MyDrive/BraTS21'\n",
    "    },\n",
    "    'MnM': {\n",
    "        'num_classes': 4,  # Background, RV, Myo, LV\n",
    "        'class_names': ['Background', 'RV', 'Myocardium', 'LV'],\n",
    "        'in_channels': 1,\n",
    "        'img_size': 256,\n",
    "        'drive_path': '/content/drive/MyDrive/MnM'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get current config\n",
    "config_data = DATASET_CONFIG[DATASET]\n",
    "print(f\"ðŸ“Š Selected Dataset: {DATASET}\")\n",
    "print(f\"   Classes: {config_data['num_classes']} - {config_data['class_names']}\")\n",
    "print(f\"   Input channels: {config_data['in_channels']}\")\n",
    "print(f\"   Image size: {config_data['img_size']}Ã—{config_data['img_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset location and structure\n",
    "import glob\n",
    "\n",
    "# Try to find data in Drive\n",
    "drive_path = config_data['drive_path']\n",
    "\n",
    "if os.path.exists(drive_path):\n",
    "    print(f\"âœ… Found dataset at: {drive_path}\")\n",
    "    print(\"\\nðŸ“ Directory structure:\")\n",
    "    for item in os.listdir(drive_path)[:10]:\n",
    "        item_path = os.path.join(drive_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"   ðŸ“‚ {item}/\")\n",
    "            sub_items = os.listdir(item_path)[:3]\n",
    "            for sub in sub_items:\n",
    "                print(f\"      â””â”€â”€ {sub}\")\n",
    "        else:\n",
    "            print(f\"   ðŸ“„ {item}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Dataset not found at: {drive_path}\")\n",
    "    print(\"\\nðŸ”§ Please update 'drive_path' in DATASET_CONFIG above\")\n",
    "    print(\"   Or upload your data to the expected location\")\n",
    "    \n",
    "    # Try alternative locations\n",
    "    alt_paths = [\n",
    "        f'/content/drive/MyDrive/{DATASET}',\n",
    "        f'/content/drive/MyDrive/Datasets/{DATASET}',\n",
    "        f'/content/drive/MyDrive/Medical/{DATASET}'\n",
    "    ]\n",
    "    for alt in alt_paths:\n",
    "        if os.path.exists(alt):\n",
    "            print(f\"\\nâœ… Found at alternative location: {alt}\")\n",
    "            drive_path = alt\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e088d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Classes for ACDC, BraTS21, M&Ms\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import h5py\n",
    "import nibabel as nib\n",
    "\n",
    "\n",
    "class ACDCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ACDC Cardiac MRI Dataset\n",
    "    Classes: Background (0), RV (1), Myocardium (2), LV (3)\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, img_size=256, split='train', transform=None):\n",
    "        self.img_size = img_size\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Find all patient folders or npz files\n",
    "        self.samples = []\n",
    "        \n",
    "        # Check for npz format (preprocessed)\n",
    "        npz_files = sorted(glob.glob(os.path.join(data_dir, f'{split}/*.npz')))\n",
    "        if len(npz_files) > 0:\n",
    "            self.samples = npz_files\n",
    "            self.format = 'npz'\n",
    "            print(f\"   Found {len(self.samples)} NPZ files\")\n",
    "        else:\n",
    "            # Check for h5 format\n",
    "            h5_files = sorted(glob.glob(os.path.join(data_dir, f'{split}/*.h5')))\n",
    "            if len(h5_files) > 0:\n",
    "                self.samples = h5_files\n",
    "                self.format = 'h5'\n",
    "                print(f\"   Found {len(self.samples)} H5 files\")\n",
    "            else:\n",
    "                # Check for NIfTI format (raw ACDC)\n",
    "                patient_dirs = sorted(glob.glob(os.path.join(data_dir, 'patient*')))\n",
    "                for patient_dir in patient_dirs:\n",
    "                    # Find frame images and ground truth\n",
    "                    gt_files = glob.glob(os.path.join(patient_dir, '*_gt.nii.gz'))\n",
    "                    for gt_file in gt_files:\n",
    "                        img_file = gt_file.replace('_gt.nii.gz', '.nii.gz')\n",
    "                        if os.path.exists(img_file):\n",
    "                            self.samples.append((img_file, gt_file))\n",
    "                self.format = 'nifti'\n",
    "                print(f\"   Found {len(self.samples)} NIfTI pairs\")\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.format == 'npz':\n",
    "            data = np.load(self.samples[idx])\n",
    "            img = data['image'].astype(np.float32)\n",
    "            mask = data['label'].astype(np.int64)\n",
    "            \n",
    "        elif self.format == 'h5':\n",
    "            with h5py.File(self.samples[idx], 'r') as f:\n",
    "                img = f['image'][()].astype(np.float32)\n",
    "                mask = f['label'][()].astype(np.int64)\n",
    "                \n",
    "        elif self.format == 'nifti':\n",
    "            img_file, gt_file = self.samples[idx]\n",
    "            img_nii = nib.load(img_file)\n",
    "            gt_nii = nib.load(gt_file)\n",
    "            \n",
    "            img = img_nii.get_fdata().astype(np.float32)\n",
    "            mask = gt_nii.get_fdata().astype(np.int64)\n",
    "            \n",
    "            # Take middle slice if 3D\n",
    "            if len(img.shape) == 3:\n",
    "                mid_slice = img.shape[2] // 2\n",
    "                img = img[:, :, mid_slice]\n",
    "                mask = mask[:, :, mid_slice]\n",
    "        \n",
    "        # Normalize image\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "        \n",
    "        # Resize if needed\n",
    "        if img.shape[0] != self.img_size or img.shape[1] != self.img_size:\n",
    "            img = np.array(Image.fromarray(img).resize((self.img_size, self.img_size), Image.BILINEAR))\n",
    "            mask = np.array(Image.fromarray(mask.astype(np.uint8)).resize((self.img_size, self.img_size), Image.NEAREST))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        img = torch.from_numpy(img).unsqueeze(0).float()\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "\n",
    "class BraTS21Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    BraTS 2021 Brain Tumor Segmentation Dataset\n",
    "    Modalities: T1, T1ce, T2, FLAIR (4 channels)\n",
    "    Classes: Background (0), NCR/NET (1), Edema (2), Enhancing Tumor (3)\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, img_size=256, split='train', transform=None):\n",
    "        self.img_size = img_size\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        \n",
    "        # Check for npz format\n",
    "        npz_files = sorted(glob.glob(os.path.join(data_dir, f'{split}/*.npz')))\n",
    "        if len(npz_files) > 0:\n",
    "            self.samples = npz_files\n",
    "            self.format = 'npz'\n",
    "            print(f\"   Found {len(self.samples)} NPZ files\")\n",
    "        else:\n",
    "            # Check for NIfTI format\n",
    "            patient_dirs = sorted(glob.glob(os.path.join(data_dir, 'BraTS*')))\n",
    "            for patient_dir in patient_dirs:\n",
    "                t1_file = glob.glob(os.path.join(patient_dir, '*_t1.nii.gz'))\n",
    "                t1ce_file = glob.glob(os.path.join(patient_dir, '*_t1ce.nii.gz'))\n",
    "                t2_file = glob.glob(os.path.join(patient_dir, '*_t2.nii.gz'))\n",
    "                flair_file = glob.glob(os.path.join(patient_dir, '*_flair.nii.gz'))\n",
    "                seg_file = glob.glob(os.path.join(patient_dir, '*_seg.nii.gz'))\n",
    "                \n",
    "                if all([t1_file, t1ce_file, t2_file, flair_file, seg_file]):\n",
    "                    self.samples.append({\n",
    "                        't1': t1_file[0],\n",
    "                        't1ce': t1ce_file[0],\n",
    "                        't2': t2_file[0],\n",
    "                        'flair': flair_file[0],\n",
    "                        'seg': seg_file[0]\n",
    "                    })\n",
    "            self.format = 'nifti'\n",
    "            print(f\"   Found {len(self.samples)} patient folders\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.format == 'npz':\n",
    "            data = np.load(self.samples[idx])\n",
    "            img = data['image'].astype(np.float32)  # (4, H, W) or (H, W, 4)\n",
    "            mask = data['label'].astype(np.int64)\n",
    "            \n",
    "            if len(img.shape) == 3 and img.shape[-1] == 4:\n",
    "                img = img.transpose(2, 0, 1)  # (H, W, 4) -> (4, H, W)\n",
    "                \n",
    "        elif self.format == 'nifti':\n",
    "            sample = self.samples[idx]\n",
    "            \n",
    "            # Load all modalities\n",
    "            t1 = nib.load(sample['t1']).get_fdata()\n",
    "            t1ce = nib.load(sample['t1ce']).get_fdata()\n",
    "            t2 = nib.load(sample['t2']).get_fdata()\n",
    "            flair = nib.load(sample['flair']).get_fdata()\n",
    "            mask = nib.load(sample['seg']).get_fdata()\n",
    "            \n",
    "            # Take middle slice\n",
    "            mid_slice = t1.shape[2] // 2\n",
    "            t1 = t1[:, :, mid_slice]\n",
    "            t1ce = t1ce[:, :, mid_slice]\n",
    "            t2 = t2[:, :, mid_slice]\n",
    "            flair = flair[:, :, mid_slice]\n",
    "            mask = mask[:, :, mid_slice]\n",
    "            \n",
    "            # Stack modalities\n",
    "            img = np.stack([t1, t1ce, t2, flair], axis=0).astype(np.float32)\n",
    "            mask = mask.astype(np.int64)\n",
    "        \n",
    "        # Normalize each channel\n",
    "        for c in range(img.shape[0]):\n",
    "            ch = img[c]\n",
    "            img[c] = (ch - ch.min()) / (ch.max() - ch.min() + 1e-8)\n",
    "        \n",
    "        # Resize if needed\n",
    "        if img.shape[1] != self.img_size:\n",
    "            new_img = np.zeros((img.shape[0], self.img_size, self.img_size), dtype=np.float32)\n",
    "            for c in range(img.shape[0]):\n",
    "                new_img[c] = np.array(Image.fromarray(img[c]).resize((self.img_size, self.img_size), Image.BILINEAR))\n",
    "            img = new_img\n",
    "            mask = np.array(Image.fromarray(mask.astype(np.uint8)).resize((self.img_size, self.img_size), Image.NEAREST))\n",
    "        \n",
    "        # Remap BraTS labels: 0->0, 1->1, 2->2, 4->3\n",
    "        mask[mask == 4] = 3\n",
    "        \n",
    "        img = torch.from_numpy(img).float()\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "\n",
    "class MnMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    M&Ms (Multi-Centre, Multi-Vendor) Cardiac MRI Dataset\n",
    "    Classes: Background (0), RV (1), Myocardium (2), LV (3)\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, img_size=256, split='train', transform=None):\n",
    "        self.img_size = img_size\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        \n",
    "        # Check for npz format\n",
    "        npz_files = sorted(glob.glob(os.path.join(data_dir, f'{split}/*.npz')))\n",
    "        if len(npz_files) > 0:\n",
    "            self.samples = npz_files\n",
    "            self.format = 'npz'\n",
    "            print(f\"   Found {len(self.samples)} NPZ files\")\n",
    "        else:\n",
    "            # Check for h5 format\n",
    "            h5_files = sorted(glob.glob(os.path.join(data_dir, f'{split}/*.h5')))\n",
    "            if len(h5_files) > 0:\n",
    "                self.samples = h5_files\n",
    "                self.format = 'h5'\n",
    "                print(f\"   Found {len(self.samples)} H5 files\")\n",
    "            else:\n",
    "                # Check for NIfTI (raw M&Ms)\n",
    "                patient_dirs = sorted(glob.glob(os.path.join(data_dir, '*/sa.nii.gz')))\n",
    "                for sa_file in patient_dirs:\n",
    "                    gt_file = sa_file.replace('sa.nii.gz', 'sa_gt.nii.gz')\n",
    "                    if os.path.exists(gt_file):\n",
    "                        self.samples.append((sa_file, gt_file))\n",
    "                self.format = 'nifti'\n",
    "                print(f\"   Found {len(self.samples)} NIfTI pairs\")\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.format == 'npz':\n",
    "            data = np.load(self.samples[idx])\n",
    "            img = data['image'].astype(np.float32)\n",
    "            mask = data['label'].astype(np.int64)\n",
    "            \n",
    "        elif self.format == 'h5':\n",
    "            with h5py.File(self.samples[idx], 'r') as f:\n",
    "                img = f['image'][()].astype(np.float32)\n",
    "                mask = f['label'][()].astype(np.int64)\n",
    "                \n",
    "        elif self.format == 'nifti':\n",
    "            img_file, gt_file = self.samples[idx]\n",
    "            img = nib.load(img_file).get_fdata().astype(np.float32)\n",
    "            mask = nib.load(gt_file).get_fdata().astype(np.int64)\n",
    "            \n",
    "            # Take middle slice\n",
    "            if len(img.shape) >= 3:\n",
    "                mid_slice = img.shape[2] // 2\n",
    "                img = img[:, :, mid_slice]\n",
    "                mask = mask[:, :, mid_slice]\n",
    "        \n",
    "        # Normalize\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "        \n",
    "        # Resize\n",
    "        if img.shape[0] != self.img_size:\n",
    "            img = np.array(Image.fromarray(img).resize((self.img_size, self.img_size), Image.BILINEAR))\n",
    "            mask = np.array(Image.fromarray(mask.astype(np.uint8)).resize((self.img_size, self.img_size), Image.NEAREST))\n",
    "        \n",
    "        img = torch.from_numpy(img).unsqueeze(0).float()\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        return img, mask\n",
    "\n",
    "\n",
    "# Create dataset based on selection\n",
    "print(f\"\\nðŸ“Š Loading {DATASET} dataset...\")\n",
    "\n",
    "if DATASET == 'ACDC':\n",
    "    train_dataset = ACDCDataset(drive_path, img_size=config_data['img_size'], split='train')\n",
    "    val_dataset = ACDCDataset(drive_path, img_size=config_data['img_size'], split='val')\n",
    "elif DATASET == 'BraTS21':\n",
    "    train_dataset = BraTS21Dataset(drive_path, img_size=config_data['img_size'], split='train')\n",
    "    val_dataset = BraTS21Dataset(drive_path, img_size=config_data['img_size'], split='val')\n",
    "elif DATASET == 'MnM':\n",
    "    train_dataset = MnMDataset(drive_path, img_size=config_data['img_size'], split='train')\n",
    "    val_dataset = MnMDataset(drive_path, img_size=config_data['img_size'], split='val')\n",
    "\n",
    "# Fallback to synthetic if no data found\n",
    "if len(train_dataset) == 0:\n",
    "    print(\"âš ï¸ No data found! Using synthetic dataset for testing...\")\n",
    "    \n",
    "    class SyntheticMedicalDataset(Dataset):\n",
    "        def __init__(self, num_samples=500, img_size=256, num_classes=4, in_channels=1):\n",
    "            self.num_samples = num_samples\n",
    "            self.img_size = img_size\n",
    "            self.num_classes = num_classes\n",
    "            self.in_channels = in_channels\n",
    "            \n",
    "        def __len__(self):\n",
    "            return self.num_samples\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            np.random.seed(idx)\n",
    "            img = np.random.randn(self.in_channels, self.img_size, self.img_size).astype(np.float32)\n",
    "            mask = np.random.randint(0, self.num_classes, (self.img_size, self.img_size)).astype(np.int64)\n",
    "            return torch.from_numpy(img), torch.from_numpy(mask)\n",
    "    \n",
    "    train_dataset = SyntheticMedicalDataset(500, config_data['img_size'], config_data['num_classes'], config_data['in_channels'])\n",
    "    val_dataset = SyntheticMedicalDataset(100, config_data['img_size'], config_data['num_classes'], config_data['in_channels'])\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 4\n",
    "NUM_CLASSES = config_data['num_classes']\n",
    "IN_CHANNELS = config_data['in_channels']\n",
    "IMG_SIZE = config_data['img_size']\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"\\nâœ… Dataset loaded!\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"   Input channels: {IN_CHANNELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some training samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(4):\n",
    "    img, mask = train_dataset[i * 50]\n",
    "    \n",
    "    axes[0, i].imshow(img[0], cmap='gray')\n",
    "    axes[0, i].set_title(f'Image {i+1}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(mask, cmap='viridis')\n",
    "    axes[1, i].set_title(f'Mask {i+1}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Training Samples', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d968686b",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    # Dataset (auto-configured from above)\n",
    "    'dataset': DATASET,\n",
    "    'in_channels': IN_CHANNELS,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'img_size': IMG_SIZE,\n",
    "    'class_names': config_data['class_names'],\n",
    "    \n",
    "    # Model\n",
    "    'model': 'egm_net',           # 'egm_net', 'egm_net_lite', 'spectral_vmamba'\n",
    "    'base_channels': 64,\n",
    "    \n",
    "    # Training\n",
    "    'num_epochs': 100,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    \n",
    "    # Loss weights\n",
    "    'dice_weight': 1.0,\n",
    "    'ce_weight': 1.0,\n",
    "    'boundary_weight': 0.5,\n",
    "    \n",
    "    # Implicit representation\n",
    "    'num_points': 2048,           # Points sampled for implicit loss\n",
    "    'boundary_ratio': 0.5,        # Ratio of points near boundaries\n",
    "    \n",
    "    # Checkpointing\n",
    "    'save_every': 10,\n",
    "    'checkpoint_dir': f'./checkpoints_{DATASET}',\n",
    "    \n",
    "    # Early stopping\n",
    "    'patience': 20,\n",
    "}\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "print(\"ðŸ“‹ Training Configuration:\")\n",
    "print(f\"   Dataset: {config['dataset']}\")\n",
    "print(f\"   Classes: {config['class_names']}\")\n",
    "print(f\"   Model: {config['model']}\")\n",
    "print(f\"   Epochs: {config['num_epochs']}\")\n",
    "print(f\"   LR: {config['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model based on config\n",
    "print(f\"Creating {config['model']} model...\")\n",
    "print(f\"   Input channels: {config['in_channels']}\")\n",
    "print(f\"   Output classes: {config['num_classes']}\")\n",
    "\n",
    "if config['model'] == 'egm_net':\n",
    "    model = EGMNet(\n",
    "        in_channels=config['in_channels'],\n",
    "        num_classes=config['num_classes'],\n",
    "        img_size=config['img_size'],\n",
    "        base_channels=config['base_channels'],\n",
    "        num_stages=4,\n",
    "        encoder_depth=2\n",
    "    )\n",
    "elif config['model'] == 'egm_net_lite':\n",
    "    model = EGMNetLite(\n",
    "        in_channels=config['in_channels'],\n",
    "        num_classes=config['num_classes'],\n",
    "        img_size=config['img_size']\n",
    "    )\n",
    "else:  # spectral_vmamba\n",
    "    model = SpectralVMUNet(\n",
    "        in_channels=config['in_channels'],\n",
    "        out_channels=config['num_classes'],\n",
    "        img_size=config['img_size'],\n",
    "        base_channels=config['base_channels'],\n",
    "        num_stages=4\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nâœ… Model created!\")\n",
    "print(f\"   Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919f90c",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Loss Functions & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11420cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss for segmentation.\"\"\"\n",
    "    def __init__(self, smooth=1e-5):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # pred: (B, C, H, W) logits\n",
    "        # target: (B, H, W) class indices\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        num_classes = pred.shape[1]\n",
    "        \n",
    "        target_one_hot = F.one_hot(target, num_classes).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        intersection = (pred * target_one_hot).sum(dim=(2, 3))\n",
    "        union = pred.sum(dim=(2, 3)) + target_one_hot.sum(dim=(2, 3))\n",
    "        \n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1.0 - dice.mean()\n",
    "\n",
    "\n",
    "class BoundaryLoss(nn.Module):\n",
    "    \"\"\"Boundary-aware loss using Sobel edge detection.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Sobel kernels\n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)\n",
    "        self.register_buffer('sobel_x', sobel_x.view(1, 1, 3, 3))\n",
    "        self.register_buffer('sobel_y', sobel_y.view(1, 1, 3, 3))\n",
    "        \n",
    "    def get_boundaries(self, mask):\n",
    "        # mask: (B, H, W)\n",
    "        mask = mask.float().unsqueeze(1)\n",
    "        edge_x = F.conv2d(mask, self.sobel_x, padding=1)\n",
    "        edge_y = F.conv2d(mask, self.sobel_y, padding=1)\n",
    "        edges = torch.sqrt(edge_x**2 + edge_y**2)\n",
    "        return (edges > 0.5).float()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Get boundary regions\n",
    "        boundaries = self.get_boundaries(target)\n",
    "        \n",
    "        # Weight loss by boundary\n",
    "        pred_probs = F.softmax(pred, dim=1)\n",
    "        target_one_hot = F.one_hot(target, pred.shape[1]).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        # BCE at boundaries\n",
    "        boundary_loss = F.binary_cross_entropy(\n",
    "            pred_probs * boundaries, \n",
    "            target_one_hot * boundaries,\n",
    "            reduction='sum'\n",
    "        ) / (boundaries.sum() + 1e-6)\n",
    "        \n",
    "        return boundary_loss\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combined loss for EGM-Net training.\"\"\"\n",
    "    def __init__(self, dice_weight=1.0, ce_weight=1.0, boundary_weight=0.5, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_weight = ce_weight\n",
    "        self.boundary_weight = boundary_weight\n",
    "        \n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.boundary_loss = BoundaryLoss()\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        # For EGM-Net, outputs is a dict\n",
    "        if isinstance(outputs, dict):\n",
    "            pred = outputs['output']\n",
    "            coarse = outputs.get('coarse')\n",
    "            fine = outputs.get('fine')\n",
    "            \n",
    "            # Main loss\n",
    "            loss = self.dice_weight * self.dice_loss(pred, targets)\n",
    "            loss += self.ce_weight * self.ce_loss(pred, targets)\n",
    "            loss += self.boundary_weight * self.boundary_loss(pred, targets)\n",
    "            \n",
    "            # Auxiliary losses (coarse and fine branches)\n",
    "            if coarse is not None:\n",
    "                loss += 0.3 * self.ce_loss(coarse, targets)\n",
    "            if fine is not None:\n",
    "                loss += 0.3 * self.ce_loss(fine, targets)\n",
    "                \n",
    "            return loss\n",
    "        else:\n",
    "            # Standard output (SpectralVMUNet)\n",
    "            loss = self.dice_weight * self.dice_loss(outputs, targets)\n",
    "            loss += self.ce_weight * self.ce_loss(outputs, targets)\n",
    "            loss += self.boundary_weight * self.boundary_loss(outputs, targets)\n",
    "            return loss\n",
    "\n",
    "\n",
    "# Metrics\n",
    "def compute_dice(pred, target, num_classes):\n",
    "    \"\"\"Compute per-class Dice scores.\"\"\"\n",
    "    dice_scores = []\n",
    "    pred_classes = torch.argmax(pred, dim=1)\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        pred_c = (pred_classes == c).float()\n",
    "        target_c = (target == c).float()\n",
    "        \n",
    "        intersection = (pred_c * target_c).sum()\n",
    "        union = pred_c.sum() + target_c.sum()\n",
    "        \n",
    "        if union > 0:\n",
    "            dice = (2.0 * intersection) / union\n",
    "        else:\n",
    "            dice = torch.tensor(1.0)  # Both empty = perfect\n",
    "            \n",
    "        dice_scores.append(dice.item())\n",
    "    \n",
    "    return dice_scores\n",
    "\n",
    "\n",
    "def compute_iou(pred, target, num_classes):\n",
    "    \"\"\"Compute per-class IoU scores.\"\"\"\n",
    "    iou_scores = []\n",
    "    pred_classes = torch.argmax(pred, dim=1)\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        pred_c = (pred_classes == c).float()\n",
    "        target_c = (target == c).float()\n",
    "        \n",
    "        intersection = (pred_c * target_c).sum()\n",
    "        union = pred_c.sum() + target_c.sum() - intersection\n",
    "        \n",
    "        if union > 0:\n",
    "            iou = intersection / union\n",
    "        else:\n",
    "            iou = torch.tensor(1.0)\n",
    "            \n",
    "        iou_scores.append(iou.item())\n",
    "    \n",
    "    return iou_scores\n",
    "\n",
    "\n",
    "print(\"âœ… Loss functions and metrics defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4374d9",
   "metadata": {},
   "source": [
    "## ðŸš€ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f10568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training function\n",
    "def train_model(model, train_loader, val_loader, config, device):\n",
    "    \"\"\"Complete training loop with validation and checkpointing.\"\"\"\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = CombinedLoss(\n",
    "        dice_weight=config['dice_weight'],\n",
    "        ce_weight=config['ce_weight'],\n",
    "        boundary_weight=config['boundary_weight'],\n",
    "        num_classes=config['num_classes']\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler (cosine annealing)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=config['num_epochs'],\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_dice': [],\n",
    "        'val_iou': [],\n",
    "        'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    best_val_dice = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸš€ Starting Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        # =============== Training ===============\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\")\n",
    "        \n",
    "        for batch_idx, (images, masks) in enumerate(train_pbar):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # =============== Validation ===============\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_dice = []\n",
    "        all_iou = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Val]\"):\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get prediction tensor\n",
    "                pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "                \n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Compute metrics\n",
    "                dice_scores = compute_dice(pred, masks, config['num_classes'])\n",
    "                iou_scores = compute_iou(pred, masks, config['num_classes'])\n",
    "                \n",
    "                all_dice.append(np.mean(dice_scores[1:]))  # Exclude background\n",
    "                all_iou.append(np.mean(iou_scores[1:]))\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_dice = np.mean(all_dice)\n",
    "        avg_val_iou = np.mean(all_iou)\n",
    "        \n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_dice'].append(avg_val_dice)\n",
    "        history['val_iou'].append(avg_val_iou)\n",
    "        history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nðŸ“Š Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "        print(f\"   Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"   Val Loss:   {avg_val_loss:.4f}\")\n",
    "        print(f\"   Val Dice:   {avg_val_dice:.4f}\")\n",
    "        print(f\"   Val IoU:    {avg_val_iou:.4f}\")\n",
    "        print(f\"   LR:         {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # =============== Checkpointing ===============\n",
    "        # Save best model\n",
    "        if avg_val_dice > best_val_dice:\n",
    "            best_val_dice = avg_val_dice\n",
    "            patience_counter = 0\n",
    "            \n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_dice': best_val_dice,\n",
    "                'config': config,\n",
    "                'history': history\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(config['checkpoint_dir'], 'best_model.pth'))\n",
    "            print(f\"   âœ… New best model saved! (Dice: {best_val_dice:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Save periodic checkpoints\n",
    "        if (epoch + 1) % config['save_every'] == 0:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'history': history\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(config['checkpoint_dir'], f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= config['patience']:\n",
    "            print(f\"\\nâš ï¸ Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ðŸŽ‰ Training completed!\")\n",
    "    print(f\"   Best Val Dice: {best_val_dice:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "print(\"âœ… Training function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0bfd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸš€ START TRAINING\n",
    "history = train_model(model, train_loader, val_loader, config, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d9cd45",
   "metadata": {},
   "source": [
    "## ðŸ“Š Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da620a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training & Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Dice score\n",
    "axes[0, 1].plot(history['val_dice'], label='Val Dice', linewidth=2, color='green')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Dice Score')\n",
    "axes[0, 1].set_title('Validation Dice Score')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].axhline(y=max(history['val_dice']), color='r', linestyle='--', alpha=0.5, label=f\"Best: {max(history['val_dice']):.4f}\")\n",
    "\n",
    "# IoU score\n",
    "axes[1, 0].plot(history['val_iou'], label='Val IoU', linewidth=2, color='orange')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('IoU Score')\n",
    "axes[1, 0].set_title('Validation IoU Score')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 1].plot(history['learning_rate'], label='Learning Rate', linewidth=2, color='purple')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Learning Rate')\n",
    "axes[1, 1].set_title('Learning Rate Schedule')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.suptitle('Training Progress', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Training Summary:\")\n",
    "print(f\"   Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"   Final Val Loss:   {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"   Best Val Dice:    {max(history['val_dice']):.4f}\")\n",
    "print(f\"   Best Val IoU:     {max(history['val_iou']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288e93d",
   "metadata": {},
   "source": [
    "## ðŸ” Inference & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4617ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(os.path.join(config['checkpoint_dir'], 'best_model.pth'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"âœ… Loaded best model from epoch {checkpoint['epoch']} (Dice: {checkpoint['best_val_dice']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on validation set\n",
    "model.eval()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(4):\n",
    "        idx = i * (len(val_dataset) // 4)\n",
    "        img, mask = val_dataset[idx]\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get prediction\n",
    "        outputs = model(img)\n",
    "        pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "        pred_mask = torch.argmax(pred, dim=1)[0].cpu()\n",
    "        \n",
    "        # Get energy map if available\n",
    "        energy = outputs.get('energy', None)\n",
    "        \n",
    "        # Plot\n",
    "        axes[i, 0].imshow(img[0, 0].cpu(), cmap='gray')\n",
    "        axes[i, 0].set_title('Input Image')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(mask, cmap='viridis')\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(pred_mask, cmap='viridis')\n",
    "        axes[i, 2].set_title('Prediction')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        overlay = img[0, 0].cpu().numpy()\n",
    "        overlay = np.stack([overlay, overlay, overlay], axis=-1)\n",
    "        pred_np = pred_mask.numpy()\n",
    "        mask_np = mask.numpy()\n",
    "        \n",
    "        # Red for prediction, blue for ground truth\n",
    "        overlay[..., 0] = np.where(pred_np > 0, 1.0, overlay[..., 0])\n",
    "        overlay[..., 2] = np.where(mask_np > 0, 1.0, overlay[..., 2])\n",
    "        overlay = np.clip(overlay, 0, 1)\n",
    "        \n",
    "        axes[i, 3].imshow(overlay)\n",
    "        axes[i, 3].set_title('Overlay (Red=Pred, Blue=GT)')\n",
    "        axes[i, 3].axis('off')\n",
    "\n",
    "plt.suptitle('Validation Predictions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce67046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize EGM-Net branches (Coarse vs Fine)\n",
    "if config['model'] in ['egm_net', 'egm_net_lite']:\n",
    "    fig, axes = plt.subplots(3, 5, figsize=(20, 12))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(3):\n",
    "            idx = i * (len(val_dataset) // 3)\n",
    "            img, mask = val_dataset[idx]\n",
    "            img = img.unsqueeze(0).to(device)\n",
    "            \n",
    "            outputs = model(img)\n",
    "            \n",
    "            # Extract all outputs\n",
    "            final_pred = torch.argmax(outputs['output'], dim=1)[0].cpu()\n",
    "            coarse_pred = torch.argmax(outputs['coarse'], dim=1)[0].cpu()\n",
    "            fine_pred = torch.argmax(outputs['fine'], dim=1)[0].cpu()\n",
    "            energy = outputs['energy'][0, 0].cpu()\n",
    "            \n",
    "            # Plot\n",
    "            axes[i, 0].imshow(img[0, 0].cpu(), cmap='gray')\n",
    "            axes[i, 0].set_title('Input')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(energy, cmap='hot')\n",
    "            axes[i, 1].set_title('Energy Map')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            axes[i, 2].imshow(coarse_pred, cmap='viridis')\n",
    "            axes[i, 2].set_title('Coarse Branch')\n",
    "            axes[i, 2].axis('off')\n",
    "            \n",
    "            axes[i, 3].imshow(fine_pred, cmap='viridis')\n",
    "            axes[i, 3].set_title('Fine Branch')\n",
    "            axes[i, 3].axis('off')\n",
    "            \n",
    "            axes[i, 4].imshow(final_pred, cmap='viridis')\n",
    "            axes[i, 4].set_title('Final (Fused)')\n",
    "            axes[i, 4].axis('off')\n",
    "    \n",
    "    plt.suptitle('EGM-Net Branch Analysis\\\\n(Energy-Gated Fusion of Coarse + Fine)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('branch_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01eb4b2",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Resolution-Free Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate resolution-free inference (unique to EGM-Net)\n",
    "if config['model'] in ['egm_net', 'egm_net_lite']:\n",
    "    print(\"ðŸ”¬ Resolution-Free Inference Demo\")\n",
    "    print(\"   EGM-Net can render at ANY resolution without retraining!\")\n",
    "    \n",
    "    resolutions = [64, 128, 256, 512]\n",
    "    \n",
    "    # Get a sample image\n",
    "    img, mask = val_dataset[0]\n",
    "    img = img.unsqueeze(0).to(device)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, len(resolutions), figsize=(16, 8))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, res in enumerate(resolutions):\n",
    "            # Render at different resolutions\n",
    "            outputs = model(img, output_size=(res, res))\n",
    "            pred = torch.argmax(outputs['output'], dim=1)[0].cpu()\n",
    "            \n",
    "            # Also show input at same res for comparison\n",
    "            input_resized = F.interpolate(img, size=(res, res), mode='bilinear', align_corners=False)\n",
    "            \n",
    "            axes[0, i].imshow(input_resized[0, 0].cpu(), cmap='gray')\n",
    "            axes[0, i].set_title(f'Input {res}Ã—{res}')\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            axes[1, i].imshow(pred, cmap='viridis')\n",
    "            axes[1, i].set_title(f'Prediction {res}Ã—{res}')\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Resolution-Free Rendering\\\\n(Same model weights, different output resolutions)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('resolution_free.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\\\nâœ… Same model can output at 64Ã—64 to 512Ã—512 (or higher)!\")\n",
    "    print(\"   This is impossible with standard CNN decoders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f3f21",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Save & Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model to Google Drive\n",
    "from google.colab import drive\n",
    "\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Save to Drive\n",
    "    save_path = '/content/drive/MyDrive/EGM_Net_Models'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    final_checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': config,\n",
    "        'history': history,\n",
    "        'best_val_dice': max(history['val_dice'])\n",
    "    }\n",
    "    torch.save(final_checkpoint, os.path.join(save_path, 'egm_net_trained.pth'))\n",
    "    \n",
    "    # Also copy training curves\n",
    "    import shutil\n",
    "    shutil.copy('training_curves.png', save_path)\n",
    "    shutil.copy('predictions.png', save_path)\n",
    "    \n",
    "    print(f\"âœ… Model saved to Google Drive: {save_path}\")\n",
    "    print(f\"   - egm_net_trained.pth\")\n",
    "    print(f\"   - training_curves.png\")\n",
    "    print(f\"   - predictions.png\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not save to Google Drive: {e}\")\n",
    "    print(\"   Model is saved locally in ./checkpoints/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ea463",
   "metadata": {},
   "source": [
    "## ðŸ“Š Final Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b0f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on full validation set\n",
    "model.eval()\n",
    "\n",
    "all_dice_scores = []\n",
    "all_iou_scores = []\n",
    "all_hd95_scores = []  # Hausdorff Distance 95\n",
    "\n",
    "# Helper function for Hausdorff distance\n",
    "def compute_hausdorff_95(pred, target):\n",
    "    \"\"\"Compute 95th percentile Hausdorff distance.\"\"\"\n",
    "    from scipy.ndimage import distance_transform_edt\n",
    "    \n",
    "    pred_np = pred.numpy().astype(bool)\n",
    "    target_np = target.numpy().astype(bool)\n",
    "    \n",
    "    if pred_np.sum() == 0 or target_np.sum() == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Distance transforms\n",
    "    pred_dist = distance_transform_edt(~pred_np)\n",
    "    target_dist = distance_transform_edt(~target_np)\n",
    "    \n",
    "    # Get surface points\n",
    "    pred_surface = pred_np & (distance_transform_edt(pred_np) <= 1)\n",
    "    target_surface = target_np & (distance_transform_edt(target_np) <= 1)\n",
    "    \n",
    "    # Distances from pred surface to target, and vice versa\n",
    "    d_pred_to_target = target_dist[pred_surface]\n",
    "    d_target_to_pred = pred_dist[target_surface]\n",
    "    \n",
    "    if len(d_pred_to_target) == 0 or len(d_target_to_pred) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # 95th percentile\n",
    "    hd95 = max(np.percentile(d_pred_to_target, 95), np.percentile(d_target_to_pred, 95))\n",
    "    return hd95\n",
    "\n",
    "print(\"ðŸ” Running final evaluation...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "        pred_masks = torch.argmax(pred, dim=1)\n",
    "        \n",
    "        for b in range(images.shape[0]):\n",
    "            # Per-sample metrics\n",
    "            dice = compute_dice(pred[b:b+1], masks[b:b+1], config['num_classes'])\n",
    "            iou = compute_iou(pred[b:b+1], masks[b:b+1], config['num_classes'])\n",
    "            \n",
    "            all_dice_scores.append(np.mean(dice[1:]))  # Exclude background\n",
    "            all_iou_scores.append(np.mean(iou[1:]))\n",
    "            \n",
    "            # Hausdorff distance (for foreground)\n",
    "            try:\n",
    "                hd95 = compute_hausdorff_95(\n",
    "                    (pred_masks[b] > 0).cpu(),\n",
    "                    (masks[b] > 0).cpu()\n",
    "                )\n",
    "                all_hd95_scores.append(hd95)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š FINAL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Metric':<20} {'Mean':<12} {'Std':<12}\")\n",
    "print(\"-\"*44)\n",
    "print(f\"{'Dice Score':<20} {np.mean(all_dice_scores):.4f}       {np.std(all_dice_scores):.4f}\")\n",
    "print(f\"{'IoU Score':<20} {np.mean(all_iou_scores):.4f}       {np.std(all_iou_scores):.4f}\")\n",
    "if all_hd95_scores:\n",
    "    print(f\"{'HD95 (mm)':<20} {np.mean(all_hd95_scores):.2f}         {np.std(all_hd95_scores):.2f}\")\n",
    "print(\"-\"*44)\n",
    "print(f\"\\nTotal validation samples: {len(all_dice_scores)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e47b55f",
   "metadata": {},
   "source": [
    "## 3. Test Monogenic Signal Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea0165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test image with edges\n",
    "def create_test_image(size=256):\n",
    "    \"\"\"Create synthetic medical-like image with organs.\"\"\"\n",
    "    img = torch.zeros(1, 1, size, size)\n",
    "    \n",
    "    # Add circular \"organ\"\n",
    "    y, x = torch.meshgrid(torch.arange(size), torch.arange(size), indexing='ij')\n",
    "    center1 = (size // 2, size // 2)\n",
    "    radius1 = size // 4\n",
    "    mask1 = ((x - center1[0])**2 + (y - center1[1])**2) < radius1**2\n",
    "    img[0, 0, mask1] = 0.7\n",
    "    \n",
    "    # Add smaller \"tumor\"\n",
    "    center2 = (size // 2 + 30, size // 2 - 20)\n",
    "    radius2 = size // 10\n",
    "    mask2 = ((x - center2[0])**2 + (y - center2[1])**2) < radius2**2\n",
    "    img[0, 0, mask2] = 1.0\n",
    "    \n",
    "    # Add noise\n",
    "    img = img + 0.05 * torch.randn_like(img)\n",
    "    \n",
    "    return img, mask1.float(), mask2.float()\n",
    "\n",
    "# Create test image\n",
    "test_img, organ_mask, tumor_mask = create_test_image(256)\n",
    "print(f\"Test image shape: {test_img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18065bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Monogenic Energy Extraction\n",
    "energy_extractor = EnergyMap(normalize=True, smoothing_sigma=1.0)\n",
    "energy, mono_out = energy_extractor(test_img)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].imshow(test_img[0, 0], cmap='gray')\n",
    "axes[0, 0].set_title('Input Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(energy[0, 0].detach(), cmap='hot')\n",
    "axes[0, 1].set_title('Energy Map (Edges)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(mono_out['phase'][0, 0].detach(), cmap='twilight')\n",
    "axes[0, 2].set_title('Phase')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(mono_out['orientation'][0, 0].detach(), cmap='hsv')\n",
    "axes[1, 0].set_title('Orientation')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(mono_out['riesz_x'][0, 0].detach(), cmap='RdBu')\n",
    "axes[1, 1].set_title('Riesz X Component')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(mono_out['riesz_y'][0, 0].detach(), cmap='RdBu')\n",
    "axes[1, 2].set_title('Riesz Y Component')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Monogenic Signal Decomposition', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Monogenic processing works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee600ecb",
   "metadata": {},
   "source": [
    "## 4. Test Gabor Basis vs Fourier Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gabor_implicit import GaborBasis, FourierFeatures\n",
    "\n",
    "# Create coordinate grid\n",
    "size = 128\n",
    "y = torch.linspace(-1, 1, size)\n",
    "x = torch.linspace(-1, 1, size)\n",
    "yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "coords = torch.stack([xx, yy], dim=-1).view(1, -1, 2)  # (1, size*size, 2)\n",
    "\n",
    "# Compare Gabor vs Fourier\n",
    "gabor = GaborBasis(input_dim=2, num_frequencies=32)\n",
    "fourier = FourierFeatures(input_dim=2, num_frequencies=32, scale=10.0)\n",
    "\n",
    "gabor_features = gabor(coords)\n",
    "fourier_features = fourier(coords)\n",
    "\n",
    "print(f\"Gabor features shape: {gabor_features.shape}\")\n",
    "print(f\"Fourier features shape: {fourier_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7aaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first few basis functions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(4):\n",
    "    # Gabor\n",
    "    gabor_vis = gabor_features[0, :, i].view(size, size).detach().numpy()\n",
    "    axes[0, i].imshow(gabor_vis, cmap='RdBu', vmin=-1, vmax=1)\n",
    "    axes[0, i].set_title(f'Gabor Basis {i+1}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Fourier\n",
    "    fourier_vis = fourier_features[0, :, i].view(size, size).detach().numpy()\n",
    "    axes[1, i].imshow(fourier_vis, cmap='RdBu', vmin=-1, vmax=1)\n",
    "    axes[1, i].set_title(f'Fourier Basis {i+1}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Gabor\\n(Localized)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Fourier\\n(Global)', fontsize=12)\n",
    "\n",
    "plt.suptitle('Gabor vs Fourier Basis Functions\\n(Gabor is localized â†’ No Gibbs ringing)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ef9df",
   "metadata": {},
   "source": [
    "## 5. Create and Analyze Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c426ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create EGM-Net models\n",
    "print(\"Creating models...\")\n",
    "\n",
    "# Full model\n",
    "egm_net = EGMNet(\n",
    "    in_channels=1,\n",
    "    num_classes=3,\n",
    "    img_size=256,\n",
    "    base_channels=64,\n",
    "    num_stages=4,\n",
    "    encoder_depth=2\n",
    ").to(device)\n",
    "\n",
    "# Lite model\n",
    "egm_lite = EGMNetLite(\n",
    "    in_channels=1,\n",
    "    num_classes=3,\n",
    "    img_size=256\n",
    ").to(device)\n",
    "\n",
    "# Spectral Mamba (comparison)\n",
    "spec_mamba = SpectralVMUNet(\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    img_size=256,\n",
    "    base_channels=64,\n",
    "    num_stages=4\n",
    ").to(device)\n",
    "\n",
    "print(\"\\nðŸ“Š Model Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "models = {\n",
    "    'EGM-Net Full': egm_net,\n",
    "    'EGM-Net Lite': egm_lite,\n",
    "    'SpectralVMUNet': spec_mamba\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name:20s}: {params:,} parameters ({params/1e6:.2f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeffa14a",
   "metadata": {},
   "source": [
    "## 6. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c023bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "test_input = torch.randn(2, 1, 256, 256).to(device)\n",
    "\n",
    "print(\"Testing forward pass...\")\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # EGM-Net\n",
    "    egm_out = egm_net(test_input)\n",
    "    print(f\"\\nðŸ”¹ EGM-Net Output:\")\n",
    "    for k, v in egm_out.items():\n",
    "        print(f\"   {k}: {v.shape}\")\n",
    "    \n",
    "    # SpectralVMUNet\n",
    "    spec_out = spec_mamba(test_input)\n",
    "    print(f\"\\nðŸ”¹ SpectralVMUNet Output: {spec_out.shape}\")\n",
    "\n",
    "print(\"\\nâœ… Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4b153",
   "metadata": {},
   "source": [
    "## 7. Test Resolution-Free Inference (Unique to EGM-Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb14e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EGM-Net can query at arbitrary coordinates!\n",
    "print(\"Testing Resolution-Free Inference...\")\n",
    "\n",
    "# Create query points (random locations)\n",
    "num_points = 10000\n",
    "random_coords = torch.rand(1, num_points, 2).to(device) * 2 - 1  # [-1, 1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Query at random points\n",
    "    point_output = egm_net.query_points(test_input[:1], random_coords)\n",
    "    \n",
    "print(f\"Query coordinates: {random_coords.shape}\")\n",
    "print(f\"Point outputs: {point_output.shape}\")\n",
    "print(\"\\nâœ… Resolution-free inference works!\")\n",
    "print(\"   â†’ You can zoom into boundaries at ANY resolution!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate resolution-free: render at different resolutions\n",
    "resolutions = [64, 128, 256, 512]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, res in enumerate(resolutions):\n",
    "        # Render at this resolution\n",
    "        output = egm_net(test_input[:1], output_size=(res, res))\n",
    "        pred = torch.argmax(output['output'], dim=1)[0].cpu().numpy()\n",
    "        \n",
    "        axes[idx].imshow(pred, cmap='viridis')\n",
    "        axes[idx].set_title(f'{res}Ã—{res}')\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Resolution-Free Rendering (Same model, different output sizes)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04640dc",
   "metadata": {},
   "source": [
    "## 8. Visualize Energy-Gated Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01794b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dual-branch architecture\n",
    "with torch.no_grad():\n",
    "    outputs = egm_net(test_input[:1])\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Input\n",
    "axes[0, 0].imshow(test_input[0, 0].cpu(), cmap='gray')\n",
    "axes[0, 0].set_title('Input Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Energy Map\n",
    "axes[0, 1].imshow(outputs['energy'][0, 0].cpu(), cmap='hot')\n",
    "axes[0, 1].set_title('Energy Map (Edge Detection)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Coarse Branch\n",
    "coarse_pred = torch.argmax(outputs['coarse'], dim=1)[0].cpu()\n",
    "axes[0, 2].imshow(coarse_pred, cmap='viridis')\n",
    "axes[0, 2].set_title('Coarse Branch (Smooth)')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Fine Branch\n",
    "fine_pred = torch.argmax(outputs['fine'], dim=1)[0].cpu()\n",
    "axes[1, 0].imshow(fine_pred, cmap='viridis')\n",
    "axes[1, 0].set_title('Fine Branch (Sharp)')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Final Output\n",
    "final_pred = torch.argmax(outputs['output'], dim=1)[0].cpu()\n",
    "axes[1, 1].imshow(final_pred, cmap='viridis')\n",
    "axes[1, 1].set_title('Final Output (Fused)')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Difference\n",
    "diff = (fine_pred != coarse_pred).float()\n",
    "axes[1, 2].imshow(diff, cmap='Reds')\n",
    "axes[1, 2].set_title('Difference (Fine vs Coarse)')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle('EGM-Net Dual-Branch Architecture', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c51b26",
   "metadata": {},
   "source": [
    "## 9. Quick Training Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d86f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_egm import EGMNetTrainer, create_dummy_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create small dummy dataset\n",
    "print(\"Creating dummy dataset...\")\n",
    "dataset = create_dummy_dataset(num_samples=16, img_size=256, num_classes=3)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Training config\n",
    "config = {\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'num_epochs': 2,\n",
    "    'num_points': 1024,\n",
    "    'boundary_ratio': 0.5,\n",
    "    'checkpoint_dir': './checkpoints_demo'\n",
    "}\n",
    "\n",
    "# Use lite model for faster training\n",
    "model = EGMNetLite(in_channels=1, num_classes=3, img_size=256)\n",
    "print(f\"Model: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b6e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for a few epochs\n",
    "print(\"\\nStarting training demo...\")\n",
    "trainer = EGMNetTrainer(model, config, device=device)\n",
    "trainer.train(train_loader, num_epochs=2)\n",
    "\n",
    "print(\"\\nâœ… Training demo completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c9e44f",
   "metadata": {},
   "source": [
    "## 10. Inference Speed Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_model(model, input_tensor, num_runs=50, warmup=10):\n",
    "    \"\"\"Benchmark inference speed.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(input_tensor)\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            start = time.time()\n",
    "            _ = model(input_tensor)\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "    \n",
    "    return np.mean(times) * 1000, np.std(times) * 1000  # ms\n",
    "\n",
    "# Benchmark\n",
    "print(\"Benchmarking inference speed...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "test_input = torch.randn(1, 1, 256, 256).to(device)\n",
    "\n",
    "for name, model in [('EGM-Net Full', egm_net), ('EGM-Net Lite', egm_lite)]:\n",
    "    mean_time, std_time = benchmark_model(model, test_input)\n",
    "    fps = 1000 / mean_time\n",
    "    print(f\"{name:20s}: {mean_time:.2f} Â± {std_time:.2f} ms ({fps:.1f} FPS)\")\n",
    "\n",
    "print(\"\\nâœ… Benchmark completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17a31bd",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb75fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    EGM-NET ARCHITECTURE SUMMARY                       â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  ðŸ”¬ KEY INNOVATIONS:                                                  â•‘\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  1. MONOGENIC ENERGY GATING                                          â•‘\n",
    "â•‘     â€¢ Physics-based edge detection (Riesz Transform)                 â•‘\n",
    "â•‘     â€¢ Automatically focuses on boundary regions                      â•‘\n",
    "â•‘     â€¢ Suppresses artifacts in flat regions                           â•‘\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  2. GABOR BASIS (vs Fourier)                                         â•‘\n",
    "â•‘     â€¢ Localized oscillations (Gaussian Ã— sin)                        â•‘\n",
    "â•‘     â€¢ NO Gibbs ringing artifacts                                     â•‘\n",
    "â•‘     â€¢ Sharp edges remain clean                                       â•‘\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  3. DUAL-PATH ARCHITECTURE                                           â•‘\n",
    "â•‘     â€¢ Coarse Branch: Smooth body regions (Conv decoder)              â•‘\n",
    "â•‘     â€¢ Fine Branch: Sharp boundaries (Gabor Implicit)                 â•‘\n",
    "â•‘     â€¢ Energy-gated fusion: Best of both worlds                       â•‘\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  4. RESOLUTION-FREE INFERENCE                                        â•‘\n",
    "â•‘     â€¢ Query at ANY coordinate â†’ Infinite zoom                        â•‘\n",
    "â•‘     â€¢ No retraining needed for different resolutions                 â•‘\n",
    "â•‘     â€¢ Perfect for high-resolution medical imaging                    â•‘\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  5. MAMBA ENCODER                                                    â•‘\n",
    "â•‘     â€¢ O(N) complexity (vs O(NÂ²) for Transformers)                    â•‘\n",
    "â•‘     â€¢ Global context awareness                                       â•‘\n",
    "â•‘     â€¢ Efficient for large images                                     â•‘\n",
    "â•‘                                                                       â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  ðŸ“Š MODEL SIZES:                                                      â•‘\n",
    "â•‘     â€¢ EGM-Net Full:  ~9.13M parameters                               â•‘\n",
    "â•‘     â€¢ EGM-Net Lite:  ~635K parameters                                â•‘\n",
    "â•‘     â€¢ SpectralVMUNet: ~10.31M parameters                             â•‘\n",
    "â•‘                                                                       â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36957ab8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“š Next Steps\n",
    "\n",
    "1. **Train on real data**: Replace dummy dataset with medical imaging dataset (e.g., Synapse, ACDC)\n",
    "2. **Tune hyperparameters**: Adjust `num_frequencies`, `boundary_ratio`, learning rate\n",
    "3. **Evaluate metrics**: Dice score, IoU, Hausdorff distance\n",
    "4. **Ablation study**: Compare Gabor vs Fourier, with/without energy gating\n",
    "\n",
    "---\n",
    "\n",
    "**Repository**: https://github.com/QuocKhanhLuong/FourierNetwork"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
