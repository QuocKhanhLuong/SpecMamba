{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a590fe",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678d1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/QuocKhanhLuong/FourierNetwork.git\n",
    "%cd FourierNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f537f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567d48f",
   "metadata": {},
   "source": [
    "## 2. Import Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our models\n",
    "from monogenic import EnergyMap, MonogenicSignal, BoundaryDetector\n",
    "from gabor_implicit import GaborBasis, GaborNet, ImplicitSegmentationHead\n",
    "from egm_net import EGMNet, EGMNetLite\n",
    "from spectral_mamba import SpectralVMUNet\n",
    "\n",
    "print(\"‚úÖ All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1eae7",
   "metadata": {},
   "source": [
    "## üè• REAL DATA TRAINING\n",
    "\n",
    "### ACDC Dataset (Automated Cardiac Diagnosis Challenge)\n",
    "- **Modality**: Cine-MRI (Cardiac)\n",
    "- **150 patients**: 100 training + 50 testing\n",
    "- **Classes**: 4 (Background, RV, Myocardium, LV)\n",
    "- **Frames**: ED (End Diastole) + ES (End Systole) per patient\n",
    "- **Total slices**: ~1900 (varies 6-18 slices per volume)\n",
    "\n",
    "### BraTS21 Dataset (Brain Tumor Segmentation)\n",
    "- **Modality**: Multi-modal MRI (T1, T1ce, T2, FLAIR)\n",
    "- **Classes**: 4 (Background, Necrotic/Non-enhancing, Edema, Enhancing)\n",
    "\n",
    "### M&Ms Dataset (Multi-Centre, Multi-Vendor)\n",
    "- **Modality**: Cardiac MRI from 4 different vendors\n",
    "- **Classes**: 4 (Background, RV, Myocardium, LV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03af621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download dataset from Google Drive\n",
    "import os\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q gdown nibabel scikit-image\n",
    "\n",
    "# ============================================================\n",
    "# üîß SELECT YOUR DATASET HERE\n",
    "# ============================================================\n",
    "DATASET = 'ACDC'  # Options: 'ACDC', 'BraTS21', 'MnM'\n",
    "\n",
    "# ============================================================\n",
    "# Google Drive folder IDs\n",
    "# ============================================================\n",
    "# N·∫øu b·∫°n c√≥ 2 links ri√™ng cho training v√† testing, ƒëi·ªÅn v√†o ƒë√¢y:\n",
    "DRIVE_FOLDERS = {\n",
    "    'ACDC': {\n",
    "        'all': '1EelzBVjIoDQ4uzt0_2JzmF_PuUHsD93e',  # Folder ch·ª©a c·∫£ training v√† testing\n",
    "        # N·∫øu training/testing t√°ch ri√™ng, uncomment d∆∞·ªõi ƒë√¢y:\n",
    "        # 'training': 'YOUR_TRAINING_FOLDER_ID_HERE',\n",
    "        # 'testing': 'YOUR_TESTING_FOLDER_ID_HERE'\n",
    "    },\n",
    "    'BraTS21': {\n",
    "        'all': '1m7b5u_6cEj9PbqzZQDNoDonnXIowBCY2'\n",
    "    },\n",
    "    'MnM': {\n",
    "        'all': '1DpW8ucYE17Tj8iMlsAev_yM6TaZZmWYj'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dataset configurations\n",
    "DATASET_CONFIG = {\n",
    "    'ACDC': {\n",
    "        'num_classes': 4,\n",
    "        'class_names': ['Background', 'RV', 'Myocardium', 'LV'],\n",
    "        'in_channels': 1,\n",
    "        'img_size': 224,\n",
    "        # ACDC: 100 training patients (patient001-100), 50 testing patients (patient101-150)\n",
    "        # Each patient has ED + ES frames, each frame has 6-18 slices (avg ~9)\n",
    "        # Total expected: ~1900-2000 slices\n",
    "    },\n",
    "    'BraTS21': {\n",
    "        'num_classes': 4,\n",
    "        'class_names': ['Background', 'NCR/NET', 'Edema', 'Enhancing'],\n",
    "        'in_channels': 4,\n",
    "        'img_size': 224,\n",
    "    },\n",
    "    'MnM': {\n",
    "        'num_classes': 4,\n",
    "        'class_names': ['Background', 'RV', 'Myocardium', 'LV'],\n",
    "        'in_channels': 1,\n",
    "        'img_size': 224,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Paths\n",
    "RAW_DATA_DIR = f'./data/{DATASET}'\n",
    "PREPROCESSED_DIR = f'./preprocessed_data/{DATASET}'\n",
    "\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Download from Google Drive\n",
    "folder_config = DRIVE_FOLDERS[DATASET]\n",
    "print(f\"üì• Step 1: Downloading {DATASET} from Google Drive...\")\n",
    "\n",
    "# Check if training and testing are separate folders\n",
    "if 'training' in folder_config and 'testing' in folder_config:\n",
    "    # Separate links for training and testing\n",
    "    print(\"   Mode: Separate training and testing folders\")\n",
    "    \n",
    "    training_id = folder_config['training']\n",
    "    testing_id = folder_config['testing']\n",
    "    \n",
    "    print(f\"\\n   üì¶ Downloading TRAINING data...\")\n",
    "    !gdown --folder \"https://drive.google.com/drive/folders/{training_id}\" -O {RAW_DATA_DIR}/training --remaining-ok\n",
    "    \n",
    "    print(f\"\\n   üì¶ Downloading TESTING data...\")\n",
    "    !gdown --folder \"https://drive.google.com/drive/folders/{testing_id}\" -O {RAW_DATA_DIR}/testing --remaining-ok\n",
    "else:\n",
    "    # Single folder containing all data\n",
    "    folder_id = folder_config['all']\n",
    "    print(f\"   Mode: Single folder (should contain training + testing subfolders)\")\n",
    "    print(f\"   Folder ID: {folder_id}\")\n",
    "    \n",
    "    !gdown --folder \"https://drive.google.com/drive/folders/{folder_id}\" -O {RAW_DATA_DIR} --remaining-ok\n",
    "\n",
    "print(f\"\\n‚úÖ Download complete!\")\n",
    "\n",
    "# Get config\n",
    "config_data = DATASET_CONFIG[DATASET]\n",
    "print(f\"\\nüìä Dataset: {DATASET}\")\n",
    "print(f\"   Expected slices: ~1900 for ACDC (100 training + 50 testing patients)\")\n",
    "print(f\"   Classes: {config_data['class_names']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Check downloaded data structure\n",
    "import glob\n",
    "\n",
    "print(f\"üìÇ Step 2: Checking downloaded data structure...\")\n",
    "print(f\"   Root: {RAW_DATA_DIR}\")\n",
    "print()\n",
    "\n",
    "# Show folder structure\n",
    "def show_tree(path, prefix=\"\", max_depth=3, current_depth=0):\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        entries = sorted(os.listdir(path))\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "    dirs = [e for e in entries if os.path.isdir(os.path.join(path, e))]\n",
    "    files = [e for e in entries if os.path.isfile(os.path.join(path, e))]\n",
    "    \n",
    "    # Show directories\n",
    "    for d in dirs[:10]:  # Max 10 folders\n",
    "        print(f\"{prefix}üìÅ {d}/\")\n",
    "        show_tree(os.path.join(path, d), prefix + \"   \", max_depth, current_depth + 1)\n",
    "    if len(dirs) > 10:\n",
    "        print(f\"{prefix}   ... and {len(dirs)-10} more folders\")\n",
    "    \n",
    "    # Show files (first 5 only)\n",
    "    for f in files[:5]:\n",
    "        print(f\"{prefix}üìÑ {f}\")\n",
    "    if len(files) > 5:\n",
    "        print(f\"{prefix}   ... and {len(files)-5} more files\")\n",
    "\n",
    "show_tree(RAW_DATA_DIR)\n",
    "\n",
    "# Count patients\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"üìä ACDC Patient Count Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find all patient folders recursively\n",
    "all_patients = []\n",
    "for root, dirs, files in os.walk(RAW_DATA_DIR):\n",
    "    for d in dirs:\n",
    "        if d.startswith('patient'):\n",
    "            all_patients.append((d, root))\n",
    "\n",
    "# Separate training vs testing\n",
    "training_patients = [p for p, path in all_patients if 'training' in path.lower()]\n",
    "testing_patients = [p for p, path in all_patients if 'testing' in path.lower()]\n",
    "unknown_patients = [p for p, path in all_patients if 'training' not in path.lower() and 'testing' not in path.lower()]\n",
    "\n",
    "# Also check by patient number\n",
    "patients_by_number = {}\n",
    "for p, path in all_patients:\n",
    "    try:\n",
    "        num = int(p.replace('patient', ''))\n",
    "        patients_by_number[num] = path\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "training_by_num = [n for n in patients_by_number if n <= 100]\n",
    "testing_by_num = [n for n in patients_by_number if n > 100]\n",
    "\n",
    "print(f\"\\nüìç By folder structure:\")\n",
    "print(f\"   Training folder patients: {len(training_patients)}\")\n",
    "print(f\"   Testing folder patients: {len(testing_patients)}\")\n",
    "print(f\"   Unknown location: {len(unknown_patients)}\")\n",
    "\n",
    "print(f\"\\nüìç By patient number (ACDC convention):\")\n",
    "print(f\"   Training (001-100): {len(training_by_num)} patients\")\n",
    "print(f\"   Testing (101-150): {len(testing_by_num)} patients\")\n",
    "print(f\"   Total unique patients: {len(patients_by_number)}\")\n",
    "\n",
    "# Expected vs actual\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"‚ö†Ô∏è DIAGNOSTIC\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(patients_by_number) >= 140:\n",
    "    print(f\"‚úÖ Looks good! Found {len(patients_by_number)} patients (expected ~150)\")\n",
    "elif len(patients_by_number) >= 90:\n",
    "    print(f\"‚ö†Ô∏è Partial data: Found {len(patients_by_number)} patients\")\n",
    "    print(f\"   Missing: {'training' if len(training_by_num) < 90 else 'testing'} data\")\n",
    "elif len(patients_by_number) < 60:\n",
    "    print(f\"‚ùå Incomplete: Only {len(patients_by_number)} patients found!\")\n",
    "    print(f\"   Expected: 150 (100 training + 50 testing)\")\n",
    "    print(f\"\\nüîß Possible fixes:\")\n",
    "    print(f\"   1. Check if your Drive folder has training/ and testing/ subfolders\")\n",
    "    print(f\"   2. If they're separate links, update DRIVE_FOLDERS above with both IDs\")\n",
    "    print(f\"   3. Make sure sharing is enabled on the Drive folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e088d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocess ACDC data (NIfTI ‚Üí .npy)\n",
    "# CH·ªà x·ª≠ l√Ω folder TRAINING (100 patients) - chia 80/20 cho train/val\n",
    "import configparser\n",
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_single_patient_acdc(patient_path, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Process one ACDC patient.\n",
    "    \n",
    "    ACDC structure per patient:\n",
    "    - Info.cfg: contains ED and ES frame numbers\n",
    "    - patient001_frame01.nii(.gz): 3D volume at frame 01\n",
    "    - patient001_frame01_gt.nii(.gz): Ground truth segmentation\n",
    "    \n",
    "    Returns:\n",
    "        List of (volume_3d, mask_3d, volume_id) tuples\n",
    "    \"\"\"\n",
    "    patient_folder = os.path.basename(patient_path)\n",
    "    info_cfg_path = os.path.join(patient_path, 'Info.cfg')\n",
    "    \n",
    "    if not os.path.exists(info_cfg_path):\n",
    "        # Try to find frames manually\n",
    "        gt_files = glob.glob(os.path.join(patient_path, '*_gt.nii*'))\n",
    "        if not gt_files:\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        for gt_file in gt_files:\n",
    "            img_file = gt_file.replace('_gt.nii', '.nii')\n",
    "            if os.path.exists(img_file):\n",
    "                frame_name = os.path.basename(gt_file).split('_gt')[0].split('_')[-1]\n",
    "                try:\n",
    "                    img_data = nib.load(img_file).get_fdata()\n",
    "                    mask_data = nib.load(gt_file).get_fdata()\n",
    "                    volume, mask = process_volume(img_data, mask_data, target_size)\n",
    "                    volume_id = f\"{patient_folder}_{frame_name}\"\n",
    "                    results.append((volume, mask, volume_id))\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error: {e}\")\n",
    "        return results\n",
    "    \n",
    "    # Read Info.cfg to get ED and ES frame numbers\n",
    "    try:\n",
    "        parser = configparser.ConfigParser()\n",
    "        with open(info_cfg_path, 'r') as f:\n",
    "            config_string = '[DEFAULT]\\n' + f.read()\n",
    "        parser.read_string(config_string)\n",
    "        ed_frame = int(parser['DEFAULT']['ED'])\n",
    "        es_frame = int(parser['DEFAULT']['ES'])\n",
    "    except Exception as e:\n",
    "        print(f\"  Error reading Info.cfg for {patient_folder}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for frame_num, frame_name in [(ed_frame, 'ED'), (es_frame, 'ES')]:\n",
    "        img_path = None\n",
    "        mask_path = None\n",
    "        \n",
    "        for ext in ['.nii.gz', '.nii']:\n",
    "            test_img = os.path.join(patient_path, f'{patient_folder}_frame{frame_num:02d}{ext}')\n",
    "            test_mask = os.path.join(patient_path, f'{patient_folder}_frame{frame_num:02d}_gt{ext}')\n",
    "            \n",
    "            if os.path.exists(test_img) and os.path.exists(test_mask):\n",
    "                img_path = test_img\n",
    "                mask_path = test_mask\n",
    "                break\n",
    "        \n",
    "        if img_path is None:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            img_data = nib.load(img_path).get_fdata()\n",
    "            mask_data = nib.load(mask_path).get_fdata()\n",
    "            volume, mask = process_volume(img_data, mask_data, target_size)\n",
    "            volume_id = f\"{patient_folder}_{frame_name}\"\n",
    "            results.append((volume, mask, volume_id))\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {patient_folder} {frame_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def process_volume(img_data, mask_data, target_size):\n",
    "    \"\"\"Process a 3D volume: resize each slice, normalize.\"\"\"\n",
    "    num_slices = img_data.shape[2]\n",
    "    \n",
    "    resized_img = np.zeros((target_size[0], target_size[1], num_slices), dtype=np.float32)\n",
    "    resized_mask = np.zeros((target_size[0], target_size[1], num_slices), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(num_slices):\n",
    "        resized_img[:, :, i] = resize(\n",
    "            img_data[:, :, i], target_size, \n",
    "            order=1, preserve_range=True, anti_aliasing=True, mode='reflect'\n",
    "        )\n",
    "        resized_mask[:, :, i] = resize(\n",
    "            mask_data[:, :, i], target_size, \n",
    "            order=0, preserve_range=True, anti_aliasing=False, mode='reflect'\n",
    "        )\n",
    "    \n",
    "    max_val = resized_img.max()\n",
    "    if max_val > 0:\n",
    "        resized_img = resized_img / max_val\n",
    "    \n",
    "    return resized_img, resized_mask\n",
    "\n",
    "\n",
    "def preprocess_acdc_training_only(input_dir, output_dir, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Preprocess ONLY training folder from ACDC.\n",
    "    Training: 100 patients ‚Üí split 80/20 for train/val\n",
    "    Testing folder s·∫Ω d√πng ri√™ng ƒë·ªÉ final evaluation\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    volumes_dir = os.path.join(output_dir, 'volumes')\n",
    "    masks_dir = os.path.join(output_dir, 'masks')\n",
    "    os.makedirs(volumes_dir, exist_ok=True)\n",
    "    os.makedirs(masks_dir, exist_ok=True)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # T√åM TRAINING FOLDER\n",
    "    # =========================================================================\n",
    "    training_dir = None\n",
    "    \n",
    "    # Check common locations\n",
    "    possible_paths = [\n",
    "        os.path.join(input_dir, 'training'),\n",
    "        os.path.join(input_dir, 'Training'),\n",
    "        os.path.join(input_dir, 'TRAINING'),\n",
    "        input_dir  # Fallback: maybe patients are directly in root\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            # Check if this folder contains patient subfolders\n",
    "            patient_dirs = [d for d in os.listdir(path) if d.startswith('patient') and os.path.isdir(os.path.join(path, d))]\n",
    "            if len(patient_dirs) > 0:\n",
    "                training_dir = path\n",
    "                print(f\"‚úÖ Found training data at: {path}\")\n",
    "                print(f\"   Contains {len(patient_dirs)} patient folders\")\n",
    "                break\n",
    "    \n",
    "    if training_dir is None:\n",
    "        # Deep search\n",
    "        for root, dirs, files in os.walk(input_dir):\n",
    "            if 'training' in root.lower():\n",
    "                patient_dirs = [d for d in dirs if d.startswith('patient')]\n",
    "                if len(patient_dirs) > 0:\n",
    "                    training_dir = root\n",
    "                    print(f\"‚úÖ Found training data at: {root}\")\n",
    "                    print(f\"   Contains {len(patient_dirs)} patient folders\")\n",
    "                    break\n",
    "    \n",
    "    if training_dir is None:\n",
    "        print(f\"‚ùå Cannot find training folder in {input_dir}\")\n",
    "        print(f\"   Looking for folder containing patient* subfolders...\")\n",
    "        return 0, 0, []\n",
    "    \n",
    "    # =========================================================================\n",
    "    # GET ALL TRAINING PATIENTS\n",
    "    # =========================================================================\n",
    "    patient_folders = sorted([\n",
    "        os.path.join(training_dir, d) \n",
    "        for d in os.listdir(training_dir) \n",
    "        if d.startswith('patient') and os.path.isdir(os.path.join(training_dir, d))\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\nüìä Training patients found: {len(patient_folders)}\")\n",
    "    if len(patient_folders) > 0:\n",
    "        print(f\"   First: {os.path.basename(patient_folders[0])}\")\n",
    "        print(f\"   Last: {os.path.basename(patient_folders[-1])}\")\n",
    "    \n",
    "    if len(patient_folders) == 0:\n",
    "        print(\"‚ùå No patient folders found!\")\n",
    "        return 0, 0, []\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PROCESS ALL PATIENTS\n",
    "    # =========================================================================\n",
    "    volume_info = {}\n",
    "    total_slices = 0\n",
    "    all_volume_ids = []\n",
    "    \n",
    "    for patient_path in tqdm(patient_folders, desc=\"Preprocessing ACDC Training\"):\n",
    "        patient_results = preprocess_single_patient_acdc(patient_path, target_size)\n",
    "        \n",
    "        for volume, mask, volume_id in patient_results:\n",
    "            volume_save_path = os.path.join(volumes_dir, f'{volume_id}.npy')\n",
    "            mask_save_path = os.path.join(masks_dir, f'{volume_id}.npy')\n",
    "            \n",
    "            np.save(volume_save_path, volume)\n",
    "            np.save(mask_save_path, mask)\n",
    "            \n",
    "            num_slices = mask.shape[2]\n",
    "            volume_info[volume_id] = {'num_slices': num_slices}\n",
    "            total_slices += num_slices\n",
    "            all_volume_ids.append(volume_id)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SPLIT 80/20 FOR TRAIN/VAL\n",
    "    # =========================================================================\n",
    "    np.random.seed(42)  # Reproducible split\n",
    "    shuffled_ids = np.random.permutation(all_volume_ids).tolist()\n",
    "    \n",
    "    split_idx = int(0.8 * len(shuffled_ids))\n",
    "    train_volumes = shuffled_ids[:split_idx]\n",
    "    val_volumes = shuffled_ids[split_idx:]\n",
    "    \n",
    "    # Save metadata with split info\n",
    "    metadata = {\n",
    "        'dataset': 'ACDC',\n",
    "        'split_source': 'training_folder_only',\n",
    "        'target_size': list(target_size),\n",
    "        'total_volumes': len(volume_info),\n",
    "        'total_slices': total_slices,\n",
    "        'train_volumes': train_volumes,\n",
    "        'val_volumes': val_volumes,\n",
    "        'train_ratio': 0.8,\n",
    "        'volume_info': volume_info,\n",
    "        'num_classes': 4,\n",
    "        'class_names': ['Background', 'RV', 'MYO', 'LV']\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Preprocessing Complete!\")\n",
    "    print(f\"   Total volumes: {len(volume_info)} (from {len(patient_folders)} patients)\")\n",
    "    print(f\"   Total slices: {total_slices}\")\n",
    "    print(f\"   Train volumes: {len(train_volumes)} (80%)\")\n",
    "    print(f\"   Val volumes: {len(val_volumes)} (20%)\")\n",
    "    print(f\"   Output: {output_dir}\")\n",
    "    \n",
    "    return len(volume_info), total_slices, all_volume_ids\n",
    "\n",
    "\n",
    "# Run preprocessing\n",
    "print(f\"\\nüîÑ Step 3: Preprocessing {DATASET} TRAINING data...\")\n",
    "print(f\"   Input: {RAW_DATA_DIR}\")\n",
    "print(f\"   Output: {PREPROCESSED_DIR}\")\n",
    "print(f\"   Target size: {config_data['img_size']}√ó{config_data['img_size']}\")\n",
    "print(f\"   Split: 80% train, 20% val\")\n",
    "print()\n",
    "\n",
    "target_size = (config_data['img_size'], config_data['img_size'])\n",
    "num_volumes, total_slices, all_ids = preprocess_acdc_training_only(RAW_DATA_DIR, PREPROCESSED_DIR, target_size)\n",
    "\n",
    "print(f\"\\nüìä Expected vs Actual:\")\n",
    "print(f\"   Expected: ~1800 slices (100 patients √ó 2 frames √ó ~9 slices)\")\n",
    "print(f\"   Actual: {total_slices} slices\")\n",
    "\n",
    "if total_slices < 500:\n",
    "    print(f\"\\n‚ö†Ô∏è Slice count is low! Check if training folder was downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load preprocessed data v·ªõi split 80/20\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "class ACDCPreprocessedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ACDC Dataset from preprocessed .npy files.\n",
    "    Uses predefined train/val split from metadata (80/20).\n",
    "    Classes: Background (0), RV (1), Myocardium (2), LV (3)\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, split='train', min_foreground=50):\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.min_foreground = min_foreground\n",
    "        \n",
    "        volumes_dir = os.path.join(data_dir, 'volumes')\n",
    "        masks_dir = os.path.join(data_dir, 'masks')\n",
    "        \n",
    "        # Load metadata with train/val split\n",
    "        metadata_path = os.path.join(data_dir, 'metadata.json')\n",
    "        if not os.path.exists(metadata_path):\n",
    "            print(f\"‚ùå metadata.json not found in {data_dir}\")\n",
    "            self.slices = []\n",
    "            return\n",
    "        \n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        # Get volumes for this split (80/20 ƒë√£ chia s·∫µn)\n",
    "        if split == 'train':\n",
    "            volume_ids = metadata.get('train_volumes', [])\n",
    "        else:\n",
    "            volume_ids = metadata.get('val_volumes', [])\n",
    "        \n",
    "        if len(volume_ids) == 0:\n",
    "            # Fallback: split by ratio if metadata doesn't have split info\n",
    "            all_volumes = sorted(glob.glob(os.path.join(volumes_dir, '*.npy')))\n",
    "            split_idx = int(0.8 * len(all_volumes))\n",
    "            if split == 'train':\n",
    "                volume_files = all_volumes[:split_idx]\n",
    "            else:\n",
    "                volume_files = all_volumes[split_idx:]\n",
    "            volume_ids = [os.path.basename(f).replace('.npy', '') for f in volume_files]\n",
    "        \n",
    "        # Build slice index\n",
    "        self.slices = []\n",
    "        for vol_id in volume_ids:\n",
    "            vol_path = os.path.join(volumes_dir, f'{vol_id}.npy')\n",
    "            mask_path = os.path.join(masks_dir, f'{vol_id}.npy')\n",
    "            \n",
    "            if not os.path.exists(vol_path) or not os.path.exists(mask_path):\n",
    "                continue\n",
    "            \n",
    "            # Load mask to check which slices have foreground\n",
    "            mask = np.load(mask_path)\n",
    "            num_slices = mask.shape[2]\n",
    "            \n",
    "            for slice_idx in range(num_slices):\n",
    "                mask_slice = mask[:, :, slice_idx]\n",
    "                if np.sum(mask_slice > 0) >= self.min_foreground:\n",
    "                    self.slices.append((vol_path, mask_path, slice_idx))\n",
    "        \n",
    "        print(f\"   {split.upper()}: {len(self.slices)} slices from {len(volume_ids)} volumes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        vol_path, mask_path, slice_idx = self.slices[idx]\n",
    "        \n",
    "        # Load using memmap for speed\n",
    "        volume = np.load(vol_path, mmap_mode='r')\n",
    "        mask = np.load(mask_path, mmap_mode='r')\n",
    "        \n",
    "        img = volume[:, :, slice_idx].copy()\n",
    "        seg = mask[:, :, slice_idx].copy()\n",
    "        \n",
    "        # To tensor [C, H, W]\n",
    "        img = torch.from_numpy(img).unsqueeze(0).float()\n",
    "        seg = torch.from_numpy(seg).long()\n",
    "        \n",
    "        return img, seg\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "print(f\"\\nüìä Step 4: Loading preprocessed {DATASET} dataset...\")\n",
    "print(f\"   Data source: TRAINING folder only (100 patients)\")\n",
    "print(f\"   Split ratio: 80% train, 20% val\")\n",
    "print()\n",
    "\n",
    "train_dataset = ACDCPreprocessedDataset(PREPROCESSED_DIR, split='train')\n",
    "val_dataset = ACDCPreprocessedDataset(PREPROCESSED_DIR, split='val')\n",
    "\n",
    "# Check if data loaded\n",
    "if len(train_dataset) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è No data loaded! Please check preprocessing step.\")\n",
    "else:\n",
    "    # Create dataloaders\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_CLASSES = config_data['num_classes']\n",
    "    IN_CHANNELS = config_data['in_channels']\n",
    "    IMG_SIZE = config_data['img_size']\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=2, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=2, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"   Training slices: {len(train_dataset)} (80%)\")\n",
    "    print(f\"   Validation slices: {len(val_dataset)} (20%)\")\n",
    "    print(f\"   Total: {len(train_dataset) + len(val_dataset)} slices\")\n",
    "    print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"   Number of classes: {NUM_CLASSES}\")\n",
    "    print(f\"   Image size: {IMG_SIZE}√ó{IMG_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d968686b",
   "metadata": {},
   "source": [
    "## üéØ Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    # Dataset (auto-configured from above)\n",
    "    'dataset': DATASET,\n",
    "    'in_channels': IN_CHANNELS,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'img_size': IMG_SIZE,\n",
    "    'class_names': config_data['class_names'],\n",
    "    \n",
    "    # Model\n",
    "    'model': 'egm_net',           # 'egm_net', 'egm_net_lite', 'spectral_vmamba'\n",
    "    'base_channels': 64,\n",
    "    \n",
    "    # Training\n",
    "    'num_epochs': 100,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    \n",
    "    # Loss weights\n",
    "    'dice_weight': 1.0,\n",
    "    'ce_weight': 1.0,\n",
    "    'boundary_weight': 0.5,\n",
    "    \n",
    "    # Implicit representation\n",
    "    'num_points': 2048,           # Points sampled for implicit loss\n",
    "    'boundary_ratio': 0.5,        # Ratio of points near boundaries\n",
    "    \n",
    "    # Checkpointing\n",
    "    'save_every': 10,\n",
    "    'checkpoint_dir': f'./checkpoints_{DATASET}',\n",
    "    \n",
    "    # Early stopping\n",
    "    'patience': 20,\n",
    "}\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "print(\"üìã Training Configuration:\")\n",
    "print(f\"   Dataset: {config['dataset']}\")\n",
    "print(f\"   Classes: {config['class_names']}\")\n",
    "print(f\"   Model: {config['model']}\")\n",
    "print(f\"   Epochs: {config['num_epochs']}\")\n",
    "print(f\"   LR: {config['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model based on config\n",
    "print(f\"Creating {config['model']} model...\")\n",
    "print(f\"   Input channels: {config['in_channels']}\")\n",
    "print(f\"   Output classes: {config['num_classes']}\")\n",
    "\n",
    "if config['model'] == 'egm_net':\n",
    "    model = EGMNet(\n",
    "        in_channels=config['in_channels'],\n",
    "        num_classes=config['num_classes'],\n",
    "        img_size=config['img_size'],\n",
    "        base_channels=config['base_channels'],\n",
    "        num_stages=4,\n",
    "        encoder_depth=2\n",
    "    )\n",
    "elif config['model'] == 'egm_net_lite':\n",
    "    model = EGMNetLite(\n",
    "        in_channels=config['in_channels'],\n",
    "        num_classes=config['num_classes'],\n",
    "        img_size=config['img_size']\n",
    "    )\n",
    "else:  # spectral_vmamba\n",
    "    model = SpectralVMUNet(\n",
    "        in_channels=config['in_channels'],\n",
    "        out_channels=config['num_classes'],\n",
    "        img_size=config['img_size'],\n",
    "        base_channels=config['base_channels'],\n",
    "        num_stages=4\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n‚úÖ Model created!\")\n",
    "print(f\"   Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919f90c",
   "metadata": {},
   "source": [
    "## üìà Loss Functions & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11420cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice loss for segmentation.\"\"\"\n",
    "    def __init__(self, smooth=1e-5):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # pred: (B, C, H, W) logits\n",
    "        # target: (B, H, W) class indices\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        num_classes = pred.shape[1]\n",
    "        \n",
    "        target_one_hot = F.one_hot(target, num_classes).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        intersection = (pred * target_one_hot).sum(dim=(2, 3))\n",
    "        union = pred.sum(dim=(2, 3)) + target_one_hot.sum(dim=(2, 3))\n",
    "        \n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1.0 - dice.mean()\n",
    "\n",
    "\n",
    "class BoundaryLoss(nn.Module):\n",
    "    \"\"\"Boundary-aware loss using Sobel edge detection.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Sobel kernels\n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)\n",
    "        self.register_buffer('sobel_x', sobel_x.view(1, 1, 3, 3))\n",
    "        self.register_buffer('sobel_y', sobel_y.view(1, 1, 3, 3))\n",
    "        \n",
    "    def get_boundaries(self, mask):\n",
    "        # mask: (B, H, W)\n",
    "        mask = mask.float().unsqueeze(1)\n",
    "        edge_x = F.conv2d(mask, self.sobel_x, padding=1)\n",
    "        edge_y = F.conv2d(mask, self.sobel_y, padding=1)\n",
    "        edges = torch.sqrt(edge_x**2 + edge_y**2)\n",
    "        return (edges > 0.5).float()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Get boundary regions\n",
    "        boundaries = self.get_boundaries(target)\n",
    "        \n",
    "        # Weight loss by boundary\n",
    "        pred_probs = F.softmax(pred, dim=1)\n",
    "        target_one_hot = F.one_hot(target, pred.shape[1]).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        # BCE at boundaries\n",
    "        boundary_loss = F.binary_cross_entropy(\n",
    "            pred_probs * boundaries, \n",
    "            target_one_hot * boundaries,\n",
    "            reduction='sum'\n",
    "        ) / (boundaries.sum() + 1e-6)\n",
    "        \n",
    "        return boundary_loss\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combined loss for EGM-Net training.\"\"\"\n",
    "    def __init__(self, dice_weight=1.0, ce_weight=1.0, boundary_weight=0.5, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_weight = ce_weight\n",
    "        self.boundary_weight = boundary_weight\n",
    "        \n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.boundary_loss = BoundaryLoss()\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        # For EGM-Net, outputs is a dict\n",
    "        if isinstance(outputs, dict):\n",
    "            pred = outputs['output']\n",
    "            coarse = outputs.get('coarse')\n",
    "            fine = outputs.get('fine')\n",
    "            \n",
    "            # Main loss\n",
    "            loss = self.dice_weight * self.dice_loss(pred, targets)\n",
    "            loss += self.ce_weight * self.ce_loss(pred, targets)\n",
    "            loss += self.boundary_weight * self.boundary_loss(pred, targets)\n",
    "            \n",
    "            # Auxiliary losses (coarse and fine branches)\n",
    "            if coarse is not None:\n",
    "                loss += 0.3 * self.ce_loss(coarse, targets)\n",
    "            if fine is not None:\n",
    "                loss += 0.3 * self.ce_loss(fine, targets)\n",
    "                \n",
    "            return loss\n",
    "        else:\n",
    "            # Standard output (SpectralVMUNet)\n",
    "            loss = self.dice_weight * self.dice_loss(outputs, targets)\n",
    "            loss += self.ce_weight * self.ce_loss(outputs, targets)\n",
    "            loss += self.boundary_weight * self.boundary_loss(outputs, targets)\n",
    "            return loss\n",
    "\n",
    "\n",
    "# Metrics\n",
    "def compute_dice(pred, target, num_classes):\n",
    "    \"\"\"Compute per-class Dice scores.\"\"\"\n",
    "    dice_scores = []\n",
    "    pred_classes = torch.argmax(pred, dim=1)\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        pred_c = (pred_classes == c).float()\n",
    "        target_c = (target == c).float()\n",
    "        \n",
    "        intersection = (pred_c * target_c).sum()\n",
    "        union = pred_c.sum() + target_c.sum()\n",
    "        \n",
    "        if union > 0:\n",
    "            dice = (2.0 * intersection) / union\n",
    "        else:\n",
    "            dice = torch.tensor(1.0)  # Both empty = perfect\n",
    "            \n",
    "        dice_scores.append(dice.item())\n",
    "    \n",
    "    return dice_scores\n",
    "\n",
    "\n",
    "def compute_iou(pred, target, num_classes):\n",
    "    \"\"\"Compute per-class IoU scores.\"\"\"\n",
    "    iou_scores = []\n",
    "    pred_classes = torch.argmax(pred, dim=1)\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        pred_c = (pred_classes == c).float()\n",
    "        target_c = (target == c).float()\n",
    "        \n",
    "        intersection = (pred_c * target_c).sum()\n",
    "        union = pred_c.sum() + target_c.sum() - intersection\n",
    "        \n",
    "        if union > 0:\n",
    "            iou = intersection / union\n",
    "        else:\n",
    "            iou = torch.tensor(1.0)\n",
    "            \n",
    "        iou_scores.append(iou.item())\n",
    "    \n",
    "    return iou_scores\n",
    "\n",
    "\n",
    "print(\"‚úÖ Loss functions and metrics defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4374d9",
   "metadata": {},
   "source": [
    "## üöÄ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f10568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training function\n",
    "def train_model(model, train_loader, val_loader, config, device):\n",
    "    \"\"\"Complete training loop with validation and checkpointing.\"\"\"\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = CombinedLoss(\n",
    "        dice_weight=config['dice_weight'],\n",
    "        ce_weight=config['ce_weight'],\n",
    "        boundary_weight=config['boundary_weight'],\n",
    "        num_classes=config['num_classes']\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler (cosine annealing)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=config['num_epochs'],\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_dice': [],\n",
    "        'val_iou': [],\n",
    "        'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    best_val_dice = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ Starting Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        # =============== Training ===============\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\")\n",
    "        \n",
    "        for batch_idx, (images, masks) in enumerate(train_pbar):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # =============== Validation ===============\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_dice = []\n",
    "        all_iou = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Val]\"):\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get prediction tensor\n",
    "                pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "                \n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Compute metrics\n",
    "                dice_scores = compute_dice(pred, masks, config['num_classes'])\n",
    "                iou_scores = compute_iou(pred, masks, config['num_classes'])\n",
    "                \n",
    "                all_dice.append(np.mean(dice_scores[1:]))  # Exclude background\n",
    "                all_iou.append(np.mean(iou_scores[1:]))\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_dice = np.mean(all_dice)\n",
    "        avg_val_iou = np.mean(all_iou)\n",
    "        \n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_dice'].append(avg_val_dice)\n",
    "        history['val_iou'].append(avg_val_iou)\n",
    "        history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nüìä Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "        print(f\"   Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"   Val Loss:   {avg_val_loss:.4f}\")\n",
    "        print(f\"   Val Dice:   {avg_val_dice:.4f}\")\n",
    "        print(f\"   Val IoU:    {avg_val_iou:.4f}\")\n",
    "        print(f\"   LR:         {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # =============== Checkpointing ===============\n",
    "        # Save best model\n",
    "        if avg_val_dice > best_val_dice:\n",
    "            best_val_dice = avg_val_dice\n",
    "            patience_counter = 0\n",
    "            \n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_dice': best_val_dice,\n",
    "                'config': config,\n",
    "                'history': history\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(config['checkpoint_dir'], 'best_model.pth'))\n",
    "            print(f\"   ‚úÖ New best model saved! (Dice: {best_val_dice:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Save periodic checkpoints\n",
    "        if (epoch + 1) % config['save_every'] == 0:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'history': history\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(config['checkpoint_dir'], f'checkpoint_epoch_{epoch+1}.pth'))\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= config['patience']:\n",
    "            print(f\"\\n‚ö†Ô∏è Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üéâ Training completed!\")\n",
    "    print(f\"   Best Val Dice: {best_val_dice:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0bfd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ START TRAINING\n",
    "history = train_model(model, train_loader, val_loader, config, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d9cd45",
   "metadata": {},
   "source": [
    "## üìä Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da620a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training & Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Dice score\n",
    "axes[0, 1].plot(history['val_dice'], label='Val Dice', linewidth=2, color='green')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Dice Score')\n",
    "axes[0, 1].set_title('Validation Dice Score')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].axhline(y=max(history['val_dice']), color='r', linestyle='--', alpha=0.5, label=f\"Best: {max(history['val_dice']):.4f}\")\n",
    "\n",
    "# IoU score\n",
    "axes[1, 0].plot(history['val_iou'], label='Val IoU', linewidth=2, color='orange')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('IoU Score')\n",
    "axes[1, 0].set_title('Validation IoU Score')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 1].plot(history['learning_rate'], label='Learning Rate', linewidth=2, color='purple')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Learning Rate')\n",
    "axes[1, 1].set_title('Learning Rate Schedule')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.suptitle('Training Progress', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìà Training Summary:\")\n",
    "print(f\"   Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"   Final Val Loss:   {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"   Best Val Dice:    {max(history['val_dice']):.4f}\")\n",
    "print(f\"   Best Val IoU:     {max(history['val_iou']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0288e93d",
   "metadata": {},
   "source": [
    "## üîç Inference & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4617ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(os.path.join(config['checkpoint_dir'], 'best_model.pth'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"‚úÖ Loaded best model from epoch {checkpoint['epoch']} (Dice: {checkpoint['best_val_dice']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on validation set\n",
    "model.eval()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(4):\n",
    "        idx = i * (len(val_dataset) // 4)\n",
    "        img, mask = val_dataset[idx]\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get prediction\n",
    "        outputs = model(img)\n",
    "        pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "        pred_mask = torch.argmax(pred, dim=1)[0].cpu()\n",
    "        \n",
    "        # Get energy map if available\n",
    "        energy = outputs.get('energy', None)\n",
    "        \n",
    "        # Plot\n",
    "        axes[i, 0].imshow(img[0, 0].cpu(), cmap='gray')\n",
    "        axes[i, 0].set_title('Input Image')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(mask, cmap='viridis')\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(pred_mask, cmap='viridis')\n",
    "        axes[i, 2].set_title('Prediction')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        overlay = img[0, 0].cpu().numpy()\n",
    "        overlay = np.stack([overlay, overlay, overlay], axis=-1)\n",
    "        pred_np = pred_mask.numpy()\n",
    "        mask_np = mask.numpy()\n",
    "        \n",
    "        # Red for prediction, blue for ground truth\n",
    "        overlay[..., 0] = np.where(pred_np > 0, 1.0, overlay[..., 0])\n",
    "        overlay[..., 2] = np.where(mask_np > 0, 1.0, overlay[..., 2])\n",
    "        overlay = np.clip(overlay, 0, 1)\n",
    "        \n",
    "        axes[i, 3].imshow(overlay)\n",
    "        axes[i, 3].set_title('Overlay (Red=Pred, Blue=GT)')\n",
    "        axes[i, 3].axis('off')\n",
    "\n",
    "plt.suptitle('Validation Predictions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce67046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize EGM-Net branches (Coarse vs Fine)\n",
    "if config['model'] in ['egm_net', 'egm_net_lite']:\n",
    "    fig, axes = plt.subplots(3, 5, figsize=(20, 12))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(3):\n",
    "            idx = i * (len(val_dataset) // 3)\n",
    "            img, mask = val_dataset[idx]\n",
    "            img = img.unsqueeze(0).to(device)\n",
    "            \n",
    "            outputs = model(img)\n",
    "            \n",
    "            # Extract all outputs\n",
    "            final_pred = torch.argmax(outputs['output'], dim=1)[0].cpu()\n",
    "            coarse_pred = torch.argmax(outputs['coarse'], dim=1)[0].cpu()\n",
    "            fine_pred = torch.argmax(outputs['fine'], dim=1)[0].cpu()\n",
    "            energy = outputs['energy'][0, 0].cpu()\n",
    "            \n",
    "            # Plot\n",
    "            axes[i, 0].imshow(img[0, 0].cpu(), cmap='gray')\n",
    "            axes[i, 0].set_title('Input')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(energy, cmap='hot')\n",
    "            axes[i, 1].set_title('Energy Map')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            axes[i, 2].imshow(coarse_pred, cmap='viridis')\n",
    "            axes[i, 2].set_title('Coarse Branch')\n",
    "            axes[i, 2].axis('off')\n",
    "            \n",
    "            axes[i, 3].imshow(fine_pred, cmap='viridis')\n",
    "            axes[i, 3].set_title('Fine Branch')\n",
    "            axes[i, 3].axis('off')\n",
    "            \n",
    "            axes[i, 4].imshow(final_pred, cmap='viridis')\n",
    "            axes[i, 4].set_title('Final (Fused)')\n",
    "            axes[i, 4].axis('off')\n",
    "    \n",
    "    plt.suptitle('EGM-Net Branch Analysis\\\\n(Energy-Gated Fusion of Coarse + Fine)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('branch_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01eb4b2",
   "metadata": {},
   "source": [
    "## üéØ Resolution-Free Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate resolution-free inference (unique to EGM-Net)\n",
    "if config['model'] in ['egm_net', 'egm_net_lite']:\n",
    "    print(\"üî¨ Resolution-Free Inference Demo\")\n",
    "    print(\"   EGM-Net can render at ANY resolution without retraining!\")\n",
    "    \n",
    "    resolutions = [64, 128, 256, 512]\n",
    "    \n",
    "    # Get a sample image\n",
    "    img, mask = val_dataset[0]\n",
    "    img = img.unsqueeze(0).to(device)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, len(resolutions), figsize=(16, 8))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, res in enumerate(resolutions):\n",
    "            # Render at different resolutions\n",
    "            outputs = model(img, output_size=(res, res))\n",
    "            pred = torch.argmax(outputs['output'], dim=1)[0].cpu()\n",
    "            \n",
    "            # Also show input at same res for comparison\n",
    "            input_resized = F.interpolate(img, size=(res, res), mode='bilinear', align_corners=False)\n",
    "            \n",
    "            axes[0, i].imshow(input_resized[0, 0].cpu(), cmap='gray')\n",
    "            axes[0, i].set_title(f'Input {res}√ó{res}')\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            axes[1, i].imshow(pred, cmap='viridis')\n",
    "            axes[1, i].set_title(f'Prediction {res}√ó{res}')\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Resolution-Free Rendering\\\\n(Same model weights, different output resolutions)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('resolution_free.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\\\n‚úÖ Same model can output at 64√ó64 to 512√ó512 (or higher)!\")\n",
    "    print(\"   This is impossible with standard CNN decoders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f3f21",
   "metadata": {},
   "source": [
    "## üíæ Save & Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model to Google Drive\n",
    "from google.colab import drive\n",
    "\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Save to Drive\n",
    "    save_path = '/content/drive/MyDrive/EGM_Net_Models'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    final_checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': config,\n",
    "        'history': history,\n",
    "        'best_val_dice': max(history['val_dice'])\n",
    "    }\n",
    "    torch.save(final_checkpoint, os.path.join(save_path, 'egm_net_trained.pth'))\n",
    "    \n",
    "    # Also copy training curves\n",
    "    import shutil\n",
    "    shutil.copy('training_curves.png', save_path)\n",
    "    shutil.copy('predictions.png', save_path)\n",
    "    \n",
    "    print(f\"‚úÖ Model saved to Google Drive: {save_path}\")\n",
    "    print(f\"   - egm_net_trained.pth\")\n",
    "    print(f\"   - training_curves.png\")\n",
    "    print(f\"   - predictions.png\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save to Google Drive: {e}\")\n",
    "    print(\"   Model is saved locally in ./checkpoints/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ea463",
   "metadata": {},
   "source": [
    "## üìä Final Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b0f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on full validation set\n",
    "model.eval()\n",
    "\n",
    "all_dice_scores = []\n",
    "all_iou_scores = []\n",
    "all_hd95_scores = []  # Hausdorff Distance 95\n",
    "\n",
    "# Helper function for Hausdorff distance\n",
    "def compute_hausdorff_95(pred, target):\n",
    "    \"\"\"Compute 95th percentile Hausdorff distance.\"\"\"\n",
    "    from scipy.ndimage import distance_transform_edt\n",
    "    \n",
    "    pred_np = pred.numpy().astype(bool)\n",
    "    target_np = target.numpy().astype(bool)\n",
    "    \n",
    "    if pred_np.sum() == 0 or target_np.sum() == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Distance transforms\n",
    "    pred_dist = distance_transform_edt(~pred_np)\n",
    "    target_dist = distance_transform_edt(~target_np)\n",
    "    \n",
    "    # Get surface points\n",
    "    pred_surface = pred_np & (distance_transform_edt(pred_np) <= 1)\n",
    "    target_surface = target_np & (distance_transform_edt(target_np) <= 1)\n",
    "    \n",
    "    # Distances from pred surface to target, and vice versa\n",
    "    d_pred_to_target = target_dist[pred_surface]\n",
    "    d_target_to_pred = pred_dist[target_surface]\n",
    "    \n",
    "    if len(d_pred_to_target) == 0 or len(d_target_to_pred) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # 95th percentile\n",
    "    hd95 = max(np.percentile(d_pred_to_target, 95), np.percentile(d_target_to_pred, 95))\n",
    "    return hd95\n",
    "\n",
    "print(\"üîç Running final evaluation...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "        pred_masks = torch.argmax(pred, dim=1)\n",
    "        \n",
    "        for b in range(images.shape[0]):\n",
    "            # Per-sample metrics\n",
    "            dice = compute_dice(pred[b:b+1], masks[b:b+1], config['num_classes'])\n",
    "            iou = compute_iou(pred[b:b+1], masks[b:b+1], config['num_classes'])\n",
    "            \n",
    "            all_dice_scores.append(np.mean(dice[1:]))  # Exclude background\n",
    "            all_iou_scores.append(np.mean(iou[1:]))\n",
    "            \n",
    "            # Hausdorff distance (for foreground)\n",
    "            try:\n",
    "                hd95 = compute_hausdorff_95(\n",
    "                    (pred_masks[b] > 0).cpu(),\n",
    "                    (masks[b] > 0).cpu()\n",
    "                )\n",
    "                all_hd95_scores.append(hd95)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Metric':<20} {'Mean':<12} {'Std':<12}\")\n",
    "print(\"-\"*44)\n",
    "print(f\"{'Dice Score':<20} {np.mean(all_dice_scores):.4f}       {np.std(all_dice_scores):.4f}\")\n",
    "print(f\"{'IoU Score':<20} {np.mean(all_iou_scores):.4f}       {np.std(all_iou_scores):.4f}\")\n",
    "if all_hd95_scores:\n",
    "    print(f\"{'HD95 (mm)':<20} {np.mean(all_hd95_scores):.2f}         {np.std(all_hd95_scores):.2f}\")\n",
    "print(\"-\"*44)\n",
    "print(f\"\\nTotal validation samples: {len(all_dice_scores)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e47b55f",
   "metadata": {},
   "source": [
    "## 3. Test Monogenic Signal Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea0165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test image with edges\n",
    "def create_test_image(size=256):\n",
    "    \"\"\"Create synthetic medical-like image with organs.\"\"\"\n",
    "    img = torch.zeros(1, 1, size, size)\n",
    "    \n",
    "    # Add circular \"organ\"\n",
    "    y, x = torch.meshgrid(torch.arange(size), torch.arange(size), indexing='ij')\n",
    "    center1 = (size // 2, size // 2)\n",
    "    radius1 = size // 4\n",
    "    mask1 = ((x - center1[0])**2 + (y - center1[1])**2) < radius1**2\n",
    "    img[0, 0, mask1] = 0.7\n",
    "    \n",
    "    # Add smaller \"tumor\"\n",
    "    center2 = (size // 2 + 30, size // 2 - 20)\n",
    "    radius2 = size // 10\n",
    "    mask2 = ((x - center2[0])**2 + (y - center2[1])**2) < radius2**2\n",
    "    img[0, 0, mask2] = 1.0\n",
    "    \n",
    "    # Add noise\n",
    "    img = img + 0.05 * torch.randn_like(img)\n",
    "    \n",
    "    return img, mask1.float(), mask2.float()\n",
    "\n",
    "# Create test image\n",
    "test_img, organ_mask, tumor_mask = create_test_image(256)\n",
    "print(f\"Test image shape: {test_img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18065bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Monogenic Energy Extraction\n",
    "energy_extractor = EnergyMap(normalize=True, smoothing_sigma=1.0)\n",
    "energy, mono_out = energy_extractor(test_img)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].imshow(test_img[0, 0], cmap='gray')\n",
    "axes[0, 0].set_title('Input Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(energy[0, 0].detach(), cmap='hot')\n",
    "axes[0, 1].set_title('Energy Map (Edges)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(mono_out['phase'][0, 0].detach(), cmap='twilight')\n",
    "axes[0, 2].set_title('Phase')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(mono_out['orientation'][0, 0].detach(), cmap='hsv')\n",
    "axes[1, 0].set_title('Orientation')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(mono_out['riesz_x'][0, 0].detach(), cmap='RdBu')\n",
    "axes[1, 1].set_title('Riesz X Component')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(mono_out['riesz_y'][0, 0].detach(), cmap='RdBu')\n",
    "axes[1, 2].set_title('Riesz Y Component')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Monogenic Signal Decomposition', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Monogenic processing works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee600ecb",
   "metadata": {},
   "source": [
    "## 4. Test Gabor Basis vs Fourier Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gabor_implicit import GaborBasis, FourierFeatures\n",
    "\n",
    "# Create coordinate grid\n",
    "size = 128\n",
    "y = torch.linspace(-1, 1, size)\n",
    "x = torch.linspace(-1, 1, size)\n",
    "yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "coords = torch.stack([xx, yy], dim=-1).view(1, -1, 2)  # (1, size*size, 2)\n",
    "\n",
    "# Compare Gabor vs Fourier\n",
    "gabor = GaborBasis(input_dim=2, num_frequencies=32)\n",
    "fourier = FourierFeatures(input_dim=2, num_frequencies=32, scale=10.0)\n",
    "\n",
    "gabor_features = gabor(coords)\n",
    "fourier_features = fourier(coords)\n",
    "\n",
    "print(f\"Gabor features shape: {gabor_features.shape}\")\n",
    "print(f\"Fourier features shape: {fourier_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7aaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first few basis functions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(4):\n",
    "    # Gabor\n",
    "    gabor_vis = gabor_features[0, :, i].view(size, size).detach().numpy()\n",
    "    axes[0, i].imshow(gabor_vis, cmap='RdBu', vmin=-1, vmax=1)\n",
    "    axes[0, i].set_title(f'Gabor Basis {i+1}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Fourier\n",
    "    fourier_vis = fourier_features[0, :, i].view(size, size).detach().numpy()\n",
    "    axes[1, i].imshow(fourier_vis, cmap='RdBu', vmin=-1, vmax=1)\n",
    "    axes[1, i].set_title(f'Fourier Basis {i+1}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Gabor\\n(Localized)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Fourier\\n(Global)', fontsize=12)\n",
    "\n",
    "plt.suptitle('Gabor vs Fourier Basis Functions\\n(Gabor is localized ‚Üí No Gibbs ringing)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ef9df",
   "metadata": {},
   "source": [
    "## 5. Create and Analyze Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c426ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create EGM-Net models\n",
    "print(\"Creating models...\")\n",
    "\n",
    "# Full model\n",
    "egm_net = EGMNet(\n",
    "    in_channels=1,\n",
    "    num_classes=3,\n",
    "    img_size=256,\n",
    "    base_channels=64,\n",
    "    num_stages=4,\n",
    "    encoder_depth=2\n",
    ").to(device)\n",
    "\n",
    "# Lite model\n",
    "egm_lite = EGMNetLite(\n",
    "    in_channels=1,\n",
    "    num_classes=3,\n",
    "    img_size=256\n",
    ").to(device)\n",
    "\n",
    "# Spectral Mamba (comparison)\n",
    "spec_mamba = SpectralVMUNet(\n",
    "    in_channels=1,\n",
    "    out_channels=3,\n",
    "    img_size=256,\n",
    "    base_channels=64,\n",
    "    num_stages=4\n",
    ").to(device)\n",
    "\n",
    "print(\"\\nüìä Model Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "models = {\n",
    "    'EGM-Net Full': egm_net,\n",
    "    'EGM-Net Lite': egm_lite,\n",
    "    'SpectralVMUNet': spec_mamba\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name:20s}: {params:,} parameters ({params/1e6:.2f}M)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeffa14a",
   "metadata": {},
   "source": [
    "## 6. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c023bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "test_input = torch.randn(2, 1, 256, 256).to(device)\n",
    "\n",
    "print(\"Testing forward pass...\")\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # EGM-Net\n",
    "    egm_out = egm_net(test_input)\n",
    "    print(f\"\\nüîπ EGM-Net Output:\")\n",
    "    for k, v in egm_out.items():\n",
    "        print(f\"   {k}: {v.shape}\")\n",
    "    \n",
    "    # SpectralVMUNet\n",
    "    spec_out = spec_mamba(test_input)\n",
    "    print(f\"\\nüîπ SpectralVMUNet Output: {spec_out.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4b153",
   "metadata": {},
   "source": [
    "## 7. Test Resolution-Free Inference (Unique to EGM-Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb14e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EGM-Net can query at arbitrary coordinates!\n",
    "print(\"Testing Resolution-Free Inference...\")\n",
    "\n",
    "# Create query points (random locations)\n",
    "num_points = 10000\n",
    "random_coords = torch.rand(1, num_points, 2).to(device) * 2 - 1  # [-1, 1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Query at random points\n",
    "    point_output = egm_net.query_points(test_input[:1], random_coords)\n",
    "    \n",
    "print(f\"Query coordinates: {random_coords.shape}\")\n",
    "print(f\"Point outputs: {point_output.shape}\")\n",
    "print(\"\\n‚úÖ Resolution-free inference works!\")\n",
    "print(\"   ‚Üí You can zoom into boundaries at ANY resolution!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate resolution-free: render at different resolutions\n",
    "resolutions = [64, 128, 256, 512]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, res in enumerate(resolutions):\n",
    "        # Render at this resolution\n",
    "        output = egm_net(test_input[:1], output_size=(res, res))\n",
    "        pred = torch.argmax(output['output'], dim=1)[0].cpu().numpy()\n",
    "        \n",
    "        axes[idx].imshow(pred, cmap='viridis')\n",
    "        axes[idx].set_title(f'{res}√ó{res}')\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Resolution-Free Rendering (Same model, different output sizes)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04640dc",
   "metadata": {},
   "source": [
    "## 8. Visualize Energy-Gated Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01794b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dual-branch architecture\n",
    "with torch.no_grad():\n",
    "    outputs = egm_net(test_input[:1])\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Input\n",
    "axes[0, 0].imshow(test_input[0, 0].cpu(), cmap='gray')\n",
    "axes[0, 0].set_title('Input Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Energy Map\n",
    "axes[0, 1].imshow(outputs['energy'][0, 0].cpu(), cmap='hot')\n",
    "axes[0, 1].set_title('Energy Map (Edge Detection)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Coarse Branch\n",
    "coarse_pred = torch.argmax(outputs['coarse'], dim=1)[0].cpu()\n",
    "axes[0, 2].imshow(coarse_pred, cmap='viridis')\n",
    "axes[0, 2].set_title('Coarse Branch (Smooth)')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Fine Branch\n",
    "fine_pred = torch.argmax(outputs['fine'], dim=1)[0].cpu()\n",
    "axes[1, 0].imshow(fine_pred, cmap='viridis')\n",
    "axes[1, 0].set_title('Fine Branch (Sharp)')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Final Output\n",
    "final_pred = torch.argmax(outputs['output'], dim=1)[0].cpu()\n",
    "axes[1, 1].imshow(final_pred, cmap='viridis')\n",
    "axes[1, 1].set_title('Final Output (Fused)')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Difference\n",
    "diff = (fine_pred != coarse_pred).float()\n",
    "axes[1, 2].imshow(diff, cmap='Reds')\n",
    "axes[1, 2].set_title('Difference (Fine vs Coarse)')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle('EGM-Net Dual-Branch Architecture', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c51b26",
   "metadata": {},
   "source": [
    "## 9. Quick Training Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d86f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_egm import EGMNetTrainer, create_dummy_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create small dummy dataset\n",
    "print(\"Creating dummy dataset...\")\n",
    "dataset = create_dummy_dataset(num_samples=16, img_size=256, num_classes=3)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Training config\n",
    "config = {\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'num_epochs': 2,\n",
    "    'num_points': 1024,\n",
    "    'boundary_ratio': 0.5,\n",
    "    'checkpoint_dir': './checkpoints_demo'\n",
    "}\n",
    "\n",
    "# Use lite model for faster training\n",
    "model = EGMNetLite(in_channels=1, num_classes=3, img_size=256)\n",
    "print(f\"Model: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b6e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for a few epochs\n",
    "print(\"\\nStarting training demo...\")\n",
    "trainer = EGMNetTrainer(model, config, device=device)\n",
    "trainer.train(train_loader, num_epochs=2)\n",
    "\n",
    "print(\"\\n‚úÖ Training demo completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c9e44f",
   "metadata": {},
   "source": [
    "## 10. Inference Speed Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_model(model, input_tensor, num_runs=50, warmup=10):\n",
    "    \"\"\"Benchmark inference speed.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(input_tensor)\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            start = time.time()\n",
    "            _ = model(input_tensor)\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "    \n",
    "    return np.mean(times) * 1000, np.std(times) * 1000  # ms\n",
    "\n",
    "# Benchmark\n",
    "print(\"Benchmarking inference speed...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "test_input = torch.randn(1, 1, 256, 256).to(device)\n",
    "\n",
    "for name, model in [('EGM-Net Full', egm_net), ('EGM-Net Lite', egm_lite)]:\n",
    "    mean_time, std_time = benchmark_model(model, test_input)\n",
    "    fps = 1000 / mean_time\n",
    "    print(f\"{name:20s}: {mean_time:.2f} ¬± {std_time:.2f} ms ({fps:.1f} FPS)\")\n",
    "\n",
    "print(\"\\n‚úÖ Benchmark completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17a31bd",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb75fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                    EGM-NET ARCHITECTURE SUMMARY                       ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  üî¨ KEY INNOVATIONS:                                                  ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  1. MONOGENIC ENERGY GATING                                          ‚ïë\n",
    "‚ïë     ‚Ä¢ Physics-based edge detection (Riesz Transform)                 ‚ïë\n",
    "‚ïë     ‚Ä¢ Automatically focuses on boundary regions                      ‚ïë\n",
    "‚ïë     ‚Ä¢ Suppresses artifacts in flat regions                           ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  2. GABOR BASIS (vs Fourier)                                         ‚ïë\n",
    "‚ïë     ‚Ä¢ Localized oscillations (Gaussian √ó sin)                        ‚ïë\n",
    "‚ïë     ‚Ä¢ NO Gibbs ringing artifacts                                     ‚ïë\n",
    "‚ïë     ‚Ä¢ Sharp edges remain clean                                       ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  3. DUAL-PATH ARCHITECTURE                                           ‚ïë\n",
    "‚ïë     ‚Ä¢ Coarse Branch: Smooth body regions (Conv decoder)              ‚ïë\n",
    "‚ïë     ‚Ä¢ Fine Branch: Sharp boundaries (Gabor Implicit)                 ‚ïë\n",
    "‚ïë     ‚Ä¢ Energy-gated fusion: Best of both worlds                       ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  4. RESOLUTION-FREE INFERENCE                                        ‚ïë\n",
    "‚ïë     ‚Ä¢ Query at ANY coordinate ‚Üí Infinite zoom                        ‚ïë\n",
    "‚ïë     ‚Ä¢ No retraining needed for different resolutions                 ‚ïë\n",
    "‚ïë     ‚Ä¢ Perfect for high-resolution medical imaging                    ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  5. MAMBA ENCODER                                                    ‚ïë\n",
    "‚ïë     ‚Ä¢ O(N) complexity (vs O(N¬≤) for Transformers)                    ‚ïë\n",
    "‚ïë     ‚Ä¢ Global context awareness                                       ‚ïë\n",
    "‚ïë     ‚Ä¢ Efficient for large images                                     ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  üìä MODEL SIZES:                                                      ‚ïë\n",
    "‚ïë     ‚Ä¢ EGM-Net Full:  ~9.13M parameters                               ‚ïë\n",
    "‚ïë     ‚Ä¢ EGM-Net Lite:  ~635K parameters                                ‚ïë\n",
    "‚ïë     ‚Ä¢ SpectralVMUNet: ~10.31M parameters                             ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36957ab8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Next Steps\n",
    "\n",
    "1. **Train on real data**: Replace dummy dataset with medical imaging dataset (e.g., Synapse, ACDC)\n",
    "2. **Tune hyperparameters**: Adjust `num_frequencies`, `boundary_ratio`, learning rate\n",
    "3. **Evaluate metrics**: Dice score, IoU, Hausdorff distance\n",
    "4. **Ablation study**: Compare Gabor vs Fourier, with/without energy gating\n",
    "\n",
    "---\n",
    "\n",
    "**Repository**: https://github.com/QuocKhanhLuong/FourierNetwork"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
