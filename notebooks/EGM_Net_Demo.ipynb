{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4d6004",
   "metadata": {},
   "source": [
    "# ðŸ§  EGM-Net: Energy-Gated Gabor Mamba Network\n",
    "\n",
    "**Medical Image Segmentation with Implicit Neural Representations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879d899",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6517affa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install -q torch torchvision numpy matplotlib tqdm gdown nibabel scikit-image monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed28aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ–¥ï¸ Device: cuda\n",
      "   GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os, glob, json\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ðŸ–¥ï¸ Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed39886",
   "metadata": {},
   "source": [
    "---\n",
    "## 2ï¸âƒ£ Model Architecture\n",
    "\n",
    "All model code is defined inline below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3dda03",
   "metadata": {},
   "source": [
    "### 2.1 Mamba Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e5665c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 64, 64, 64])\n",
      "Output shape: torch.Size([2, 64, 64, 64])\n",
      "Module parameters: 31648\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "class DepthwiseSeparableConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2, groups=in_channels, bias=False\n",
    "        )\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=True)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class DirectionalScanner(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, scan_dim: int = 64):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.scan_dim = scan_dim\n",
    "        \n",
    "        # Learnable projection to scan_dim for each direction\n",
    "        self.proj_in = nn.Linear(channels, scan_dim)\n",
    "        \n",
    "        # GRU cell for sequential state processing (simulates SSM)\n",
    "        self.gru_cell = nn.GRUCell(scan_dim, scan_dim)\n",
    "        \n",
    "        # Project back to original channels\n",
    "        self.proj_out = nn.Linear(scan_dim, channels)\n",
    "        \n",
    "    def _scan_direction(self, x: torch.Tensor, direction: str) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Prepare sequence based on direction\n",
    "        if direction == \"right\":\n",
    "            # Scan left-to-right: (B, H*W, C) after reshape\n",
    "            x = x.permute(0, 2, 3, 1).reshape(B * H, W, C)  # (B*H, W, C)\n",
    "        elif direction == \"down\":\n",
    "            # Scan top-to-bottom\n",
    "            x = x.permute(0, 3, 2, 1).reshape(B * W, H, C)  # (B*W, H, C)\n",
    "        elif direction == \"left\":\n",
    "            # Scan right-to-left (reverse)\n",
    "            x = x.permute(0, 2, 3, 1).flip(1).reshape(B * H, W, C)  # (B*H, W, C)\n",
    "        elif direction == \"up\":\n",
    "            # Scan bottom-to-top (reverse)\n",
    "            x = x.permute(0, 3, 2, 1).flip(1).reshape(B * W, H, C)  # (B*W, H, C)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown direction: {direction}\")\n",
    "        \n",
    "        # Project to scan dimension\n",
    "        x = self.proj_in(x)  # (*, W/H, scan_dim)\n",
    "        \n",
    "        # Apply GRU cell sequentially (simulates SSM forward pass)\n",
    "        outputs = []\n",
    "        h = torch.zeros(x.shape[0], self.scan_dim, device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        for t in range(x.shape[1]):\n",
    "            h = self.gru_cell(x[:, t], h)  # GRU step\n",
    "            outputs.append(h)\n",
    "        \n",
    "        x = torch.stack(outputs, dim=1)  # (*, W/H, scan_dim)\n",
    "        \n",
    "        # Project back to original channels\n",
    "        x = self.proj_out(x)  # (*, W/H, C)\n",
    "        \n",
    "        # Reshape back to (B, C, H, W)\n",
    "        if direction == \"right\":\n",
    "            x = x.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        elif direction == \"down\":\n",
    "            x = x.reshape(B, W, H, C).permute(0, 3, 2, 1)\n",
    "        elif direction == \"left\":\n",
    "            x = x.reshape(B, H, W, C).permute(0, 3, 1, 2).flip(-1)\n",
    "        elif direction == \"up\":\n",
    "            x = x.reshape(B, W, H, C).permute(0, 3, 2, 1).flip(-2)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Scan in all 4 directions\n",
    "        scan_right = self._scan_direction(x, \"right\")\n",
    "        scan_down = self._scan_direction(x, \"down\")\n",
    "        scan_left = self._scan_direction(x, \"left\")\n",
    "        scan_up = self._scan_direction(x, \"up\")\n",
    "        \n",
    "        # Aggregate by averaging\n",
    "        output = (scan_right + scan_down + scan_left + scan_up) / 4.0\n",
    "        \n",
    "        return output\n",
    "\n",
    "class VSSBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, hidden_dim: Optional[int] = None, \n",
    "                 scan_dim: int = 64, expansion_ratio: float = 2.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        hidden_dim = hidden_dim or int(channels * expansion_ratio)\n",
    "        \n",
    "        # Preprocessing: expand channels\n",
    "        self.norm1 = nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6)\n",
    "        self.conv_expand = nn.Conv2d(channels, hidden_dim, kernel_size=1, bias=True)\n",
    "        \n",
    "        # Directional scanning\n",
    "        self.scanner = DirectionalScanner(hidden_dim, scan_dim=scan_dim)\n",
    "        \n",
    "        # Postprocessing: contract channels back\n",
    "        self.norm2 = nn.GroupNorm(num_groups=32, num_channels=hidden_dim, eps=1e-6)\n",
    "        self.conv_contract = nn.Conv2d(hidden_dim, channels, kernel_size=1, bias=True)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        residual = x\n",
    "        \n",
    "        # Preprocessing\n",
    "        x = self.norm1(x)\n",
    "        x = self.conv_expand(x)\n",
    "        x = F.gelu(x)\n",
    "        \n",
    "        # Directional scanning (core SSM-like operation)\n",
    "        x = self.scanner(x)\n",
    "        \n",
    "        # Postprocessing\n",
    "        x = self.norm2(x)\n",
    "        x = self.conv_contract(x)\n",
    "        \n",
    "        # Residual connection\n",
    "        output = x + residual\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MambaBlockStack(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, depth: int = 2, **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VSSBlock(channels, **kwargs) for _ in range(depth)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test VSSBlock\n",
    "    batch_size, channels, height, width = 2, 64, 64, 64\n",
    "    x = torch.randn(batch_size, channels, height, width)\n",
    "    \n",
    "    vss_block = VSSBlock(channels, scan_dim=32)\n",
    "    output = vss_block(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Module parameters: {sum(p.numel() for p in vss_block.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0488aab1",
   "metadata": {},
   "source": [
    "### 2.2 Spectral Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1c8fd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 64, 64, 64])\n",
      "Output shape: torch.Size([2, 64, 64, 64])\n",
      "Module parameters: 270336\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "class SpectralGating(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, height: int, width: int, \n",
    "                 threshold: float = 0.1, complex_init: str = \"kaiming\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # Create learnable complex weights for frequency domain\n",
    "        # Shape: (channels, height, width//2 + 1) for rfft2\n",
    "        self.register_buffer(\n",
    "            \"freq_shape\",\n",
    "            torch.tensor([channels, height, width // 2 + 1], dtype=torch.long)\n",
    "        )\n",
    "        \n",
    "        # Real and Imaginary parts of complex weights\n",
    "        self.weight_real = nn.Parameter(\n",
    "            torch.zeros(channels, height, width // 2 + 1)\n",
    "        )\n",
    "        self.weight_imag = nn.Parameter(\n",
    "            torch.zeros(channels, height, width // 2 + 1)\n",
    "        )\n",
    "        \n",
    "        self._init_weights(complex_init)\n",
    "        \n",
    "    def _init_weights(self, strategy: str = \"kaiming\"):\n",
    "        \n",
    "        if strategy == \"identity\":\n",
    "            # Initialize close to identity (magnitude ~1, phase ~0)\n",
    "            nn.init.ones_(self.weight_real)\n",
    "            nn.init.zeros_(self.weight_imag)\n",
    "        elif strategy == \"kaiming\":\n",
    "            # Kaiming initialization adapted for complex numbers\n",
    "            fan_in = self.height * (self.width // 2 + 1)\n",
    "            std = (2.0 / fan_in) ** 0.5\n",
    "            nn.init.normal_(self.weight_real, 0, std)\n",
    "            nn.init.normal_(self.weight_imag, 0, std)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown init strategy: {strategy}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Apply FFT to convert to frequency domain\n",
    "        # rfft2 returns complex tensor\n",
    "        x_freq = torch.fft.rfft2(x, dim=(-2, -1), norm=\"ortho\")\n",
    "        \n",
    "        # Create complex weight matrix: weight_real + 1j * weight_imag\n",
    "        # Reshape to (1, C, H, W//2+1) for broadcasting\n",
    "        complex_weight = (\n",
    "            self.weight_real.unsqueeze(0) + \n",
    "            1j * self.weight_imag.unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        # Apply channel-wise multiplication in frequency domain\n",
    "        # Shape: (B, C, H, W//2+1) * (1, C, H, W//2+1) -> (B, C, H, W//2+1)\n",
    "        x_filtered = x_freq * complex_weight\n",
    "        \n",
    "        # Optional: Hard thresholding to remove low-amplitude noise\n",
    "        if self.threshold > 0:\n",
    "            magnitude = torch.abs(x_filtered)\n",
    "            mask = magnitude > self.threshold\n",
    "            x_filtered = x_filtered * mask.float()\n",
    "        \n",
    "        # Apply inverse FFT to return to spatial domain\n",
    "        output = torch.fft.irfft2(x_filtered, s=(H, W), dim=(-2, -1), norm=\"ortho\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "class FrequencyLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, weight: float = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Apply FFT\n",
    "        pred_freq = torch.fft.rfft2(pred, dim=(-2, -1), norm=\"ortho\")\n",
    "        target_freq = torch.fft.rfft2(target, dim=(-2, -1), norm=\"ortho\")\n",
    "        \n",
    "        # Compute L2 distance in frequency domain\n",
    "        # Using both magnitude and phase information\n",
    "        loss_real = F.mse_loss(pred_freq.real, target_freq.real)\n",
    "        loss_imag = F.mse_loss(pred_freq.imag, target_freq.imag)\n",
    "        \n",
    "        return loss_real + loss_imag\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test SpectralGating\n",
    "    batch_size, channels, height, width = 2, 64, 64, 64\n",
    "    x = torch.randn(batch_size, channels, height, width)\n",
    "    \n",
    "    spec_gate = SpectralGating(channels, height, width)\n",
    "    output = spec_gate(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Module parameters: {sum(p.numel() for p in spec_gate.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d867eeb",
   "metadata": {},
   "source": [
    "### 2.3 Monogenic Signal Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b7daa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Monogenic Signal Processing...\n",
      "Input shape: torch.Size([1, 1, 128, 128])\n",
      "Energy map shape: torch.Size([1, 1, 128, 128])\n",
      "Energy range: [0.000, 1.000]\n",
      "Monogenic components: ['amplitude', 'phase', 'orientation', 'riesz_x', 'riesz_y']\n",
      "Boundary map shape: torch.Size([1, 1, 128, 128])\n",
      "Boundary range: [0.000, 1.000]\n",
      "\n",
      "âœ“ All tests passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class RieszTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, epsilon: float = 1e-8):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Create frequency grid\n",
    "        freq_y = torch.fft.fftfreq(H, device=x.device, dtype=x.dtype)\n",
    "        freq_x = torch.fft.fftfreq(W, device=x.device, dtype=x.dtype)\n",
    "        freq_y, freq_x = torch.meshgrid(freq_y, freq_x, indexing='ij')\n",
    "        \n",
    "        # Compute radial frequency (avoid division by zero)\n",
    "        radius = torch.sqrt(freq_x**2 + freq_y**2 + self.epsilon)\n",
    "        \n",
    "        # Riesz kernels in frequency domain\n",
    "        # H1 = -j * u / |w|, H2 = -j * v / |w|\n",
    "        kernel_x = freq_x / radius\n",
    "        kernel_y = freq_y / radius\n",
    "        \n",
    "        # Set DC component to zero\n",
    "        kernel_x[0, 0] = 0\n",
    "        kernel_y[0, 0] = 0\n",
    "        \n",
    "        # Apply FFT to input\n",
    "        x_fft = torch.fft.fft2(x)\n",
    "        \n",
    "        # Apply Riesz kernels (multiplication by -j in frequency = Hilbert-like)\n",
    "        # -j * X = real(X) * (-j) + imag(X) * (-j) * j = imag(X) - j*real(X)\n",
    "        riesz_x_fft = -1j * x_fft * kernel_x.unsqueeze(0).unsqueeze(0)\n",
    "        riesz_y_fft = -1j * x_fft * kernel_y.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Inverse FFT\n",
    "        riesz_x = torch.fft.ifft2(riesz_x_fft).real\n",
    "        riesz_y = torch.fft.ifft2(riesz_y_fft).real\n",
    "        \n",
    "        return riesz_x, riesz_y\n",
    "\n",
    "class LogGaborFilter(nn.Module):\n",
    "\n",
    "    def __init__(self, num_scales: int = 4, num_orientations: int = 6,\n",
    "                 min_wavelength: float = 3.0, mult: float = 2.1,\n",
    "                 sigma_on_f: float = 0.55):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_scales = num_scales\n",
    "        self.num_orientations = num_orientations\n",
    "        self.min_wavelength = min_wavelength\n",
    "        self.mult = mult\n",
    "        self.sigma_on_f = sigma_on_f\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        device = x.device\n",
    "        dtype = x.dtype\n",
    "        \n",
    "        # Create frequency grid\n",
    "        freq_y = torch.fft.fftfreq(H, device=device, dtype=dtype)\n",
    "        freq_x = torch.fft.fftfreq(W, device=device, dtype=dtype)\n",
    "        freq_y, freq_x = torch.meshgrid(freq_y, freq_x, indexing='ij')\n",
    "        \n",
    "        # Polar coordinates\n",
    "        radius = torch.sqrt(freq_x**2 + freq_y**2)\n",
    "        radius[0, 0] = 1  # Avoid log(0)\n",
    "        theta = torch.atan2(freq_y, freq_x)\n",
    "        \n",
    "        # FFT of input\n",
    "        x_fft = torch.fft.fft2(x)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for scale in range(self.num_scales):\n",
    "            wavelength = self.min_wavelength * (self.mult ** scale)\n",
    "            fo = 1.0 / wavelength  # Center frequency\n",
    "            \n",
    "            # Log-Gabor radial component\n",
    "            log_gabor_radial = torch.exp(\n",
    "                -(torch.log(radius / fo) ** 2) / (2 * math.log(self.sigma_on_f) ** 2)\n",
    "            )\n",
    "            log_gabor_radial[0, 0] = 0  # Zero DC\n",
    "            \n",
    "            for orient in range(self.num_orientations):\n",
    "                angle = orient * math.pi / self.num_orientations\n",
    "                \n",
    "                # Angular component\n",
    "                ds = torch.sin(theta - angle)\n",
    "                dc = torch.cos(theta - angle)\n",
    "                dtheta = torch.abs(torch.atan2(ds, dc))\n",
    "                \n",
    "                # Angular spread\n",
    "                angular_spread = torch.exp(\n",
    "                    -(dtheta ** 2) / (2 * (math.pi / self.num_orientations) ** 2)\n",
    "                )\n",
    "                \n",
    "                # Combined filter\n",
    "                log_gabor = log_gabor_radial * angular_spread\n",
    "                \n",
    "                # Apply filter\n",
    "                filtered = torch.fft.ifft2(x_fft * log_gabor.unsqueeze(0).unsqueeze(0))\n",
    "                outputs.append(filtered.abs())\n",
    "        \n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "class MonogenicSignal(nn.Module):\n",
    "\n",
    "    def __init__(self, epsilon: float = 1e-8):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.riesz = RieszTransform(epsilon=epsilon)\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> dict:\n",
    "        \n",
    "        # Get Riesz components\n",
    "        riesz_x, riesz_y = self.riesz(x)\n",
    "        \n",
    "        # Compute amplitude (local energy)\n",
    "        # A = sqrt(f^2 + h1^2 + h2^2)\n",
    "        amplitude = torch.sqrt(x**2 + riesz_x**2 + riesz_y**2 + self.epsilon)\n",
    "        \n",
    "        # Compute orientation\n",
    "        # theta = atan2(h2, h1)\n",
    "        orientation = torch.atan2(riesz_y, riesz_x + self.epsilon)\n",
    "        \n",
    "        # Compute phase\n",
    "        # phi = atan2(sqrt(h1^2 + h2^2), f)\n",
    "        riesz_magnitude = torch.sqrt(riesz_x**2 + riesz_y**2 + self.epsilon)\n",
    "        phase = torch.atan2(riesz_magnitude, x + self.epsilon)\n",
    "        \n",
    "        return {\n",
    "            'amplitude': amplitude,\n",
    "            'phase': phase,\n",
    "            'orientation': orientation,\n",
    "            'riesz_x': riesz_x,\n",
    "            'riesz_y': riesz_y\n",
    "        }\n",
    "\n",
    "class EnergyMap(nn.Module):\n",
    "\n",
    "    def __init__(self, normalize: bool = True, smoothing_sigma: float = 1.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.monogenic = MonogenicSignal()\n",
    "        self.normalize = normalize\n",
    "        self.smoothing_sigma = smoothing_sigma\n",
    "        \n",
    "        # Create Gaussian smoothing kernel\n",
    "        if smoothing_sigma > 0:\n",
    "            kernel_size = int(6 * smoothing_sigma) | 1  # Ensure odd\n",
    "            self.register_buffer('smooth_kernel', self._create_gaussian_kernel(\n",
    "                kernel_size, smoothing_sigma\n",
    "            ))\n",
    "        else:\n",
    "            self.smooth_kernel = None\n",
    "    \n",
    "    def _create_gaussian_kernel(self, kernel_size: int, sigma: float) -> torch.Tensor:\n",
    "        \n",
    "        x = torch.arange(kernel_size) - kernel_size // 2\n",
    "        x = x.float()\n",
    "        gaussian_1d = torch.exp(-x**2 / (2 * sigma**2))\n",
    "        gaussian_2d = gaussian_1d.unsqueeze(0) * gaussian_1d.unsqueeze(1)\n",
    "        gaussian_2d = gaussian_2d / gaussian_2d.sum()\n",
    "        return gaussian_2d.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, dict]:\n",
    "        \n",
    "        # Convert to grayscale if needed\n",
    "        if x.shape[1] > 1:\n",
    "            x = x.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        # Get monogenic decomposition\n",
    "        mono_out = self.monogenic(x)\n",
    "        \n",
    "        # Energy is the amplitude\n",
    "        energy = mono_out['amplitude']\n",
    "        \n",
    "        # Optional smoothing\n",
    "        if self.smooth_kernel is not None:\n",
    "            pad = self.smooth_kernel.shape[-1] // 2\n",
    "            energy = F.conv2d(energy, self.smooth_kernel, padding=pad)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        if self.normalize:\n",
    "            B = energy.shape[0]\n",
    "            energy_flat = energy.view(B, -1)\n",
    "            energy_min = energy_flat.min(dim=1, keepdim=True)[0].view(B, 1, 1, 1)\n",
    "            energy_max = energy_flat.max(dim=1, keepdim=True)[0].view(B, 1, 1, 1)\n",
    "            energy = (energy - energy_min) / (energy_max - energy_min + 1e-8)\n",
    "        \n",
    "        return energy, mono_out\n",
    "\n",
    "class BoundaryDetector(nn.Module):\n",
    "\n",
    "    def __init__(self, num_scales: int = 4, num_orientations: int = 6,\n",
    "                 noise_threshold: float = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.log_gabor = LogGaborFilter(num_scales, num_orientations)\n",
    "        self.num_scales = num_scales\n",
    "        self.num_orientations = num_orientations\n",
    "        self.noise_threshold = noise_threshold\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Get multi-scale responses\n",
    "        responses = self.log_gabor(x)  # (B, S*O, H, W)\n",
    "        \n",
    "        # Sum across orientations to get edge strength per scale\n",
    "        B, _, H, W = responses.shape\n",
    "        responses = responses.view(B, self.num_scales, self.num_orientations, H, W)\n",
    "        \n",
    "        # Max across orientations (strongest edge direction)\n",
    "        edge_strength = responses.max(dim=2)[0]  # (B, S, H, W)\n",
    "        \n",
    "        # Sum across scales\n",
    "        edge_strength = edge_strength.sum(dim=1, keepdim=True)  # (B, 1, H, W)\n",
    "        \n",
    "        # Normalize and threshold\n",
    "        edge_max = edge_strength.view(B, -1).max(dim=1)[0].view(B, 1, 1, 1)\n",
    "        edge_strength = edge_strength / (edge_max + 1e-8)\n",
    "        edge_strength = torch.clamp(edge_strength - self.noise_threshold, min=0)\n",
    "        edge_strength = edge_strength / (1 - self.noise_threshold + 1e-8)\n",
    "        \n",
    "        return edge_strength\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test Monogenic Signal processing\n",
    "    print(\"Testing Monogenic Signal Processing...\")\n",
    "    \n",
    "    # Create test image with edges\n",
    "    H, W = 128, 128\n",
    "    x = torch.zeros(1, 1, H, W)\n",
    "    x[:, :, 32:96, 32:96] = 1.0  # Square\n",
    "    \n",
    "    # Add some noise\n",
    "    x = x + 0.1 * torch.randn_like(x)\n",
    "    \n",
    "    # Test Energy Map\n",
    "    energy_extractor = EnergyMap(normalize=True)\n",
    "    energy, mono = energy_extractor(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Energy map shape: {energy.shape}\")\n",
    "    print(f\"Energy range: [{energy.min():.3f}, {energy.max():.3f}]\")\n",
    "    print(f\"Monogenic components: {list(mono.keys())}\")\n",
    "    \n",
    "    # Test Boundary Detector\n",
    "    boundary_detector = BoundaryDetector()\n",
    "    boundaries = boundary_detector(x)\n",
    "    \n",
    "    print(f\"Boundary map shape: {boundaries.shape}\")\n",
    "    print(f\"Boundary range: [{boundaries.min():.3f}, {boundaries.max():.3f}]\")\n",
    "    \n",
    "    print(\"\\nâœ“ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd15abcd",
   "metadata": {},
   "source": [
    "### 2.4 Gabor Implicit Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f2b266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Gabor Implicit Modules...\n",
      "\n",
      "[1] Testing GaborBasis...\n",
      "Input coords: torch.Size([4, 100, 2])\n",
      "Gabor encoded: torch.Size([4, 100, 64])\n",
      "\n",
      "[2] Testing GaborNet...\n",
      "GaborNet output: torch.Size([4, 100, 3])\n",
      "\n",
      "[3] Testing ImplicitSegmentationHead...\n",
      "Feature map: torch.Size([2, 64, 32, 32])\n",
      "Segmentation output (grid): torch.Size([2, 3, 128, 128])\n",
      "Segmentation output (points): torch.Size([2, 500, 3])\n",
      "\n",
      "âœ“ All tests passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "class GaborBasis(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int = 2, num_frequencies: int = 64,\n",
    "                 sigma_range: Tuple[float, float] = (0.1, 2.0),\n",
    "                 freq_range: Tuple[float, float] = (1.0, 10.0),\n",
    "                 learnable: bool = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.output_dim = num_frequencies * 2  # sin and cos components\n",
    "        \n",
    "        # Initialize frequencies uniformly in log space\n",
    "        log_freqs = torch.linspace(\n",
    "            math.log(freq_range[0]), \n",
    "            math.log(freq_range[1]), \n",
    "            num_frequencies\n",
    "        )\n",
    "        freqs = torch.exp(log_freqs)\n",
    "        \n",
    "        # Initialize sigmas (Gaussian envelope widths)\n",
    "        sigmas = torch.linspace(sigma_range[0], sigma_range[1], num_frequencies)\n",
    "        \n",
    "        # Random orientations for 2D\n",
    "        orientations = torch.rand(num_frequencies) * 2 * math.pi\n",
    "        \n",
    "        # Random phases\n",
    "        phases = torch.rand(num_frequencies) * 2 * math.pi\n",
    "        \n",
    "        # Create direction vectors from orientations\n",
    "        directions = torch.stack([\n",
    "            torch.cos(orientations),\n",
    "            torch.sin(orientations)\n",
    "        ], dim=-1)  # (num_freq, 2)\n",
    "        \n",
    "        if learnable:\n",
    "            self.freqs = nn.Parameter(freqs)\n",
    "            self.sigmas = nn.Parameter(sigmas)\n",
    "            self.directions = nn.Parameter(directions)\n",
    "            self.phases = nn.Parameter(phases)\n",
    "        else:\n",
    "            self.register_buffer('freqs', freqs)\n",
    "            self.register_buffer('sigmas', sigmas)\n",
    "            self.register_buffer('directions', directions)\n",
    "            self.register_buffer('phases', phases)\n",
    "    \n",
    "    def forward(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Normalize directions\n",
    "        directions = F.normalize(self.directions, dim=-1)  # (num_freq, 2)\n",
    "        \n",
    "        # Project coordinates onto directions\n",
    "        # coords: (..., 2), directions: (num_freq, 2)\n",
    "        proj = torch.matmul(coords, directions.T)  # (..., num_freq)\n",
    "        \n",
    "        # Compute Gaussian envelope\n",
    "        # exp(-projÂ² / (2ÏƒÂ²))\n",
    "        sigmas = torch.abs(self.sigmas) + 0.01  # Ensure positive\n",
    "        gaussian = torch.exp(-proj**2 / (2 * sigmas**2 + 1e-8))\n",
    "        \n",
    "        # Compute oscillatory component\n",
    "        # cos(2Ï€fÂ·proj + Ï†), sin(2Ï€fÂ·proj + Ï†)\n",
    "        freqs = torch.abs(self.freqs) + 0.1  # Ensure positive\n",
    "        arg = 2 * math.pi * freqs * proj + self.phases\n",
    "        \n",
    "        cos_comp = gaussian * torch.cos(arg)\n",
    "        sin_comp = gaussian * torch.sin(arg)\n",
    "        \n",
    "        # Concatenate sin and cos\n",
    "        gabor_features = torch.cat([cos_comp, sin_comp], dim=-1)\n",
    "        \n",
    "        return gabor_features\n",
    "\n",
    "class FourierFeatures(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int = 2, num_frequencies: int = 64,\n",
    "                 scale: float = 10.0, learnable: bool = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.output_dim = num_frequencies * 2\n",
    "        \n",
    "        # Random frequency matrix\n",
    "        B = torch.randn(input_dim, num_frequencies) * scale\n",
    "        \n",
    "        if learnable:\n",
    "            self.B = nn.Parameter(B)\n",
    "        else:\n",
    "            self.register_buffer('B', B)\n",
    "    \n",
    "    def forward(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Project: coords @ B\n",
    "        proj = 2 * math.pi * torch.matmul(coords, self.B)  # (..., num_freq)\n",
    "        \n",
    "        # Sin and cos\n",
    "        return torch.cat([torch.cos(proj), torch.sin(proj)], dim=-1)\n",
    "\n",
    "class SIRENLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, \n",
    "                 omega_0: float = 30.0, is_first: bool = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.omega_0 = omega_0\n",
    "        self.is_first = is_first\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if self.is_first:\n",
    "                # First layer: uniform in [-1/n, 1/n]\n",
    "                self.linear.weight.uniform_(-1 / self.in_features, \n",
    "                                            1 / self.in_features)\n",
    "            else:\n",
    "                # Other layers: uniform in [-sqrt(6/n)/Ï‰â‚€, sqrt(6/n)/Ï‰â‚€]\n",
    "                bound = math.sqrt(6 / self.in_features) / self.omega_0\n",
    "                self.linear.weight.uniform_(-bound, bound)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sin(self.omega_0 * self.linear(x))\n",
    "\n",
    "class GaborNet(nn.Module):\n",
    "\n",
    "    def __init__(self, coord_dim: int = 2, feature_dim: int = 256,\n",
    "                 hidden_dim: int = 256, output_dim: int = 1,\n",
    "                 num_layers: int = 4, num_frequencies: int = 64,\n",
    "                 use_gabor: bool = True, omega_0: float = 30.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Coordinate encoding\n",
    "        if use_gabor:\n",
    "            self.coord_encoder = GaborBasis(\n",
    "                input_dim=coord_dim,\n",
    "                num_frequencies=num_frequencies,\n",
    "                learnable=True\n",
    "            )\n",
    "        else:\n",
    "            self.coord_encoder = FourierFeatures(\n",
    "                input_dim=coord_dim,\n",
    "                num_frequencies=num_frequencies,\n",
    "                learnable=False\n",
    "            )\n",
    "        \n",
    "        coord_encoded_dim = self.coord_encoder.output_dim\n",
    "        \n",
    "        # Input dimension: encoded coords + features\n",
    "        input_dim = coord_encoded_dim + feature_dim\n",
    "        \n",
    "        # Build SIREN network\n",
    "        layers = []\n",
    "        \n",
    "        # First layer\n",
    "        layers.append(SIRENLayer(input_dim, hidden_dim, omega_0, is_first=True))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(SIRENLayer(hidden_dim, hidden_dim, omega_0, is_first=False))\n",
    "        \n",
    "        # Final layer (linear, no sine activation)\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, coords: torch.Tensor, features: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Encode coordinates\n",
    "        coord_encoded = self.coord_encoder(coords)  # (B, N, coord_encoded_dim)\n",
    "        \n",
    "        # Concatenate with features\n",
    "        x = torch.cat([coord_encoded, features], dim=-1)  # (B, N, input_dim)\n",
    "        \n",
    "        # Pass through network\n",
    "        output = self.network(x)  # (B, N, output_dim)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class ImplicitSegmentationHead(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_channels: int = 64, num_classes: int = 2,\n",
    "                 hidden_dim: int = 256, num_layers: int = 4,\n",
    "                 num_frequencies: int = 64, use_gabor: bool = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_channels = feature_channels\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Feature projector (reduce channel dimension)\n",
    "        self.feature_proj = nn.Sequential(\n",
    "            nn.Conv2d(feature_channels, hidden_dim, kernel_size=1),\n",
    "            nn.GroupNorm(32, hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Implicit decoder\n",
    "        self.implicit_decoder = GaborNet(\n",
    "            coord_dim=2,\n",
    "            feature_dim=hidden_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=num_classes,\n",
    "            num_layers=num_layers,\n",
    "            num_frequencies=num_frequencies,\n",
    "            use_gabor=use_gabor\n",
    "        )\n",
    "    \n",
    "    def sample_features(self, feature_map: torch.Tensor, \n",
    "                       coords: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H, W = feature_map.shape\n",
    "        N = coords.shape[1]\n",
    "        \n",
    "        # Reshape coords for grid_sample: (B, N, 1, 2) -> (B, 1, N, 2)\n",
    "        # grid_sample expects (B, H, W, 2) where last dim is (x, y)\n",
    "        grid = coords.view(B, 1, N, 2)\n",
    "        \n",
    "        # Sample using bilinear interpolation\n",
    "        # feature_map: (B, C, H, W), grid: (B, 1, N, 2)\n",
    "        # output: (B, C, 1, N)\n",
    "        sampled = F.grid_sample(\n",
    "            feature_map, grid,\n",
    "            mode='bilinear',\n",
    "            padding_mode='border',\n",
    "            align_corners=True\n",
    "        )\n",
    "        \n",
    "        # Reshape: (B, C, 1, N) -> (B, N, C)\n",
    "        sampled = sampled.squeeze(2).permute(0, 2, 1)\n",
    "        \n",
    "        return sampled\n",
    "    \n",
    "    def forward(self, feature_map: torch.Tensor, \n",
    "                coords: Optional[torch.Tensor] = None,\n",
    "                output_size: Optional[Tuple[int, int]] = None) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H_feat, W_feat = feature_map.shape\n",
    "        device = feature_map.device\n",
    "        \n",
    "        # Project features\n",
    "        feature_map = self.feature_proj(feature_map)  # (B, hidden_dim, H, W)\n",
    "        \n",
    "        # Generate coordinates if not provided\n",
    "        if coords is None:\n",
    "            if output_size is None:\n",
    "                output_size = (H_feat * 4, W_feat * 4)\n",
    "            \n",
    "            H_out, W_out = output_size\n",
    "            \n",
    "            # Create normalized coordinate grid [-1, 1]\n",
    "            y = torch.linspace(-1, 1, H_out, device=device)\n",
    "            x = torch.linspace(-1, 1, W_out, device=device)\n",
    "            yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "            coords = torch.stack([xx, yy], dim=-1)  # (H_out, W_out, 2)\n",
    "            coords = coords.view(1, -1, 2).expand(B, -1, -1)  # (B, H*W, 2)\n",
    "            \n",
    "            reshape_output = True\n",
    "        else:\n",
    "            reshape_output = False\n",
    "            H_out, W_out = None, None\n",
    "        \n",
    "        # Sample features at coordinates\n",
    "        features = self.sample_features(feature_map, coords)  # (B, N, hidden_dim)\n",
    "        \n",
    "        # Implicit decoding\n",
    "        logits = self.implicit_decoder(coords, features)  # (B, N, num_classes)\n",
    "        \n",
    "        # Reshape to image if using grid\n",
    "        if reshape_output:\n",
    "            logits = logits.view(B, H_out, W_out, self.num_classes)\n",
    "            logits = logits.permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing Gabor Implicit Modules...\")\n",
    "    \n",
    "    # Test Gabor Basis\n",
    "    print(\"\\n[1] Testing GaborBasis...\")\n",
    "    gabor = GaborBasis(input_dim=2, num_frequencies=32)\n",
    "    coords = torch.randn(4, 100, 2)  # (B, N, 2)\n",
    "    encoded = gabor(coords)\n",
    "    print(f\"Input coords: {coords.shape}\")\n",
    "    print(f\"Gabor encoded: {encoded.shape}\")\n",
    "    \n",
    "    # Test GaborNet\n",
    "    print(\"\\n[2] Testing GaborNet...\")\n",
    "    net = GaborNet(coord_dim=2, feature_dim=64, hidden_dim=128, \n",
    "                   output_dim=3, num_layers=3, num_frequencies=32)\n",
    "    features = torch.randn(4, 100, 64)\n",
    "    output = net(coords, features)\n",
    "    print(f\"GaborNet output: {output.shape}\")\n",
    "    \n",
    "    # Test ImplicitSegmentationHead\n",
    "    print(\"\\n[3] Testing ImplicitSegmentationHead...\")\n",
    "    head = ImplicitSegmentationHead(\n",
    "        feature_channels=64, num_classes=3,\n",
    "        hidden_dim=128, num_layers=3, num_frequencies=32\n",
    "    )\n",
    "    feature_map = torch.randn(2, 64, 32, 32)\n",
    "    \n",
    "    # Test with automatic grid\n",
    "    seg_output = head(feature_map, output_size=(128, 128))\n",
    "    print(f\"Feature map: {feature_map.shape}\")\n",
    "    print(f\"Segmentation output (grid): {seg_output.shape}\")\n",
    "    \n",
    "    # Test with custom coordinates\n",
    "    custom_coords = torch.rand(2, 500, 2) * 2 - 1  # Random points in [-1, 1]\n",
    "    seg_points = head(feature_map, coords=custom_coords)\n",
    "    print(f\"Segmentation output (points): {seg_points.shape}\")\n",
    "    \n",
    "    print(\"\\nâœ“ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14d5fe",
   "metadata": {},
   "source": [
    "### 2.5 Spectral Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1093c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 1, 256, 256])\n",
      "Output shape: torch.Size([2, 3, 256, 256])\n",
      "Total parameters: 10,312,523\n",
      "Trainable parameters: 10,312,523\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, List\n",
    "\n",
    "class SpectralVSSBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, height: int, width: int,\n",
    "                 depth: int = 2, expansion_ratio: float = 2.0,\n",
    "                 threshold: float = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # Branch A: Spatial path (VSS Blocks)\n",
    "        self.vss_blocks = MambaBlockStack(\n",
    "            channels, depth=depth, \n",
    "            expansion_ratio=expansion_ratio, \n",
    "            scan_dim=min(64, channels)\n",
    "        )\n",
    "        \n",
    "        # Branch B: Spectral path (FFT-based filtering)\n",
    "        self.spectral_gate = SpectralGating(\n",
    "            channels, height, width, \n",
    "            threshold=threshold, \n",
    "            complex_init=\"kaiming\"\n",
    "        )\n",
    "        \n",
    "        # Fusion layer (learnable weighting)\n",
    "        self.fusion_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Branch A: Spatial context (VSS)\n",
    "        spatial_out = self.vss_blocks(x)\n",
    "        \n",
    "        # Branch B: Frequency filtering (Spectral)\n",
    "        spectral_out = self.spectral_gate(x)\n",
    "        \n",
    "        # Learnable fusion with sigmoid weight\n",
    "        weight = torch.sigmoid(self.fusion_weight)\n",
    "        output = weight * spatial_out + (1 - weight) * spectral_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 3, out_channels: int = 64, \n",
    "                 patch_size: int = 4):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels,\n",
    "            kernel_size=patch_size, stride=patch_size, bias=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, out_channels: Optional[int] = None):\n",
    "        \n",
    "        super().__init__()\n",
    "        out_channels = out_channels or channels * 2\n",
    "        self.conv = nn.Conv2d(channels, out_channels, kernel_size=2, \n",
    "                              stride=2, bias=True)\n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "class PatchExpanding(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, out_channels: Optional[int] = None):\n",
    "        \n",
    "        super().__init__()\n",
    "        out_channels = out_channels or channels // 2\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', \n",
    "                                    align_corners=True)\n",
    "        self.conv = nn.Conv2d(channels, out_channels, kernel_size=1, bias=True)\n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "class SpectralVMUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 1, out_channels: int = 3,\n",
    "                 img_size: int = 256, base_channels: int = 64,\n",
    "                 num_stages: int = 4, depth: int = 2):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.img_size = img_size\n",
    "        self.base_channels = base_channels\n",
    "        self.num_stages = num_stages\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(in_channels, base_channels, patch_size=4)\n",
    "        initial_size = img_size // 4\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_stages):\n",
    "            in_ch = base_channels * (2 ** i)\n",
    "            out_ch = in_ch\n",
    "            h = w = initial_size // (2 ** i)\n",
    "            \n",
    "            # SpectralVSSBlock\n",
    "            block = SpectralVSSBlock(\n",
    "                in_ch, h, w, depth=depth, expansion_ratio=2.0, threshold=0.1\n",
    "            )\n",
    "            self.encoder_blocks.append(block)\n",
    "            \n",
    "            # Downsampling (except after last encoder block)\n",
    "            if i < num_stages - 1:\n",
    "                down = PatchMerging(in_ch, in_ch * 2)\n",
    "                self.downsample_layers.append(down)\n",
    "        \n",
    "        # Bottleneck - uses the last encoder's output channels\n",
    "        # After num_stages-1 downsamplings, channels = base_channels * 2^(num_stages-1)\n",
    "        bottleneck_ch = base_channels * (2 ** (num_stages - 1))\n",
    "        bottleneck_h = bottleneck_w = initial_size // (2 ** (num_stages - 1))\n",
    "        self.bottleneck = SpectralVSSBlock(\n",
    "            bottleneck_ch, bottleneck_h, bottleneck_w,\n",
    "            depth=depth + 1, expansion_ratio=2.0, threshold=0.1\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        # We have num_stages - 1 decoder stages (matching skip connections)\n",
    "        # Each decoder stage: upsample -> concat with skip -> fusion -> SpectralVSSBlock\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        self.upsample_layers = nn.ModuleList()\n",
    "        \n",
    "        num_decoder_stages = num_stages - 1\n",
    "        \n",
    "        for i in range(num_decoder_stages):\n",
    "            # Going from deepest to shallowest\n",
    "            # i=0: from bottleneck (8x8, 512ch) -> upsample to (16x16, 256ch)\n",
    "            # i=1: from 16x16, 256ch -> upsample to (32x32, 128ch)\n",
    "            # i=2: from 32x32, 128ch -> upsample to (64x64, 64ch)\n",
    "            \n",
    "            # Input channels: for i=0, it's bottleneck_ch; else from previous decoder output\n",
    "            if i == 0:\n",
    "                in_ch = bottleneck_ch  # 512 for default\n",
    "            else:\n",
    "                in_ch = base_channels * (2 ** (num_stages - 1 - i))\n",
    "            \n",
    "            # Output channels after upsampling\n",
    "            out_ch = base_channels * (2 ** (num_stages - 2 - i))\n",
    "            \n",
    "            # Upsampling layer\n",
    "            up = PatchExpanding(in_ch, out_ch)\n",
    "            self.upsample_layers.append(up)\n",
    "            \n",
    "            # Skip connection comes from encoder at level (num_decoder_stages - 1 - i)\n",
    "            # which has same spatial size after upsampling\n",
    "            skip_ch = out_ch  # Skip has same channels as upsampled output\n",
    "            \n",
    "            # Spatial size at this level\n",
    "            h = w = initial_size // (2 ** (num_stages - 2 - i))\n",
    "            \n",
    "            # Fusion: concatenate upsampled + skip, then reduce channels\n",
    "            fused_ch = out_ch + skip_ch  # After concatenation\n",
    "            fusion = nn.Sequential(\n",
    "                nn.Conv2d(fused_ch, out_ch, kernel_size=1, bias=True),\n",
    "                nn.GroupNorm(num_groups=min(32, out_ch), num_channels=out_ch, eps=1e-6)\n",
    "            )\n",
    "            self.decoder_blocks.append(fusion)\n",
    "            \n",
    "            # SpectralVSSBlock after fusion\n",
    "            vss = SpectralVSSBlock(\n",
    "                out_ch, h, w, depth=depth, expansion_ratio=2.0, threshold=0.1\n",
    "            )\n",
    "            self.decoder_blocks.append(vss)\n",
    "        \n",
    "        # Segmentation head\n",
    "        self.seg_head = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels // 2, kernel_size=3, \n",
    "                      padding=1, bias=True),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=base_channels // 2, eps=1e-6),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(base_channels // 2, out_channels, kernel_size=1, bias=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Encoder path with skip connections storage\n",
    "        # Skip connections are saved BEFORE downsampling\n",
    "        skips = []\n",
    "        for i in range(self.num_stages):\n",
    "            x = self.encoder_blocks[i](x)\n",
    "            # Save skip connection before downsampling\n",
    "            if i < self.num_stages - 1:\n",
    "                skips.append(x)\n",
    "                x = self.downsample_layers[i](x)\n",
    "        \n",
    "        # The last encoder output goes to bottleneck (no skip for this level)\n",
    "        # skips now contains: [stage0_out, stage1_out, stage2_out] for 4 stages\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        # Decoder stages: num_stages - 1 (since last encoder has no skip)\n",
    "        num_decoder_stages = self.num_stages - 1\n",
    "        \n",
    "        for i in range(num_decoder_stages):\n",
    "            # Upsample\n",
    "            x = self.upsample_layers[i](x)\n",
    "            \n",
    "            # Concatenate skip connection (in reverse order)\n",
    "            # For i=0: skip from encoder stage num_stages-2 (last skip)\n",
    "            # For i=1: skip from encoder stage num_stages-3\n",
    "            skip_idx = num_decoder_stages - 1 - i\n",
    "            skip = skips[skip_idx]\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            \n",
    "            # Fusion and processing\n",
    "            x = self.decoder_blocks[2 * i](x)  # Fusion conv\n",
    "            x = self.decoder_blocks[2 * i + 1](x)  # SpectralVSSBlock\n",
    "        \n",
    "        # Segmentation head\n",
    "        output = self.seg_head(x)\n",
    "        \n",
    "        # Upsample to original resolution (since patch embedding uses stride 4)\n",
    "        output = F.interpolate(output, size=(self.img_size, self.img_size),\n",
    "                               mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the full architecture\n",
    "    batch_size = 2\n",
    "    in_channels = 1\n",
    "    out_channels = 3  # Binary segmentation + background\n",
    "    img_size = 256\n",
    "    \n",
    "    model = SpectralVMUNet(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        img_size=img_size,\n",
    "        base_channels=64,\n",
    "        num_stages=4,\n",
    "        depth=2\n",
    "    )\n",
    "    \n",
    "    # Create dummy input\n",
    "    x = torch.randn(batch_size, in_channels, img_size, img_size)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72355acc",
   "metadata": {},
   "source": [
    "### 2.6 EGM-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b729a20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing EGM-Net (Energy-Gated Gabor Mamba Network)\n",
      "============================================================\n",
      "\n",
      "[1] Testing EGM-Net Full...\n",
      "Input: torch.Size([2, 1, 256, 256])\n",
      "Output: torch.Size([2, 3, 256, 256])\n",
      "Coarse: torch.Size([2, 3, 256, 256])\n",
      "Fine: torch.Size([2, 3, 256, 256])\n",
      "Energy: torch.Size([2, 1, 256, 256])\n",
      "\n",
      "Total parameters: 9,133,192\n",
      "Trainable parameters: 9,133,192\n",
      "\n",
      "[2] Testing Point Query (Resolution-Free)...\n",
      "Query coords: torch.Size([2, 1000, 2])\n",
      "Point output: torch.Size([2, 1000, 3])\n",
      "\n",
      "[3] Testing EGM-Net Lite...\n",
      "Lite model parameters: 635,272\n",
      "Lite output: torch.Size([2, 3, 256, 256])\n",
      "\n",
      "============================================================\n",
      "âœ“ All tests passed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple, Dict\n",
    "import math\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 1, embed_dim: int = 64, patch_size: int = 4):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, \n",
    "                              stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.proj(x)  # (B, C, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "class DownsampleBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.norm = nn.GroupNorm(32, out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.norm(self.conv(x))\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.norm = nn.GroupNorm(min(32, out_channels), out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.norm(self.conv(self.up(x)))\n",
    "\n",
    "class MambaEncoderStage(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, depth: int = 2, spatial_size: int = 64):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VSSBlock(channels, scan_dim=min(64, channels))\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "class CoarseBranch(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, num_classes: int, num_stages: int = 3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.upsample_layers = nn.ModuleList()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        \n",
    "        channels = in_channels\n",
    "        for i in range(num_stages):\n",
    "            out_ch = max(channels // 2, 64)\n",
    "            self.upsample_layers.append(UpsampleBlock(channels, out_ch))\n",
    "            self.conv_layers.append(nn.Sequential(\n",
    "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "                nn.GroupNorm(min(32, out_ch), out_ch),\n",
    "                nn.GELU()\n",
    "            ))\n",
    "            channels = out_ch\n",
    "        \n",
    "        self.head = nn.Conv2d(channels, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for up, conv in zip(self.upsample_layers, self.conv_layers):\n",
    "            x = up(x)\n",
    "            x = conv(x)\n",
    "        return self.head(x)\n",
    "\n",
    "class EnergyGatedFusion(nn.Module):\n",
    "\n",
    "    def __init__(self, temperature: float = 1.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.gate_scale = nn.Parameter(torch.ones(1))\n",
    "        self.gate_bias = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, coarse: torch.Tensor, fine: torch.Tensor, \n",
    "                energy: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Resize energy to match prediction size\n",
    "        if energy.shape[-2:] != coarse.shape[-2:]:\n",
    "            energy = F.interpolate(energy, size=coarse.shape[-2:], \n",
    "                                   mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # Apply learnable scaling and temperature\n",
    "        gate = torch.sigmoid((energy * self.gate_scale + self.gate_bias) / self.temperature)\n",
    "        \n",
    "        # Blend: high energy â†’ use fine, low energy â†’ use coarse\n",
    "        output = coarse + gate * (fine - coarse)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class FineBranch(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_channels: int, num_classes: int,\n",
    "                 hidden_dim: int = 256, num_layers: int = 4,\n",
    "                 num_frequencies: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.implicit_head = ImplicitSegmentationHead(\n",
    "            feature_channels=feature_channels,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_frequencies=num_frequencies,\n",
    "            use_gabor=True  # Use Gabor instead of Fourier\n",
    "        )\n",
    "    \n",
    "    def forward(self, features: torch.Tensor, \n",
    "                coords: Optional[torch.Tensor] = None,\n",
    "                output_size: Optional[Tuple[int, int]] = None) -> torch.Tensor:\n",
    "        \n",
    "        return self.implicit_head(features, coords, output_size)\n",
    "\n",
    "class EGMNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 1, num_classes: int = 2,\n",
    "                 img_size: int = 256, base_channels: int = 64,\n",
    "                 num_stages: int = 4, encoder_depth: int = 2,\n",
    "                 implicit_hidden: int = 256, implicit_layers: int = 4,\n",
    "                 num_frequencies: int = 64):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.num_stages = num_stages\n",
    "        \n",
    "        # 1. Monogenic Energy Extractor (fixed, non-trainable)\n",
    "        self.energy_extractor = EnergyMap(normalize=True, smoothing_sigma=1.0)\n",
    "        \n",
    "        # 2. Patch Embedding\n",
    "        self.patch_embed = PatchEmbedding(in_channels, base_channels, patch_size=4)\n",
    "        feat_size = img_size // 4\n",
    "        \n",
    "        # 3. Mamba Encoder\n",
    "        self.encoder_stages = nn.ModuleList()\n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "        \n",
    "        channels = base_channels\n",
    "        for i in range(num_stages):\n",
    "            self.encoder_stages.append(\n",
    "                MambaEncoderStage(channels, depth=encoder_depth, spatial_size=feat_size)\n",
    "            )\n",
    "            if i < num_stages - 1:\n",
    "                self.downsample_layers.append(\n",
    "                    DownsampleBlock(channels, channels * 2)\n",
    "                )\n",
    "                channels *= 2\n",
    "                feat_size //= 2\n",
    "        \n",
    "        # Store final encoder channels\n",
    "        self.encoder_channels = channels\n",
    "        \n",
    "        # 4. Bottleneck\n",
    "        self.bottleneck = MambaEncoderStage(\n",
    "            channels, depth=encoder_depth + 1, spatial_size=feat_size\n",
    "        )\n",
    "        \n",
    "        # 5. Coarse Branch (standard decoder)\n",
    "        self.coarse_branch = CoarseBranch(\n",
    "            in_channels=channels,\n",
    "            num_classes=num_classes,\n",
    "            num_stages=num_stages - 1\n",
    "        )\n",
    "        \n",
    "        # 6. Fine Branch (Gabor implicit decoder)\n",
    "        self.fine_branch = FineBranch(\n",
    "            feature_channels=channels,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dim=implicit_hidden,\n",
    "            num_layers=implicit_layers,\n",
    "            num_frequencies=num_frequencies\n",
    "        )\n",
    "        \n",
    "        # 7. Energy-Gated Fusion\n",
    "        self.fusion = EnergyGatedFusion(temperature=1.0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, \n",
    "                output_size: Optional[Tuple[int, int]] = None) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        if output_size is None:\n",
    "            output_size = (H, W)\n",
    "        \n",
    "        # 1. Extract energy map (detached, no gradients for physics module)\n",
    "        with torch.no_grad():\n",
    "            # Convert to grayscale if needed\n",
    "            x_gray = x.mean(dim=1, keepdim=True) if C > 1 else x\n",
    "            energy, mono_out = self.energy_extractor(x_gray)\n",
    "        \n",
    "        # 2. Patch embedding\n",
    "        features = self.patch_embed(x)\n",
    "        \n",
    "        # 3. Encoder (Mamba stages)\n",
    "        encoder_features = []\n",
    "        for i, stage in enumerate(self.encoder_stages):\n",
    "            features = stage(features)\n",
    "            encoder_features.append(features)\n",
    "            if i < len(self.downsample_layers):\n",
    "                features = self.downsample_layers[i](features)\n",
    "        \n",
    "        # 4. Bottleneck\n",
    "        features = self.bottleneck(features)\n",
    "        \n",
    "        # 5. Coarse branch\n",
    "        coarse = self.coarse_branch(features)\n",
    "        coarse = F.interpolate(coarse, size=output_size, \n",
    "                               mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # 6. Fine branch (implicit decoder)\n",
    "        fine = self.fine_branch(features, output_size=output_size)\n",
    "        \n",
    "        # 7. Energy-gated fusion\n",
    "        output = self.fusion(coarse, fine, energy)\n",
    "        \n",
    "        return {\n",
    "            'output': output,\n",
    "            'coarse': coarse,\n",
    "            'fine': fine,\n",
    "            'energy': energy\n",
    "        }\n",
    "    \n",
    "    def inference(self, x: torch.Tensor, \n",
    "                  output_size: Optional[Tuple[int, int]] = None) -> torch.Tensor:\n",
    "        \n",
    "        return self.forward(x, output_size)['output']\n",
    "    \n",
    "    def query_points(self, x: torch.Tensor, \n",
    "                     coords: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Encode image\n",
    "        features = self.patch_embed(x)\n",
    "        for i, stage in enumerate(self.encoder_stages):\n",
    "            features = stage(features)\n",
    "            if i < len(self.downsample_layers):\n",
    "                features = self.downsample_layers[i](features)\n",
    "        features = self.bottleneck(features)\n",
    "        \n",
    "        # Query fine branch at coordinates\n",
    "        fine_points = self.fine_branch.implicit_head(features, coords=coords)\n",
    "        \n",
    "        return fine_points\n",
    "\n",
    "class EGMNetLite(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 1, num_classes: int = 2,\n",
    "                 img_size: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = EGMNet(\n",
    "            in_channels=in_channels,\n",
    "            num_classes=num_classes,\n",
    "            img_size=img_size,\n",
    "            base_channels=32,  # Reduced from 64\n",
    "            num_stages=3,      # Reduced from 4\n",
    "            encoder_depth=1,   # Reduced from 2\n",
    "            implicit_hidden=128,  # Reduced from 256\n",
    "            implicit_layers=3,    # Reduced from 4\n",
    "            num_frequencies=32    # Reduced from 64\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, output_size=None):\n",
    "        return self.model(x, output_size)\n",
    "    \n",
    "    def inference(self, x, output_size=None):\n",
    "        return self.model.inference(x, output_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Testing EGM-Net (Energy-Gated Gabor Mamba Network)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test full model\n",
    "    print(\"\\n[1] Testing EGM-Net Full...\")\n",
    "    model = EGMNet(\n",
    "        in_channels=1,\n",
    "        num_classes=3,\n",
    "        img_size=256,\n",
    "        base_channels=64,\n",
    "        num_stages=4,\n",
    "        encoder_depth=2\n",
    "    )\n",
    "    \n",
    "    x = torch.randn(2, 1, 256, 256)\n",
    "    outputs = model(x)\n",
    "    \n",
    "    print(f\"Input: {x.shape}\")\n",
    "    print(f\"Output: {outputs['output'].shape}\")\n",
    "    print(f\"Coarse: {outputs['coarse'].shape}\")\n",
    "    print(f\"Fine: {outputs['fine'].shape}\")\n",
    "    print(f\"Energy: {outputs['energy'].shape}\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Test point query (resolution-free inference)\n",
    "    print(\"\\n[2] Testing Point Query (Resolution-Free)...\")\n",
    "    coords = torch.rand(2, 1000, 2) * 2 - 1  # Random points in [-1, 1]\n",
    "    point_output = model.query_points(x, coords)\n",
    "    print(f\"Query coords: {coords.shape}\")\n",
    "    print(f\"Point output: {point_output.shape}\")\n",
    "    \n",
    "    # Test lite model\n",
    "    print(\"\\n[3] Testing EGM-Net Lite...\")\n",
    "    lite_model = EGMNetLite(in_channels=1, num_classes=3, img_size=256)\n",
    "    lite_outputs = lite_model(x)\n",
    "    \n",
    "    lite_params = sum(p.numel() for p in lite_model.parameters())\n",
    "    print(f\"Lite model parameters: {lite_params:,}\")\n",
    "    print(f\"Lite output: {lite_outputs['output'].shape}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ“ All tests passed!\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39339369",
   "metadata": {},
   "source": [
    "### 2.7 Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c6472c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 1.6979\n",
      "  dice: 0.6637\n",
      "  focal: 0.9030\n",
      "  freq: 1.3116\n",
      "  total: 1.6979\n",
      "\n",
      "Boundary Loss: 2.7665\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, smooth: float = 1e-5, reduction: str = \"mean\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        \n",
    "        # Ensure target has same shape as pred for multi-class\n",
    "        if target.ndim == 3:  # (B, H, W) -> convert to one-hot\n",
    "            target = F.one_hot(target.long(), num_classes=pred.shape[1])\n",
    "            target = target.permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        # Flatten spatial dimensions\n",
    "        pred = pred.view(pred.shape[0], pred.shape[1], -1)\n",
    "        target = target.view(target.shape[0], target.shape[1], -1)\n",
    "        \n",
    "        # Compute Dice score\n",
    "        intersection = torch.sum(pred * target, dim=2)\n",
    "        union = torch.sum(pred, dim=2) + torch.sum(target, dim=2)\n",
    "        \n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        # Return loss (1 - Dice)\n",
    "        loss = 1.0 - dice\n",
    "        \n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha: float = 0.25, gamma: float = 2.0,\n",
    "                 reduction: str = \"mean\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Get class probabilities\n",
    "        p = torch.softmax(pred, dim=1)\n",
    "        \n",
    "        # Get class log probabilities\n",
    "        ce = F.cross_entropy(pred, target.long(), reduction='none')\n",
    "        \n",
    "        # Get probability of true class\n",
    "        p_t = torch.gather(p, 1, target.long().unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute focal loss\n",
    "        focal_weight = (1.0 - p_t) ** self.gamma\n",
    "        focal_loss = focal_weight * ce\n",
    "        \n",
    "        if self.reduction == \"mean\":\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class FrequencyLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, weight: float = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Ensure both have batch and channel dimensions\n",
    "        if pred.ndim == 3:\n",
    "            pred = pred.unsqueeze(1)\n",
    "        if target.ndim == 3:\n",
    "            target = target.unsqueeze(1)\n",
    "        \n",
    "        # Flatten to single channel for FFT comparison\n",
    "        if pred.shape[1] > 1:\n",
    "            # For multi-channel, convert to grayscale by averaging\n",
    "            pred = pred.mean(dim=1, keepdim=True)\n",
    "        if target.shape[1] > 1:\n",
    "            target = target.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        # Apply FFT to convert to frequency domain\n",
    "        pred_freq = torch.fft.rfft2(pred, dim=(-2, -1), norm=\"ortho\")\n",
    "        target_freq = torch.fft.rfft2(target, dim=(-2, -1), norm=\"ortho\")\n",
    "        \n",
    "        # Compute L2 distance in frequency domain\n",
    "        # Consider both magnitude and phase information\n",
    "        loss_real = F.mse_loss(pred_freq.real, target_freq.real, reduction='mean')\n",
    "        loss_imag = F.mse_loss(pred_freq.imag, target_freq.imag, reduction='mean')\n",
    "        \n",
    "        return loss_real + loss_imag\n",
    "\n",
    "class SpectralDualLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, spatial_weight: float = 1.0, freq_weight: float = 0.1,\n",
    "                 use_dice: bool = True, use_focal: bool = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.spatial_weight = spatial_weight\n",
    "        self.freq_weight = freq_weight\n",
    "        self.use_dice = use_dice\n",
    "        self.use_focal = use_focal\n",
    "        \n",
    "        # Spatial losses\n",
    "        if use_dice:\n",
    "            self.dice_loss = DiceLoss(smooth=1e-5)\n",
    "        \n",
    "        if use_focal:\n",
    "            self.focal_loss = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "        else:\n",
    "            self.ce_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Frequency loss\n",
    "        self.freq_loss = FrequencyLoss(weight=freq_weight)\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor,\n",
    "                return_components: bool = False) -> torch.Tensor:\n",
    "        \n",
    "        # Ensure target is on same device as pred\n",
    "        target = target.to(pred.device)\n",
    "        \n",
    "        # Spatial losses\n",
    "        spatial_loss = 0.0\n",
    "        losses_dict = {}\n",
    "        \n",
    "        if self.use_dice:\n",
    "            dice = self.dice_loss(pred, target)\n",
    "            spatial_loss = spatial_loss + dice\n",
    "            losses_dict['dice'] = dice.item()\n",
    "        \n",
    "        if self.use_focal:\n",
    "            focal = self.focal_loss(pred, target)\n",
    "            spatial_loss = spatial_loss + focal\n",
    "            losses_dict['focal'] = focal.item()\n",
    "        else:\n",
    "            ce = self.ce_loss(pred, target)\n",
    "            spatial_loss = spatial_loss + ce\n",
    "            losses_dict['ce'] = ce.item()\n",
    "        \n",
    "        # Frequency loss\n",
    "        # For frequency loss, we need to extract the predicted class (argmax) and compare\n",
    "        pred_probs = torch.softmax(pred, dim=1)\n",
    "        pred_class = torch.argmax(pred_probs, dim=1)  # (B, H, W)\n",
    "        \n",
    "        freq = self.freq_loss(pred_class.float(), target.float())\n",
    "        losses_dict['freq'] = freq.item()\n",
    "        \n",
    "        # Weighted combination\n",
    "        total_loss = (self.spatial_weight * spatial_loss + \n",
    "                     self.freq_weight * freq)\n",
    "        losses_dict['total'] = total_loss.item()\n",
    "        \n",
    "        if return_components:\n",
    "            return total_loss, losses_dict\n",
    "        else:\n",
    "            return total_loss\n",
    "\n",
    "class BoundaryAwareLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, kernel_size: int = 3, weight: float = 1.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.weight = weight\n",
    "    \n",
    "    def _compute_boundaries(self, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Convert to float\n",
    "        mask = mask.float().unsqueeze(1)  # (B, 1, H, W)\n",
    "        \n",
    "        # Compute gradients using Sobel-like operation\n",
    "        kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],\n",
    "                                dtype=mask.dtype, device=mask.device)\n",
    "        kernel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]],\n",
    "                                dtype=mask.dtype, device=mask.device)\n",
    "        \n",
    "        kernel_x = kernel_x.view(1, 1, 3, 3)\n",
    "        kernel_y = kernel_y.view(1, 1, 3, 3)\n",
    "        \n",
    "        grad_x = F.conv2d(mask, kernel_x, padding=1)\n",
    "        grad_y = F.conv2d(mask, kernel_y, padding=1)\n",
    "        \n",
    "        # Compute magnitude of gradient\n",
    "        grad_magnitude = torch.sqrt(grad_x ** 2 + grad_y ** 2 + 1e-8)\n",
    "        \n",
    "        # Threshold to get boundary pixels\n",
    "        boundary = (grad_magnitude > 0).float().squeeze(1)\n",
    "        \n",
    "        return boundary\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Get predicted class\n",
    "        pred_probs = torch.softmax(pred, dim=1)\n",
    "        pred_class = torch.argmax(pred_probs, dim=1)  # (B, H, W)\n",
    "        \n",
    "        # Compute boundary maps\n",
    "        pred_boundary = self._compute_boundaries(pred_class)\n",
    "        target_boundary = self._compute_boundaries(target)\n",
    "        \n",
    "        # Compute cross-entropy loss weighted by boundary\n",
    "        ce_loss = F.cross_entropy(pred, target.long(), reduction='none')\n",
    "        \n",
    "        # Apply boundary weight (higher loss for boundary pixels)\n",
    "        boundary_weight = (pred_boundary + target_boundary).clamp(0, 1)\n",
    "        boundary_weight = 1.0 + boundary_weight  # Weight between 1 and 2\n",
    "        \n",
    "        weighted_loss = ce_loss * boundary_weight\n",
    "        \n",
    "        return weighted_loss.mean()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test losses\n",
    "    batch_size, num_classes, height, width = 2, 3, 64, 64\n",
    "    \n",
    "    # Create dummy predictions and targets\n",
    "    pred = torch.randn(batch_size, num_classes, height, width)\n",
    "    target = torch.randint(0, num_classes, (batch_size, height, width))\n",
    "    \n",
    "    # Test SpectralDualLoss\n",
    "    loss_fn = SpectralDualLoss(spatial_weight=1.0, freq_weight=0.1)\n",
    "    loss, components = loss_fn(pred, target, return_components=True)\n",
    "    \n",
    "    print(f\"Total Loss: {loss.item():.4f}\")\n",
    "    for name, value in components.items():\n",
    "        print(f\"  {name}: {value:.4f}\")\n",
    "    \n",
    "    # Test BoundaryAwareLoss\n",
    "    boundary_loss_fn = BoundaryAwareLoss()\n",
    "    boundary_loss = boundary_loss_fn(pred, target)\n",
    "    print(f\"\\nBoundary Loss: {boundary_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a78b3b",
   "metadata": {},
   "source": [
    "### 2.8 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad17bdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from monai.metrics import compute_hausdorff_distance\n",
    "\n",
    "def count_parameters(model):\n",
    "    \n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class SegmentationMetrics:\n",
    "    def __init__(self, num_classes, device):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.batches = 0\n",
    "        self.total_correct_pixels = 0\n",
    "        self.total_pixels = 0\n",
    "        \n",
    "        # Aggregated stats for Precision/Recall/F1 (Global)\n",
    "        self.tp = torch.zeros(self.num_classes, device=self.device)\n",
    "        self.fp = torch.zeros(self.num_classes, device=self.device)\n",
    "        self.fn = torch.zeros(self.num_classes, device=self.device)\n",
    "        \n",
    "        # Accumulators for averaging Batch-wise metrics\n",
    "        self.dice_sum = torch.zeros(self.num_classes, device=self.device)\n",
    "        self.iou_sum = torch.zeros(self.num_classes, device=self.device)\n",
    "        self.hd95_sum = torch.zeros(self.num_classes, device=self.device)\n",
    "        \n",
    "        # Track valid batches for HD95 (it can be NaN if class is missing)\n",
    "        self.hd95_counts = torch.zeros(self.num_classes, device=self.device)\n",
    "\n",
    "    def update(self, preds, targets):\n",
    "        \n",
    "        self.batches += 1\n",
    "        \n",
    "        # Accuracy\n",
    "        self.total_correct_pixels += (preds == targets).sum().item()\n",
    "        self.total_pixels += targets.numel()\n",
    "        \n",
    "        # Create one-hot for HD95 and Dice\n",
    "        # preds_oh: (B, C, H, W)\n",
    "        preds_oh = F.one_hot(preds, num_classes=self.num_classes).permute(0, 3, 1, 2).float()\n",
    "        targets_oh = F.one_hot(targets, num_classes=self.num_classes).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        # Helper for Dice/IoU/TP/FP/FN\n",
    "        for c in range(self.num_classes):\n",
    "            p_flat = preds_oh[:, c].reshape(-1)\n",
    "            t_flat = targets_oh[:, c].reshape(-1)\n",
    "            \n",
    "            intersection = (p_flat * t_flat).sum()\n",
    "            union = p_flat.sum() + t_flat.sum()\n",
    "            \n",
    "            # Global TP/FP/FN accumulation\n",
    "            self.tp[c] += intersection\n",
    "            self.fp[c] += (p_flat.sum() - intersection)\n",
    "            self.fn[c] += (t_flat.sum() - intersection)\n",
    "            \n",
    "            # Batch-wise Dice/IoU accumulation\n",
    "            dice = (2. * intersection + 1e-6) / (union + 1e-6)\n",
    "            iou = (intersection + 1e-6) / (union - intersection + 1e-6)\n",
    "            \n",
    "            self.dice_sum[c] += dice\n",
    "            self.iou_sum[c] += iou\n",
    "            \n",
    "        # HD95 Compliance (MONAI)\n",
    "        # compute_hausdorff_distance expects (B, C, spatial...)\n",
    "        # include_background=True usually, but we iterate.\n",
    "        # We can compute all classes at once.\n",
    "        try:\n",
    "            import warnings\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                \n",
    "                # percentile=95\n",
    "                hd95_batch = compute_hausdorff_distance(\n",
    "                    y_pred=preds_oh, \n",
    "                    y=targets_oh, \n",
    "                    include_background=True, \n",
    "                    percentile=95.0,\n",
    "                    spacing=None  # Pixel space\n",
    "                )\n",
    "            # hd95_batch is (B, C)\n",
    "            \n",
    "            for c in range(self.num_classes):\n",
    "                # Filter NaNs/Infs (happens if class missing in both pred and target, or just one)\n",
    "                # MONAI returns NaN if one is empty. We mostly care if target exists.\n",
    "                # Common practice: if target is empty, skip. If target exists but pred empty, HD is high (inf).\n",
    "                # MONAI behavior: Nan if both empty. Inf if one empty.\n",
    "                \n",
    "                valid_vals = hd95_batch[:, c]\n",
    "                valid_mask = ~torch.isnan(valid_vals) & ~torch.isinf(valid_vals)\n",
    "                \n",
    "                if valid_mask.any():\n",
    "                    self.hd95_sum[c] += valid_vals[valid_mask].sum()\n",
    "                    self.hd95_counts[c] += valid_mask.sum()\n",
    "                    \n",
    "        except Exception as e:\n",
    "            # Fallback or strict error? \n",
    "            # Often happens if shapes are weird or empty batch.\n",
    "            pass\n",
    "\n",
    "    def compute(self):\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Global Accuracy\n",
    "        metrics['accuracy'] = self.total_correct_pixels / max(self.total_pixels, 1)\n",
    "        \n",
    "        # Per-class metrics\n",
    "        dice_scores = []\n",
    "        iou_scores = []\n",
    "        hd95_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for c in range(self.num_classes):\n",
    "            # Batch-averaged Dice/IoU\n",
    "            dice_scores.append((self.dice_sum[c] / max(self.batches, 1)).item())\n",
    "            iou_scores.append((self.iou_sum[c] / max(self.batches, 1)).item())\n",
    "            \n",
    "            # Batch-averaged HD95\n",
    "            if self.hd95_counts[c] > 0:\n",
    "                hd95_scores.append((self.hd95_sum[c] / self.hd95_counts[c]).item())\n",
    "            else:\n",
    "                hd95_scores.append(float('nan')) # Or 0.0 or inf\n",
    "            \n",
    "            # Global-based Precision/Recall/F1\n",
    "            p = (self.tp[c] / (self.tp[c] + self.fp[c] + 1e-6)).item()\n",
    "            r = (self.tp[c] / (self.tp[c] + self.fn[c] + 1e-6)).item()\n",
    "            f1 = 2 * p * r / (p + r + 1e-6) if (p + r) > 0 else 0.0\n",
    "            \n",
    "            precision_scores.append(p)\n",
    "            recall_scores.append(r)\n",
    "            f1_scores.append(f1)\n",
    "            \n",
    "        metrics['dice_scores'] = dice_scores\n",
    "        metrics['iou'] = iou_scores\n",
    "        metrics['hd95'] = hd95_scores\n",
    "        metrics['precision'] = precision_scores\n",
    "        metrics['recall'] = recall_scores\n",
    "        metrics['f1_score'] = f1_scores\n",
    "        \n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af3e4086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All modules loaded!\n"
     ]
    }
   ],
   "source": [
    "print('âœ… All modules loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0373d79",
   "metadata": {},
   "source": [
    "---\n",
    "## 3ï¸âƒ£ Dataset\n",
    "\n",
    "Download and preprocess ACDC cardiac MRI dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74be26ee",
   "metadata": {},
   "source": [
    "### 3.1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f804fc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Dataset: ACDC\n",
      "   Classes: ['Background', 'RV', 'Myocardium', 'LV']\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'ACDC'\n",
    "DRIVE_FOLDER_ID = '1EelzBVjIoDQ4uzt0_2JzmF_PuUHsD93e'\n",
    "RAW_DATA_DIR = f'./data/{DATASET}'\n",
    "PREPROCESSED_DIR = f'./preprocessed_data/{DATASET}'\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 4\n",
    "CLASS_NAMES = ['Background', 'RV', 'Myocardium', 'LV']\n",
    "\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "print(f\"ðŸ“Š Dataset: {DATASET}\")\n",
    "print(f\"   Classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9206647",
   "metadata": {},
   "source": [
    "### 3.2 Download from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16142b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n",
      "Retrieving folder 1RQkxK1y5C8ydaJoOCiH-WHi51y7aMvz_ testing\n",
      "Retrieving folder 1TfoJPPOYJAy8lDZ3wN6h55mhKA5Mb1fD patient101\n",
      "Processing file 1qXKKh4DNxwiJA5zgqQDkWxIGloALUhA6 Info.cfg\n",
      "Processing file 1TCvYlYh2P4HCPJUsOebas1llY_gd0rQU MANDATORY_CITATION.md\n",
      "Processing file 1ihjf_DqLe4ywtefurEZE9rFq9_WyKaK8 patient101_4d.nii\n",
      "Processing file 1zLfzi4BU7E2b38P1owG9Aczey5uaFSHY patient101_frame01_gt.nii\n",
      "Processing file 1G8GNBb-6h9pkHLsJrzIsE0sWFzFEubSr patient101_frame01.nii\n",
      "Processing file 1IFCp8jhgswbqJ5kn0OmKiOuXr81kk9ks patient101_frame14_gt.nii\n",
      "Processing file 1mzq3gThA3Ft4J86BAD4MIe5-y_mLiF-T patient101_frame14.nii\n",
      "Retrieving folder 1w9GenTpksbWvvVApm382FoJMrTI-7cL1 patient102\n",
      "Processing file 12YOhsYZ7EIcPhVo8grzXMDvvYwNOPaN- Info.cfg\n",
      "Processing file 1QgJak3ZukdD37ddJGEWjce0Rw7ZGSGuB MANDATORY_CITATION.md\n",
      "Processing file 1kGI4cLS2Sr3k0Br8VlEfar1YxDOD3wpt patient102_4d.nii\n",
      "Processing file 1G72AKLtBVsr_U_Xy1hKsekasfjPDzN6J patient102_frame01_gt.nii\n",
      "Processing file 1tq2fOT0QllW62-Y5XlvPq2fuMtRGyiTr patient102_frame01.nii\n",
      "Processing file 1AcVh9eqHdSYHPsd5c5dO3_Sq958avIXb patient102_frame13_gt.nii\n",
      "Processing file 1tOpf9qxflqzKlj1VsoIyCEtRLcgbJvos patient102_frame13.nii\n",
      "Retrieving folder 18a4pkwWsKd0QvjYQXryFjiq8tc0VAu5W patient103\n",
      "Processing file 1w6zb0qlYduiKnlHjryuE_CVnAzZCEU9a Info.cfg\n",
      "Processing file 1r65W4mw7OL04SBZBeM19B7CnyDHrPAc7 MANDATORY_CITATION.md\n",
      "Processing file 1CJo2zcC9cnG0R9HXjVDTt066TcuKkRjx patient103_4d.nii\n",
      "Processing file 1ba9q0hZRVnXDzfbMg96P1semPTlJSl2g patient103_frame01_gt.nii\n",
      "Processing file 19NBx1ikSWBCk8Zf8yiPe5y2UHHIh-5XD patient103_frame01.nii\n",
      "Processing file 1HmvI_BZsZrDckRUl3iqMz4hAGfQ5jQbt patient103_frame11_gt.nii\n",
      "Processing file 1fnD-bY6oaJsw11Cm2Ef5HJiuYl8E4Vki patient103_frame11.nii\n",
      "Retrieving folder 1DAho64r5tfhXqIOOLyqY51bpyPxDP-LQ patient104\n",
      "Processing file 1Xg8mNp2Filx6xaOTok5z8nx02FYWhrZ8 Info.cfg\n",
      "Processing file 1niliOZ_VIgDbAtJ2k5UVuryYsMB-euui MANDATORY_CITATION.md\n",
      "Processing file 1krx_yH28mPDJ-_CYEhf6-_tplsPXYd8z patient104_4d.nii\n",
      "Processing file 1_Lr0s0RE1wZnnhRpkeyizLI3-_R6oMOS patient104_frame01_gt.nii\n",
      "Processing file 1MxUn5DRTz9Tvbz6vz48QqRrJk1buN_Z2 patient104_frame01.nii\n",
      "Processing file 1yHXxdFZ71hgA4WNa_74ILRVJEYegTXHe patient104_frame11_gt.nii\n",
      "Processing file 1epGnZEuRoZz6GE2v4DpW06O-oPK7GsCY patient104_frame11.nii\n",
      "Retrieving folder 12C_okp0VgXV6EwjCjZyMN53yoLJgZqsR patient105\n",
      "Processing file 1xIR4XJVdQkVmGP_ecq85QUF8VJqZyKxS Info.cfg\n",
      "Processing file 1lZMZugDD7hk2LD5DHNO_auLmftzggizi MANDATORY_CITATION.md\n",
      "Processing file 1h1FPMss1Vxuy71xadBvGU-Fa5qGt4eV3 patient105_4d.nii\n",
      "Processing file 1-SpcfC4diGxnDXfxZQfinOoSQsgLHQNB patient105_frame01_gt.nii\n",
      "Processing file 1sQc_A3rMRsGhNDGmPfBTGH-cb5HzXFJY patient105_frame01.nii\n",
      "Processing file 18gAFZjSpXor8s7OSKJZjHjSGRsbCfNAO patient105_frame10_gt.nii\n",
      "Processing file 1ryJR9hdxiREq60vFg4-s_jDUlqmiaeUH patient105_frame10.nii\n",
      "Retrieving folder 1mSuefz9qJWIF9OYWr2MF-wlZyAmo8v3K patient106\n",
      "Processing file 1CnVugMN54w-2X-fN0MMLophnDnRFAVMc Info.cfg\n",
      "Processing file 1QD9ooO-8l-CcLGCiLvwaMzK8YzRIqyET MANDATORY_CITATION.md\n",
      "Processing file 1OUICoJiBc003nAO_QWM4xSGfSLrhlDY0 patient106_4d.nii\n",
      "Processing file 1hMTBtL00RCxxuhKGlKnwhoFHUBNgiGqe patient106_frame01_gt.nii\n",
      "Processing file 1EVN8RV-5DW8ZDUWPxFAhI67nkX9nKj67 patient106_frame01.nii\n",
      "Processing file 1tzWWfr4bPrk-cIIa5t5LL6H_YRqAcLfR patient106_frame13_gt.nii\n",
      "Processing file 12CJB7aq1TqO2VN-KYR0GhuAYE5FQ-IiS patient106_frame13.nii\n",
      "Retrieving folder 1y7konhDeL16ciPXAqQfUNDXt9aZ1nxx8 patient107\n",
      "Processing file 1sEDfO3Bf5Q5lRs8JCaUqzgnidM7xOQ4a Info.cfg\n",
      "Processing file 1fGHin9Mwp3QkC0GOc4GW-iLguH_dXD2K MANDATORY_CITATION.md\n",
      "Processing file 1ZEcU593NXhgRImJAF2sWljOO9-lsszoL patient107_4d.nii\n",
      "Processing file 1VNvQhH8CuBqAUMox7PXSGNnpd7ErbvB_ patient107_frame01_gt.nii\n",
      "Processing file 1IxaiyVwwaFY0fF-ht0auLfql7DCrgYtm patient107_frame01.nii\n",
      "Processing file 16kd8ipm6CNStjCl0D7m4OXKZAsi2lxSF patient107_frame10_gt.nii\n",
      "Processing file 1TL_hLhiy6yt6tpdLlT1FDu9sIPIYhutn patient107_frame10.nii\n",
      "Retrieving folder 1mzgaZ7JElRnWOeyFi9DBgjsXvhmQxNj7 patient108\n",
      "Processing file 1I5xcA8MHuS4WfXZI857m-gjEgOJ4vuvL Info.cfg\n",
      "Processing file 1KSHJQT_ky6v2_anz8GRPBWEAq5OaL2jN MANDATORY_CITATION.md\n",
      "Processing file 1RDa3dueMtEJRuGe7hlZz2UVGA8DrOPXZ patient108_4d.nii\n",
      "Processing file 16HfB99vsCeKsw5et7GC1w-9k5orB30sY patient108_frame01_gt.nii\n",
      "Processing file 16UzN3qQltBCItOrJtsHY7Xn1UWxHAwwU patient108_frame01.nii\n",
      "Processing file 1zUso9_ieW68ww0H_lBlHau1Wxmoo0_9p patient108_frame09_gt.nii\n",
      "Processing file 1Wg8Q2fJSb9uOVWFY1PKusmVdM2PJf2do patient108_frame09.nii\n",
      "Retrieving folder 1PPgD00aZnUHBia-ek-c_BdJ9neGli6pJ patient109\n",
      "Processing file 1B6fZA0hpI9BtUXtdJFNabUYdkqj8QRtS Info.cfg\n",
      "Processing file 1h8Uc9phkfbcoTbjQlv9H1v1V_fRJ4LpX MANDATORY_CITATION.md\n",
      "Processing file 1wMImPdaPjnui2VCYbC_ePWNJSDXEBgWw patient109_4d.nii\n",
      "Processing file 14VQUjboBjH_Uim-rueL4A-br4wbS2Zxg patient109_frame01_gt.nii\n",
      "Processing file 1FIKmpeoabunMZPkvYPWlxX8E5cpBE3w6 patient109_frame01.nii\n",
      "Processing file 1zS9U-aikphyH-8zdStVqOnUUILWuSzTP patient109_frame10_gt.nii\n",
      "Processing file 1uPbzOqG-3U9dne_C6lQ3-C51pOw6Yunq patient109_frame10.nii\n",
      "Retrieving folder 1ZeR7wxqCmiVk2eOxgVhx9MecB5D-7GdB patient110\n",
      "Processing file 1vl3T0cVIsRRO6CCY0hmOXUIMqjFxdQyS Info.cfg\n",
      "Processing file 1mR8VFSDiFygapGoS5r6mmDa9XQEKmKRe MANDATORY_CITATION.md\n",
      "Processing file 1E8wF7kbxRn7Q30SyufF2NfcBt8Bmmeib patient110_4d.nii\n",
      "Processing file 1YQi1mau23yeUkDQmZqglf_ct3XgY4IWB patient110_frame01_gt.nii\n",
      "Processing file 1bJn5BSpY-3j3vtkjl8fUxoFACsPiv0XD patient110_frame01.nii\n",
      "Processing file 1lkiSS7mCGDgm3pSjym46_-_-lT4KuPJX patient110_frame11_gt.nii\n",
      "Processing file 1EKwuyxAcelyUgOpMKEVbKprzAJBneCYR patient110_frame11.nii\n",
      "Retrieving folder 1nDkbn-jxQkoMR0EgRhwx6cdmeBJgp60a patient111\n",
      "Processing file 1aIFA2P4GUOwpB6O-4-FDKaJpPmsW-tIy Info.cfg\n",
      "Processing file 1-COIdSMqjRAEsilcPrrqPdzQCPnZapvM MANDATORY_CITATION.md\n",
      "Processing file 1NVhtKWcAgDMzmVnexxeZxJ-qJ3gghMYI patient111_4d.nii\n",
      "Processing file 1iZOufGPATbxxcyhln_IPSqkBg3VPgto5 patient111_frame01_gt.nii\n",
      "Processing file 1KFXASxOyVQrpNujcX4M3vSr-KIuOnJWJ patient111_frame01.nii\n",
      "Processing file 1XsEe2BQVOAFcZ6Xbz5cumyQ2-VQiTzac patient111_frame07_gt.nii\n",
      "Processing file 1CQaQl9540y22k35MO9XfRohz7dX48Io1 patient111_frame07.nii\n",
      "Retrieving folder 12DvSW78fgRvnNqIBoBQQGzlOm54Sm4Oe patient112\n",
      "Processing file 18fewIs6xBjnuLZj6QseTqqxMoyr59gfa Info.cfg\n",
      "Processing file 1G7Sybg9PhyGtFBSJuYqjned2JgO-UDct MANDATORY_CITATION.md\n",
      "Processing file 13Bf9aa3xnI7-GwspCo99OQXbsMg-y2UL patient112_4d.nii\n",
      "Processing file 1tCC3oLq2xu9j9wr0XvL0prs2slJiSLR6 patient112_frame01_gt.nii\n",
      "Processing file 1JF_nT0M2-bueyp7ZLbVKgsAosrYvQNiW patient112_frame01.nii\n",
      "Processing file 1Zh5-B8do-v4ayJ_FAmdNNzitazjhOWzh patient112_frame12_gt.nii\n",
      "Processing file 1YlzRVPCWOt_0AFJkBA7-uZWuGfwMql4L patient112_frame12.nii\n",
      "Retrieving folder 1YoIc9XWZffKynViLFWD8qxTF8of57xyc patient113\n",
      "Processing file 1a9iT9lDlpISWjvHDVaOXhxFiBtNs3vY9 Info.cfg\n",
      "Processing file 1IWKIc21EBkr2gEdrRiniXPg1Nl24sTgA MANDATORY_CITATION.md\n",
      "Processing file 1zlOxe_fDlBA5ldI1kkx27nl7BPHMHxfM patient113_4d.nii\n",
      "Processing file 148hU9oEi-mKGviamwtgB_F7H2zP7F06k patient113_frame01_gt.nii\n",
      "Processing file 1CviE2E1L5TqMJNl7rNaDcvjDOQznuHMj patient113_frame01.nii\n",
      "Processing file 1RzmlWU3htHGS95TILl8oK-L-yZexRhcJ patient113_frame08_gt.nii\n",
      "Processing file 17ssD7n2gmTvXa_u_zfgKJhF44YnnlyoV patient113_frame08.nii\n",
      "Retrieving folder 1wgO-C3xR-UwbxycdxUU8PY43fnuLvRN0 patient114\n",
      "Processing file 1YVJ9VhD5GZDrXtQtaGJSrcT_cTtZKm7o Info.cfg\n",
      "Processing file 1Tcd7yOEuz9Nw4HermIo5bW4uP7hib1IK MANDATORY_CITATION.md\n",
      "Processing file 10iKAO0vJ_5It0B4kE49hwDVc230e22uv patient114_4d.nii\n",
      "Processing file 1pDlGtjJkJb5wZ_f3mx5F82L9EmjUfuIR patient114_frame01_gt.nii\n",
      "Processing file 1z9T0MlTmd_vlYiVo49S25BvNhS-kqLTp patient114_frame01.nii\n",
      "Processing file 1lBQGrQCVFX-UOBCh8obW2-1o7V9UsS8f patient114_frame11_gt.nii\n",
      "Processing file 1nrIJUtos5HE4rY5PhJ0qpLpf4Ugw3pYE patient114_frame11.nii\n",
      "Retrieving folder 1xC6ROiSQNGS40NToL45kd9NmjLMGiQaR patient115\n",
      "Processing file 1Q-_zdfR10DJyu7-3Lp7CmcgzkHRuBpZz Info.cfg\n",
      "Processing file 1CT9PY3bgjBfyvxmeZDvQa6m3NmLOXV_E MANDATORY_CITATION.md\n",
      "Processing file 1gQcOIML-5GttoFUmUywhMHaQidRPFioy patient115_4d.nii\n",
      "Processing file 1moRTUPn-RUaKQmNYKONE8tjNbM0tr3OT patient115_frame01_gt.nii\n",
      "Processing file 1UThYYmwtk0t5gBRHNDhh-qc4LV8ieLPb patient115_frame01.nii\n",
      "Processing file 1TKc0IEaTR9tazHIPXnS9rf07BhRKATiR patient115_frame13_gt.nii\n",
      "Processing file 1PKs9k8aWLUpYhy66T6kF4V6ZkBBDLQF5 patient115_frame13.nii\n",
      "Retrieving folder 1TcbhrpSM9aMHqLgllQCp0fwo4E9nRSHK patient116\n",
      "Processing file 1lFfUj_vLbc7-5B7IyWpXGJ_E9CPms_Ml Info.cfg\n",
      "Processing file 1oXmbqrexubWbvIj3kYU4zYLMutXKVYin MANDATORY_CITATION.md\n",
      "Processing file 198RQTOeegN43jt5Gi0LHuSKW5WtkZxjc patient116_4d.nii\n",
      "Processing file 1lq99pBApU5DbD0t7Xky-rsPtKGaovGrr patient116_frame01_gt.nii\n",
      "Processing file 1w5bFYfFeDH_y88T5EEdTNrcw1WpDklZ8 patient116_frame01.nii\n",
      "Processing file 1ZVg2LmRdZfG42n4uU_XiDTPhG4g8rRZm patient116_frame09_gt.nii\n",
      "Processing file 1AfzAmWj0ZL7xPIXDdNvR4Fw41OYw6o7R patient116_frame09.nii\n",
      "Retrieving folder 1drVVqqVDezjJempplA9MxuZ401cUMyvJ patient117\n",
      "Processing file 1MnvsY-82dsWMRCCPvu2NBbT91SIBGXfO Info.cfg\n",
      "Processing file 1cdFJ1eVx_xT15jlBvgsRVQYp_iDFK64_ MANDATORY_CITATION.md\n",
      "Processing file 1Lnj1wPAgTB4_hSoDHzmU4_1PZT6bTXkR patient117_4d.nii\n",
      "Processing file 1iiwQRIdNP-X7CAVB4VynT5UtIT4AyRZ1 patient117_frame01_gt.nii\n",
      "Processing file 1w4bh7fodPNirJyluueXoZJDBUOuhEGdv patient117_frame01.nii\n",
      "Processing file 1jrOzyFVCpagtVW6iq8x0RXKyjoR4GTKK patient117_frame13_gt.nii\n",
      "Processing file 1EOfVw44cfCsmfPFXNtMXPKVUwihQf_b2 patient117_frame13.nii\n",
      "Retrieving folder 1puQy8f7KAbRG-T8zK4sEU6rsFlpM_6ML patient118\n",
      "Processing file 1kRLC75buWSuzNV716cIbc9xb04r3zocw Info.cfg\n",
      "Processing file 1cBkss3UY0l9ve5qXveh18fVL_QnFKcNy MANDATORY_CITATION.md\n",
      "Processing file 1GNQXj4bQmp6TDrx0-IW_6vBfVzQdNZxd patient118_4d.nii\n",
      "Processing file 1cUW51XSvjb1ADXT5JDtA1gVxNMa7p3Yi patient118_frame01_gt.nii\n",
      "Processing file 1j3dUfB5yGgCJYN2gnPt0zL1MA8fCr-_I patient118_frame01.nii\n",
      "Processing file 13pZprg_18gP3mjI-5Oh_MW4wM8vcNLPW patient118_frame10_gt.nii\n",
      "Processing file 1N7cFTGt4wHKwAaO4G1se_gBtEitg1Dzp patient118_frame10.nii\n",
      "Retrieving folder 1efkpKEL4h1KyOHGmjXhD_4n_e3-x6EWG patient119\n",
      "Processing file 1v8SQDHraCtLd2wQRuqiwFcjHhwgwNnji Info.cfg\n",
      "Processing file 1qTFrazIwxeD5VvVIC1BftB26bvSxkMdL MANDATORY_CITATION.md\n",
      "Processing file 13vFqyURggPcffCU5RkMSEspZO4dL4NNF patient119_4d.nii\n",
      "Processing file 15ZeVe9JfVg3lnXOR0660MguCCWzovFdD patient119_frame01_gt.nii\n",
      "Processing file 1mQQE9W5t5Rn_nAvQaOd6CnpAdZdI7cV5 patient119_frame01.nii\n",
      "Processing file 1aQAKKgbPqDH4qBLXRNpgk9uMsph0wLpK patient119_frame09_gt.nii\n",
      "Processing file 18iP8k2KVbACSWlFxHqgXr-ZzM5d_RkGt patient119_frame09.nii\n",
      "Retrieving folder 1ktntaNCdiu8JFK7ZcNCWWH_9gg2LsIMu patient120\n",
      "Processing file 17TeEf6oJNyLHp3taoaG2ZECVxu_dx6g2 Info.cfg\n",
      "Processing file 1_66jB-aqyCCvuI1PfqPxCscOTedZwxei MANDATORY_CITATION.md\n",
      "Processing file 1rR-MYLBa4d0Br4kONoAoPxu3P1uz4tos patient120_4d.nii\n",
      "Processing file 1kdKr7N0uy12EYdxNTQSWIbARQ9xNJSO3 patient120_frame01_gt.nii\n",
      "Processing file 1smrrLK21OJ-TKv_vo6TdJk9BKgEAsiMq patient120_frame01.nii\n",
      "Processing file 14uDeTuUB0Q2IWIJ0rTF_9KqNf5tIw_Pe patient120_frame08_gt.nii\n",
      "Processing file 1f4G8GmAPusbmzvSFDd3IqWcFCvuaua2A patient120_frame08.nii\n",
      "Retrieving folder 1AP-GvfhMUCsSo-VUIIi1RrUXA-ZNxb_c patient121\n",
      "Processing file 1mkcge-HncCdal8tuJw0DwXXQMp9LwNmX Info.cfg\n",
      "Processing file 1pDaOMdfWoG-EKVkWUlWxy8QYSPIAPoQu MANDATORY_CITATION.md\n",
      "Processing file 1TDg6D9ejxXvuQxYav2ga-Ka3OIFbAZTm patient121_4d.nii\n",
      "Processing file 1rzKdumZ_VDqqjMg-1jnuUMHCb7dsVHj7 patient121_frame01_gt.nii\n",
      "Processing file 17ZC696SNxh-_B7EdaoKAPRYOxGLSJ-qG patient121_frame01.nii\n",
      "Processing file 1KxrDBxd_hwlUDW1_MurEvdFeLPRpYV2v patient121_frame10_gt.nii\n",
      "Processing file 1zzhmtzK41syKQVo_BrxOnush5qFlE1da patient121_frame10.nii\n",
      "Retrieving folder 1MO6WuJ5sdwjZbyFO_PJvsPsOHn_0JN4I patient122\n",
      "Processing file 16l47614bCbemdu_FxkS9B5YTBdhZy3sM Info.cfg\n",
      "Processing file 1SYelSIwgft3lKZgqzcw8pf6oLeiAcj_f MANDATORY_CITATION.md\n",
      "Processing file 1Zi8td6NJHf0X52_yqk8a8kglSYzGbVjd patient122_4d.nii\n",
      "Processing file 1K7ma2neX92HA8WYI8Gth5TtgUDWuVD-C patient122_frame01_gt.nii\n",
      "Processing file 1Y_r29zY8E8pj_jgp8XL-fCktMOGWlX8B patient122_frame01.nii\n",
      "Processing file 1f0x9UlvIWDgbkQMAiHw4ZrLKonxj4DX1 patient122_frame06_gt.nii\n",
      "Processing file 138IVnH57i_E-bazPcKSVDyw-gjr9VCA8 patient122_frame06.nii\n",
      "Retrieving folder 19yyOAC7nE3jyROoDkh7hx3RMINRdxHzc patient123\n",
      "Processing file 1FLCri2sXlzflEyxvscAgiLKJPovwtUtH Info.cfg\n",
      "Processing file 1Kr4DS1HAGLya-LbpCWyMVPAf0OzZXH-x MANDATORY_CITATION.md\n",
      "Processing file 1DBlV7FgcTl9bwzVLoA6AtFWCjRyidKIz patient123_4d.nii\n",
      "Processing file 1Rmzb9HigpAECiqmr0V00BvdPNUEtfvUs patient123_frame01_gt.nii\n",
      "Processing file 1ZPtt8DlZxdZ3UT9lc6n0xYq4fNAW4ErG patient123_frame01.nii\n",
      "Processing file 1HC77BhSzi3ffkRi0WjHZcqeGPaMmwqDX patient123_frame11_gt.nii\n",
      "Processing file 1KfE29GxbMuACL4u9bqbHpxUiTJlgHU62 patient123_frame11.nii\n",
      "Retrieving folder 1jbZ-z2JEtcygK6txELkJ5Qe1tu59d-MX patient124\n",
      "Processing file 1akBTkIK17tNd0D3tiH5UijOIV2qlulbP Info.cfg\n",
      "Processing file 1EE7x7vVlaaAXxZRBpVaE0WEGaK9cmh2_ MANDATORY_CITATION.md\n",
      "Processing file 1xTGcZt5rur6RH6dd211-gXI7PsRa5yyH patient124_4d.nii\n",
      "Processing file 1SVYrcXMkPSnIhYxahb1eWaNWVHxfQgxv patient124_frame01_gt.nii\n",
      "Processing file 1C8La5drFnqiGCSfPh-amKbOp9geHXsBg patient124_frame01.nii\n",
      "Processing file 1iW5BozD-0AS_apoO8jMJOT_MbBO_PkaM patient124_frame07_gt.nii\n",
      "Processing file 1QTfTYSMPSWWoIAygRci2Jck3DHW1DRwL patient124_frame07.nii\n",
      "Retrieving folder 15RxcQutmUvQZlNCFL9clKrSqjmMrkMUe patient125\n",
      "Processing file 1XZC9Zhpz5OR0sz5sRfd9BKv-RogG3Rp1 Info.cfg\n",
      "Processing file 18ZMMtgpI2GSMzH02dgU_EJ7js1fwWTzn MANDATORY_CITATION.md\n",
      "Processing file 1R3gGqyh12bjlf0_lpXKOcNXvrjUCfsg5 patient125_4d.nii\n",
      "Processing file 10MsafbDuWdpGG1P3VKyerPvxx3XQmFkc patient125_frame01_gt.nii\n",
      "Processing file 19m3lt5K9nvgh_nQ0aSYhd6Nj88ZU0KWh patient125_frame01.nii\n",
      "Processing file 1pkFtkeDqb5s_yGkKXYYWL35biPtV5eHh patient125_frame07_gt.nii\n",
      "Processing file 1FftaMQ29y0e-DLr-76nfznZVw-Rhw_Og patient125_frame07.nii\n",
      "Retrieving folder 1aPoNIlN5xcce-_i2pSU6IQrSaIsjSo9g patient126\n",
      "Processing file 1vTwj-XEzXW5tl-kUxaxBUNba_yjvHFCQ Info.cfg\n",
      "Processing file 1tZnm1d3IF5wTlirJsJx7mIvsk6Npudia MANDATORY_CITATION.md\n",
      "Processing file 1UMjBIbud5_cGkABGVaPrFYEBf8oA-gui patient126_4d.nii\n",
      "Processing file 1ztXiqFOmL6HLvCb4MIilyKIrSv96C3-N patient126_frame01_gt.nii\n",
      "Processing file 18FYpkzqLByn_Jjrc08e2A0Uk5iQxvx2m patient126_frame01.nii\n",
      "Processing file 1CyO8w3NNWhCyIH-S8ZyU6D-efhs0DKPf patient126_frame07_gt.nii\n",
      "Processing file 1lTlavl4zEx4x83BImlI3wzkK1bmMpUVI patient126_frame07.nii\n",
      "Retrieving folder 1_tCcNPsbgEyV78mI92aPHDKC4Tp3UVFp patient127\n",
      "Processing file 1WpiNYmJJGX4cxRCVBvQCx5WW4NDFy2EE Info.cfg\n",
      "Processing file 1YRQJjR7RpDdneVpGzYstuzkDe5iMnkh6 MANDATORY_CITATION.md\n",
      "Processing file 1YaH9d_R4ekBbzDZwS7uESpzyWnY7Bm34 patient127_4d.nii\n",
      "Processing file 1OcnCsMt4O7Cw8j_Om5V1NHs_E8Oxe6Zc patient127_frame01_gt.nii\n",
      "Processing file 13fMF8pM9VsqdsN9OC6ApmOFxk_vpLcy6 patient127_frame01.nii\n",
      "Processing file 15H_kvvDj_zWTwh9pQziijlx3Du5FBB_t patient127_frame07_gt.nii\n",
      "Processing file 1FaPIdUmIw1SlDSj70HVaKuQTnUlWbEOz patient127_frame07.nii\n",
      "Retrieving folder 1nDcbq03HjURI48H2KvRhx07mfMqNb5r- patient128\n",
      "Processing file 1aXLRP8iu3amrl6fUbSgLYIGD3D30s1G0 Info.cfg\n",
      "Processing file 1t16gCcejdGZ8fD8XldfWg0Ui3JvrGTkM MANDATORY_CITATION.md\n",
      "Processing file 1PqR4-58YzBlVc6-wVAT3nIgROdP7dhOi patient128_4d.nii\n",
      "Processing file 14HrIn7Pw7yWVj1ghMlWH8xU__X4KtnUU patient128_frame01_gt.nii\n",
      "Processing file 11-UxmOVqfwkJSQ6QHHG3Qc6IMEu8elOs patient128_frame01.nii\n",
      "Processing file 1EWhUYUubeAUDgS_YHrwlzpOxx2oS4LC2 patient128_frame11_gt.nii\n",
      "Processing file 1VfecTfQADPqMRm0F_Z2GXx3OOATjdJOo patient128_frame11.nii\n",
      "Retrieving folder 1mCnddYAbkwR_dA43ipg7rfVmIinGZ6j0 patient129\n",
      "Processing file 1ZC4BwaDyBq3VwTIKipR4cOkvsZfTgKHK Info.cfg\n",
      "Processing file 1eeWMLIR_gMjtfP4Y6KKUdhq9YExyDrD8 MANDATORY_CITATION.md\n",
      "Processing file 1O1-5sQ4OuYx4X-DJc17wZmGQwHlVrBy0 patient129_4d.nii\n",
      "Processing file 1DQQ6EryMInCAk2hi7Bn8ZUXwo0RpCfKJ patient129_frame01_gt.nii\n",
      "Processing file 11ozKyPc0iZikZNvSGhCTKJeRcPzzW27S patient129_frame01.nii\n",
      "Processing file 1fD72ARCZFKCj5wTYKPqIMhFpnjMoVqek patient129_frame08_gt.nii\n",
      "Processing file 1JpsyD4TNKL_rtQya19vqmWuHcEnhtwoF patient129_frame08.nii\n",
      "Retrieving folder 1ih7qmQ8IXRV_kbt1IlyCPoa-OXx-GA0P patient130\n",
      "Processing file 1iRrmZzyZ2VU076rL8WR5D1X80aKH1dUv Info.cfg\n",
      "Processing file 1xWHaz2yKWL9a8acPyAXKAXB1rX-Zhc_P MANDATORY_CITATION.md\n",
      "Processing file 1g30zVtR1eIeBZiE2id6TzBraVrbH0JpB patient130_4d.nii\n",
      "Processing file 1adifY5wuxBV3M388b4uqG-hz5y9YT3V0 patient130_frame01_gt.nii\n",
      "Processing file 1YnT5MMlvS0X3sTzwVQbF3zpeuhADTA-W patient130_frame01.nii\n",
      "Processing file 1uvV0sKq_vI6CbPt03dUftMgVDwqHV5jQ patient130_frame11_gt.nii\n",
      "Processing file 1PFsww1L-8jSyWawKCU9D7PQgvh7d4UJM patient130_frame11.nii\n",
      "Retrieving folder 1G-3gcR8mvTE2vEr7cNiIXvEQjB-9bq2j patient131\n",
      "Processing file 1a32YRr7HMYHAIAqwgNAwtqHpMYO4Z-7_ Info.cfg\n",
      "Processing file 1lcEO2TVqoTSuUeb-p_FrzzHCfI2TCHcN MANDATORY_CITATION.md\n",
      "Processing file 10ld6B-9uO4c934juuDlhIhE-09IVwfBm patient131_4d.nii\n",
      "Processing file 1BVCwU3YvfkZI2VsEROTRVJ_IJq7akrOt patient131_frame01_gt.nii\n",
      "Processing file 1n4ZiR-jhw_WBVBuvQMEIJFHdFyu0x0rH patient131_frame01.nii\n",
      "Processing file 1SYMrhoFTDGYp7Ac7kEb4o57ipDOvQhS5 patient131_frame09_gt.nii\n",
      "Processing file 1nVKI_qPczZkykHal7lCOkaq0tXcdQfp2 patient131_frame09.nii\n",
      "Retrieving folder 1F5e82S2Wu-IaE52J-IHsK3AX01K0coTb patient132\n",
      "Processing file 1xHp7dC4-jgypzdRswWpWE4E7GgzAistA Info.cfg\n",
      "Processing file 1HDGxNv7CV8CoDJE42RCUf8suBTnn3Rax MANDATORY_CITATION.md\n",
      "Processing file 1DeUd8dzcYtufrrrIy-302e3Fgbhc46gb patient132_4d.nii\n",
      "Processing file 1sb-FNsp28x7Ui_q3Hd5jcPHKQwhRCm34 patient132_frame01_gt.nii\n",
      "Processing file 1chCH-J_fnClQismooeoLBnYR6N1p1Avk patient132_frame01.nii\n",
      "Processing file 1DvxSW2ytqY2ztmGUmdeRSK4wGkxFwZ94 patient132_frame15_gt.nii\n",
      "Processing file 1e69Dmk0eMYYBWLs7y1Tgou_yPLFqdPIl patient132_frame15.nii\n",
      "Retrieving folder 1e8r3kTGY40pq9PsmpLdk8EF0Ll2PV6vd patient133\n",
      "Processing file 1z-ps8s-GmoVOcd_mxtVELFkzmqs7UcqW Info.cfg\n",
      "Processing file 1WpL-R2-w4AAvWKp2X6mpwwc_KQ6Pommj MANDATORY_CITATION.md\n",
      "Processing file 10j43RA1lM3os05lw4subpJ1skLVzsREp patient133_4d.nii\n",
      "Processing file 1NqLfyXOOeh1CrjIlpD4OgDeI4i3dK97S patient133_frame01_gt.nii\n",
      "Processing file 1sz_3MARMn0PbiErj-yZlE8d_DoCeEsI2 patient133_frame01.nii\n",
      "Processing file 1tLIpYk24bed8_u12jvqUKe4Rv7rST4yC patient133_frame10_gt.nii\n",
      "Processing file 1GhsMdChCdCagFpN-hGC0FU-uoY0EbFOq patient133_frame10.nii\n",
      "Retrieving folder 1WjlAVzoKReZSxRJRyiyaOmVd-Ns-Ck9i patient134\n",
      "Processing file 1R9V5eRlHmxFEYF30SVhtUvSc0dTmFYjy Info.cfg\n",
      "Processing file 1gYdBS1JxiTYT-ukOTBkXDiz2LjNmZG4D MANDATORY_CITATION.md\n",
      "Processing file 11VLe8ro5KjRpzCfp461TT7QTzfQnqJic patient134_4d.nii\n",
      "Processing file 1wbpR4ZOMQpN31v3TM15SNji_HuZzWEvB patient134_frame01_gt.nii\n",
      "Processing file 1Yr3bJcHAk6z_CA20zZQzl5S0oU4UfV4a patient134_frame01.nii\n",
      "Processing file 1zWTI869sk0jVHvBYP_xQ6WJ3vWbyG5xZ patient134_frame15_gt.nii\n",
      "Processing file 1TK6Ihsb2g6M9XN1vnSVKeELk7LZWq9iW patient134_frame15.nii\n",
      "Retrieving folder 1_56nIifqrhSEYwaVgb9wqyKDqqdWVc6Z patient135\n",
      "Processing file 18ggwFwl-GCK2uWM7FkNAqfQIdcio7GkB Info.cfg\n",
      "Processing file 1MJ0X_edj5sItP-Jmr0aSdDj8QGf6mcoj MANDATORY_CITATION.md\n",
      "Processing file 1XoTO7sSmW5ZW59JeXFZaYNdwacbvkJVy patient135_4d.nii\n",
      "Processing file 1FaLSZVqRapXsg2QYYMJx0p1oCunuGgcX patient135_frame01_gt.nii\n",
      "Processing file 1k761UU5wcXTG8elE4J8fDw0ih0wJJd1Z patient135_frame01.nii\n",
      "Processing file 1F2HKh6Kl35eFUdfg-6aebHvVHTlPmO8K patient135_frame10_gt.nii\n",
      "Processing file 1qh3-VritFdF_TyOVKwJ1sjPGp3oyrdcW patient135_frame10.nii\n",
      "Retrieving folder 1U-5q2FT92FiUqHnLcLmQJpbxY5xEeJnA patient136\n",
      "Processing file 1H9lLMhR_fqQ36omB-1d0zHeDojiswSJZ Info.cfg\n",
      "Processing file 1aeu3fgWDDIW2cdYiRKoZc6ry6XMbr1Zf MANDATORY_CITATION.md\n",
      "Processing file 1aopwgCUnquo1dyG0p_meZURBUKu2rNmA patient136_4d.nii\n",
      "Processing file 1Vzo9IeVUU5mIUNMg2-zQ0LjKhRv4mizN patient136_frame01_gt.nii\n",
      "Processing file 163piPVtP2buVFhcmyFTQGNmiUGTun5Dn patient136_frame01.nii\n",
      "Processing file 1Rgz7ezrlkxZIzhZ92wJl9kuipvYaR0-7 patient136_frame12_gt.nii\n",
      "Processing file 1Hz_kWOw92Wrk07jfmRr25wIoytRqHNM0 patient136_frame12.nii\n",
      "Retrieving folder 15wNNqCs0dIUKN_NW8uI93wzoZix5s26Z patient137\n",
      "Processing file 1LLlaFXRiRdRu6vXAIXZp0hML3JZ-zh61 Info.cfg\n",
      "Processing file 1NeyMa-eDElrPevbpd_LZI3hjp4-faJlL MANDATORY_CITATION.md\n",
      "Processing file 1OybzKC_BxeZuk4H_9dghFVIc6l4wkQDb patient137_4d.nii\n",
      "Processing file 1djpgXg8U9AIkOh_3JMZaEoXx-GSiD4Nb patient137_frame01_gt.nii\n",
      "Processing file 1IDA0vgkPXsxWoxctOwl1-Fe3KmdU-QGQ patient137_frame01.nii\n",
      "Processing file 1WZUgHXc2J37gM25ShkeeQBbdPD4bMRn6 patient137_frame11_gt.nii\n",
      "Processing file 140zqGzPVtHSnLM9x4FG0dAGMBilqp99m patient137_frame11.nii\n",
      "Retrieving folder 1-YbKSBQyxgxH6R7_4akcoUtV0WvXCuBn patient138\n",
      "Processing file 1VaEYgrvUcs0qTZS2dnD19733i33ip1Iy Info.cfg\n",
      "Processing file 1-kYwJcLl01mLXUP70LtW9GVy-dZwHHqt MANDATORY_CITATION.md\n",
      "Processing file 1Ox9CmIdeJOUzNqshUpH0B31LfFkCfw-F patient138_4d.nii\n",
      "Processing file 16lQSmDUREW5mByNOf0BiwdNH49iL2Wet patient138_frame01_gt.nii\n",
      "Processing file 1fI9JaQ3FT5pmlzO6aExUp3MFU24o0ZS8 patient138_frame01.nii\n",
      "Processing file 1S0EqTxZibMSAyKv8SagqcdL6Nv6pi3Ra patient138_frame10_gt.nii\n",
      "Processing file 15tqpKSEGMMQBIR9l0dHW5JgQagirp4Ka patient138_frame10.nii\n",
      "Retrieving folder 1FcOGgUwnxh7BoHbSWBL7dwwaeLB4cWp7 patient139\n",
      "Processing file 1FBn5TU653NyKiypKfrFCDTRs1c5s-Ug1 Info.cfg\n",
      "Processing file 1jvyQmakpmhzpdiuvO8vf6FZFkrlZsjW_ MANDATORY_CITATION.md\n",
      "Processing file 1FlaoTOUumIhFwdSnaWUuSgioUt7UC6Sh patient139_4d.nii\n",
      "Processing file 1efi6mZQuxYQScuNoUYvHmyCYREKP_pjz patient139_frame01_gt.nii\n",
      "Processing file 1gCZZ076MXdoWXjMTGjoIyoNdo-r_TOCv patient139_frame01.nii\n",
      "Processing file 1GjmVHMW086J8Z4Q0zH_HAXlnmDvwYUui patient139_frame08_gt.nii\n",
      "Processing file 1qNrZBs6pRdTNMXQKtw23YjaQvrYIq5IZ patient139_frame08.nii\n",
      "Retrieving folder 18DwuH8-Icw4yPPKZeSBCJuQl_rLiVydB patient140\n",
      "Processing file 1tgKubaXAMdg25l2Oi5uD0b2FXUtm39YO Info.cfg\n",
      "Processing file 1r1u1ONPgGUefmdhATtJp-s1Sa72dCaWB MANDATORY_CITATION.md\n",
      "Processing file 1tLAxIeO5y0ty5tBeRroa2cJlJsOHF7Ni patient140_4d.nii\n",
      "Processing file 1HQxV0OyiAEDEJDFstiHa_pEKn3tQ4Vep patient140_frame01_gt.nii\n",
      "Processing file 1aV_NjvW3Z_TYCRYZU_1Tu8SQ2ZCZRTpn patient140_frame01.nii\n",
      "Processing file 12ZNrH1lTrOy9zF0DbFV7jl-L-1DPkuQQ patient140_frame09_gt.nii\n",
      "Processing file 1zciYTNI-8Bds2M7n3rtMtd0xEJKGExJy patient140_frame09.nii\n",
      "Retrieving folder 1fhNvqZ9CJ9F7JgnQSJIi_3xnnBvetGKw patient141\n",
      "Processing file 1eLqNZxh8h0dQQZH5j0-PwZapp3se1cA1 Info.cfg\n",
      "Processing file 1SwTmsc9VTMQFGsK2smfzN7BuP9PM9Xxq MANDATORY_CITATION.md\n",
      "Processing file 1Nxj9V49Tn20c4zbyem9OjfKCypjgDUNU patient141_4d.nii\n",
      "Processing file 1Ct_lq75vR_s5J92dZibZaXEhRe_90Ozg patient141_frame01_gt.nii\n",
      "Processing file 1QsoZI18MGdXn1iqUokpMw_rc_56KFKNN patient141_frame01.nii\n",
      "Processing file 19lptLjRCnVEW7uUqTsLSujAJbPqW8YUC patient141_frame11_gt.nii\n",
      "Processing file 1YzrzRI9Sc11y5G6GGZXa0Jp8j6pUNgAt patient141_frame11.nii\n",
      "Retrieving folder 1VffFTtMXWRPCx9SUp_KBIpKGEL4nvvFb patient142\n",
      "Processing file 1CcRQwkF-G5oHtWy1KZcbXo80LKAtM2wq Info.cfg\n",
      "Processing file 1ClXR_r7ELkL93yzWYKTQN4dOFZ4X3PJw MANDATORY_CITATION.md\n",
      "Processing file 1ilvCvfiuyPupMSkDnT4SFK_61GBSPNBv patient142_4d.nii\n",
      "Processing file 194xiG0idU-ALDQ7iOXGtOMv54bZX7HXk patient142_frame01_gt.nii\n",
      "Processing file 1YzmPe8Urbo1aDQs4RwPzfcZ2BOJbTFBk patient142_frame01.nii\n",
      "Processing file 1V4-ByO67G5KZR-2lF1vYqPybObH_Q2LL patient142_frame12_gt.nii\n",
      "Processing file 1dZLaL-EzrVaKndK_6Bob7w2vLIjrYLkF patient142_frame12.nii\n",
      "Retrieving folder 1ZOlvhyMruA4B8HD4L7Fgc3ot8mXlwwC2 patient143\n",
      "Processing file 15WQDqRHzinW6qPvP3ATSRvCFiPOdv-kN Info.cfg\n",
      "Processing file 1JXA9RJ_k3DKVZARmMQ-iqxU1VY3Q1FCE MANDATORY_CITATION.md\n",
      "Processing file 1FcxcNGZhdFLFBeVLPlJwgUjW54dtu_Ww patient143_4d.nii\n",
      "Processing file 1v2EsPT_UV9jVxuNypjRQAhyvpbFhOuL3 patient143_frame01_gt.nii\n",
      "Processing file 1ZBnJCfmwI6CxTBJTd9hPO_uh8QM10nhe patient143_frame01.nii\n",
      "Processing file 1geB5aMZ327OdLUzZu3QbmXaUiNjaRLPf patient143_frame12_gt.nii\n",
      "Processing file 1SOyh0UchrQ9Wz4gotRMM3uQAlp2F3QYL patient143_frame12.nii\n",
      "Retrieving folder 1VdaJFjNha1BZVRU3ljVQlVLf-eboB4Cm patient144\n",
      "Processing file 1LRePl-VrCXYI5aMXeiuP_GZ6iphv3nz9 Info.cfg\n",
      "Processing file 1GYvUiPpP5_fEhYi_mUevKR4WJqAzo9c9 MANDATORY_CITATION.md\n",
      "Processing file 1GChmXI8UhQ5_4TeRmXGrKJmqEK_0Jpzh patient144_4d.nii\n",
      "Processing file 1lKGlaIQsFyhwQMZsxuW2HPFzK9mHQVvw patient144_frame01_gt.nii\n",
      "Processing file 1N84NPILbFErYA82OqQC8lFeTZEgf1l3L patient144_frame01.nii\n",
      "Processing file 1mJyW4YVJepBSZLz1CUGqs_3X2ie_NtE4 patient144_frame09_gt.nii\n",
      "Processing file 1QLmIoQPWxdPsUHded4S2jviZazOTSb8H patient144_frame09.nii\n",
      "Retrieving folder 1JBb7UkT-wBQOkZ6WdRAGJt-_1Ce9E1DT patient145\n",
      "Processing file 1c5NjzR4s-n4M3uwQTMVkWZ16S1-lQKxj Info.cfg\n",
      "Processing file 1Or1GFsmD2lxxO1Wcs_NdxuFFvhYrq1da MANDATORY_CITATION.md\n",
      "Processing file 1a2Y-rubHVg1KTE6M1tnfNjVdbSmFMhDj patient145_4d.nii\n",
      "Processing file 1NBVYM74_uNW3NsIEQQhhRxQn95ITwUPw patient145_frame01_gt.nii\n",
      "Processing file 1aguFFr-cydhXaz_oc4yiv3R6TFHb8LeD patient145_frame01.nii\n",
      "Processing file 103sX-1KMd3CZVQAvKW8AmdrnehRU8dm_ patient145_frame13_gt.nii\n",
      "Processing file 1sx1j6UGPx78i9hmBli1DE5OlKnDUR-Nz patient145_frame13.nii\n",
      "Retrieving folder 19qGnCvTQj2Xx8T27r721QO-7KqRk_AVI patient146\n",
      "Processing file 1jstef_vgio7JY-ramgqgQB-UY0OcyQoT Info.cfg\n",
      "Processing file 1k7LP9frDQDXszwbXxRipkITtvaXLbiQ6 MANDATORY_CITATION.md\n",
      "Processing file 17wdtkc4p1uCzhlxGELqATnDUM9o3qqgT patient146_4d.nii\n",
      "Processing file 1knyaZ8hFPkfCi8yCqBbCekinrHzo6SgT patient146_frame01_gt.nii\n",
      "Processing file 1XEZPDYasc026TjTdxSdQ-np0Lq9vN6Xk patient146_frame01.nii\n",
      "Processing file 14SHkfe1PwIfQANP_PEuacqb7kJeQp9kD patient146_frame10_gt.nii\n",
      "Processing file 1FoLq-L8epfZnmr98XMZcQrk2Z4f0S9yg patient146_frame10.nii\n",
      "Retrieving folder 1DLQ7evM0gXQciIMlyNYAjbG8jTaHg9Ai patient147\n",
      "Processing file 1TJ-n3Gp2ygjFRr3VlXQq7nZ9maxqOe48 Info.cfg\n",
      "Processing file 1EeNJtstQ-sFAFdnTkpv_FGbR6rfSFqdt MANDATORY_CITATION.md\n",
      "Processing file 1ZAKErtIMwv-xqJ0rP2wC7_N0ZDIDGBN5 patient147_4d.nii\n",
      "Processing file 1h12NxnLWrw4OFB8Oz2IAxYQcBYwNLvhr patient147_frame01_gt.nii\n",
      "Processing file 1jEWG8aWPowcQwIK7zNM8d-nXHhIL8D7f patient147_frame01.nii\n",
      "Processing file 1bfJ9y-fQl1Tz4vcn5TZFBfPJM09J416T patient147_frame09_gt.nii\n",
      "Processing file 1FJ8H3eWL7Se88-oxdlHkRgQEtid9R_EZ patient147_frame09.nii\n",
      "Retrieving folder 1HARR3arZ4zvNveBNtOutPxkQsOV1ikTR patient148\n",
      "Processing file 1uA6Sl03tquVgyMxLqj5qsM8hENtPiIMM Info.cfg\n",
      "Processing file 1Ir9GrmlOmF1Iewv1-nQcKwWfYgzL0EgV MANDATORY_CITATION.md\n",
      "Processing file 1Qz-TsQvVoARt5sgJPrDnZhcUW5aJKpV6 patient148_4d.nii\n",
      "Processing file 1PKqJ84XDpOqcBYprGB3BL2TVad-hmgPt patient148_frame01_gt.nii\n",
      "Processing file 1SM7uFxFXeqyYjFsux7LlJjkTO82DAkPW patient148_frame01.nii\n",
      "Processing file 1EE7w8HHBohhPGWht8adsoTxOnjNWcqyE patient148_frame10_gt.nii\n",
      "Processing file 1uOqDqRyktvrWWhgS6naDkrPV6Rw80Y19 patient148_frame10.nii\n",
      "Retrieving folder 18VwnoVEab5s9ubkpPof2Pc98cTVLELAN patient149\n",
      "Processing file 1USbxEEAABWUt5Cb7O3bbx6mHR8FmSTZV Info.cfg\n",
      "Processing file 1rAZDU9TztTc7SjU27onCFJfU-gp3_b_5 MANDATORY_CITATION.md\n",
      "Processing file 1z2YWD9mpvkM6J4W0DkHIp_Eo4_5sestk patient149_4d.nii\n",
      "Processing file 1HsU29mqayVqIOqqOHxgeC4Tnmbt7mP5w patient149_frame01_gt.nii\n",
      "Processing file 1Mtz38iV8rQP10Iy3IVG_peD8SaKTulHa patient149_frame01.nii\n",
      "Processing file 1bKqUHXZqcwNmmu66O_ROvfLO-NHoQiI4 patient149_frame12_gt.nii\n",
      "Processing file 1R5buMYXXo-KQ2B2vUYSOuql4I8FJ2WCF patient149_frame12.nii\n",
      "Retrieving folder 11YVsGqOHWtbFEijRnqRKRBzYbIsIjSyx patient150\n",
      "Processing file 1CNkNDA_I5mqfAVlqds785e9jkE3bVr1_ Info.cfg\n",
      "Processing file 1Be_0BEkrxk36BbqxN2tWSSlosHgYwhxG MANDATORY_CITATION.md\n",
      "Processing file 1-v6Ek1wI9KBxtCw-d-_oHhIB6C1V1AhZ patient150_4d.nii\n",
      "Processing file 1OehuMRTMmB7-G25DBh4OPV6D1C_kBgke patient150_frame01_gt.nii\n",
      "Processing file 1qJw0fzHCRBPHvkqEBwyMf22tguJW3ZJ- patient150_frame01.nii\n",
      "Processing file 1n59j90_hFXwWMkbADd00n13K54DJjhpj patient150_frame12_gt.nii\n",
      "Processing file 1cmpSqkn6kxrJHIOf4-SLnybHNQQas6e5 patient150_frame12.nii\n",
      "Retrieving folder 15BKahUSkHyJv0tctCcavnoLS30YsvNY6 training\n",
      "Retrieving folder 1l7xFQbhtC20XueHxZRKN3pquK5hgc6Dv patient001\n",
      "Processing file 1D-mEcup4CaH3FxzIdKfESl-pK8Idrztd Info.cfg\n",
      "Processing file 12hi9bN7mxqw9zOWO_5Om-YzrsgsOl2rc MANDATORY_CITATION.md\n",
      "Processing file 1n4ftOU6QHSJc_eiA8fGvkVySTGoHzn7t patient001_4d.nii\n",
      "Processing file 1E8MQyAqHWCk_ACL7Wm0lmucPF0SCUN2n patient001_frame01_gt.nii\n",
      "Processing file 1akIad5kW2CaM0XvzHIRxy-TxEUYKtKxi patient001_frame01.nii\n",
      "Processing file 1O-JE1qo9_ak1lCio2Pf66PT8-m-NG8Ev patient001_frame12_gt.nii\n",
      "Processing file 1L70SIAYZ_tcxxH4VK1uhnx5q0kke1RX6 patient001_frame12.nii\n",
      "Retrieving folder 1mGe9ozfVWKdL180uzrVUA-_ckcI5LIhO patient002\n",
      "Processing file 15XDcU7Q8oK1_m75RdpB2H9L72KPb-Zk4 Info.cfg\n",
      "Processing file 1P123iFkMQtNC8f4AJLQ9kVqFrjd4h6sg MANDATORY_CITATION.md\n",
      "Processing file 11ExIaczLzgS_XNzXF67m3v2JVBNk9fCB patient002_4d.nii\n",
      "Processing file 1U4j4zXEARtepLOcFgvD3KcfW4yQ_DqJQ patient002_frame01_gt.nii\n",
      "Processing file 1lpYGlqrx-OICnvC2acMIEDNBOQxltua1 patient002_frame01.nii\n",
      "Processing file 1O5jGEoOoA-4pxnaR8AFv6GQkENMo3-DI patient002_frame12_gt.nii\n",
      "Processing file 1uphQOPUBoUOli4HG9kk6yLf_dLhcLBZ2 patient002_frame12.nii\n",
      "Retrieving folder 1ry5KTtEgeriGHa_x3IbSzbzRV-tHjc2o patient003\n",
      "Processing file 1h3f8TSzUmfOvGEwkw2CQB7ZpgKNMbw9K Info.cfg\n",
      "Processing file 1V_-SYCbldo3MJeQXKqD4R0BS326PoaQP MANDATORY_CITATION.md\n",
      "Processing file 1slXPDoIswUU2Vy8lElkJsBMGij1QFI7- patient003_4d.nii\n",
      "Processing file 1_WtR0WHz1MjYfizGnikShC0dJvyf1Bww patient003_frame01_gt.nii\n",
      "Processing file 1HYVzf857XX-3n-qfUKTtntoxNmoalrhY patient003_frame01.nii\n",
      "Processing file 1EJhva-Thfh7YklBEpI99iEgl7Oo94lGh patient003_frame15_gt.nii\n",
      "Processing file 1OPm0V-CsndqatM-TW2JVhxdGSPCckDj1 patient003_frame15.nii\n",
      "Retrieving folder 1i6LGSC8tnTwp5MS3eypWboa5i0zWTl-G patient004\n",
      "Processing file 1lp6u2tKpOX_6x2RNUPfjgSva6oEkdj0F Info.cfg\n",
      "Processing file 1Y8ZtntlzOb6L_EqCeJFdb06D49PkHtQy MANDATORY_CITATION.md\n",
      "Processing file 1tuQOAk2_cyt5WWhnQ-Uc-7_ZmBlzP4G3 patient004_4d.nii\n",
      "Processing file 1hRE1cNxlIKl6RF3lPAHCBh6JWWZ4RBy4 patient004_frame01_gt.nii\n",
      "Processing file 1KCGFfPNrH5k7Df55nwjgMEjpFEPUiAZJ patient004_frame01.nii\n",
      "Processing file 19G5T5eBLAZi2rmJ8i7ora1urw6u6_20H patient004_frame15_gt.nii\n",
      "Processing file 19CNej5pi4CDsxq1HVl9rjfG5r3rjpjHx patient004_frame15.nii\n",
      "Retrieving folder 1suhk3E3qMoVKNtvTnEtvIXEbSV-VJ0zy patient005\n",
      "Processing file 1e0GoKRkrLZqw8XIDUMF0P52j3IVEMKh_ Info.cfg\n",
      "Processing file 1o5D6AUWKDZNWXQqcG_9sOM21P5MV1qSm MANDATORY_CITATION.md\n",
      "Processing file 1uEUzalF8NwV9WwEIDeDFqnA6qSQW8voB patient005_4d.nii\n",
      "Processing file 11qyPdT0YAaj2id-J3dGQSSlpotSibsxe patient005_frame01_gt.nii\n",
      "Processing file 1Ikfwf3_wqd9sRqYodj27cV_nj4RtPyKF patient005_frame01.nii\n",
      "Processing file 15aqi7t6M_eItQNwlQHN4GYVNe4zFO9z1 patient005_frame13_gt.nii\n",
      "Processing file 1h2e6IAkwoltpWjHbZda1WaFA1ZiP29l1 patient005_frame13.nii\n",
      "Retrieving folder 1j-wIwDwI0jrvQa-iYRRFXXJ9sdnVjzHT patient006\n",
      "Processing file 17Vj7kzTIrsryUwjanF9I0junk49DKp3f Info.cfg\n",
      "Processing file 12VJHTm7aaZXSqLdGAWn2Snnum53I4Lkk MANDATORY_CITATION.md\n",
      "Processing file 1M-JTZxTqhgyHmw3BLQfR4I4apEVK2Mlt patient006_4d.nii\n",
      "Processing file 1W4IkPnAbWaYED17CuKwGDROo2ge6oAA0 patient006_frame01_gt.nii\n",
      "Processing file 1gBbtBBq3ifnaCh5J7B_-69Y4in2x0k6Z patient006_frame01.nii\n",
      "Processing file 1DlWaqhMzOXtbTprrpzHmU3ZrGhD6KJET patient006_frame16_gt.nii\n",
      "Processing file 1zTf5rGOyCDH6ymQbWFzk3EGsPv-ccTAW patient006_frame16.nii\n",
      "Retrieving folder 1vPcC0J1iuOlNJth-6BExlzZ6FHUHe0eb patient007\n",
      "Processing file 1o2ScUsFzPzBQ740uvzPCB2xPoAoPMEcn Info.cfg\n",
      "Processing file 1O01uJvV9RmENQ2ANl7wB77uWVp9UxjUy MANDATORY_CITATION.md\n",
      "Processing file 1KYy4ktn-9WtkCMpYHJwGIp0K0D2BTkWV patient007_4d.nii\n",
      "Processing file 11yemVkLIpHhOPjzwSloULDjPGTRF4OcR patient007_frame01_gt.nii\n",
      "Processing file 1EgOLOONSVV8GB_zkcMS5_iTwjqkGQKeA patient007_frame01.nii\n",
      "Processing file 1NBiwB2pOwHC4K3zZAvtgIaaFYvNmhsMV patient007_frame07_gt.nii\n",
      "Processing file 1AdE-xzTbtGaBR-FsVNwA0OVvm9qewJ2R patient007_frame07.nii\n",
      "Retrieving folder 15FSRsw1iMjXoCW31B3IeJwYetHcsCsnv patient008\n",
      "Processing file 1tG8vyU96BmLzrf-7A5eKtbgFDTtSpS2H Info.cfg\n",
      "Processing file 1ssp7-7z8nBLHR-buNOqvfOWZMIjsdGLF MANDATORY_CITATION.md\n",
      "Processing file 1tQPu3gL2y1bDfyiQlw9FAvysizntWZYo patient008_4d.nii\n",
      "Processing file 1VPE2Zu9kgRLYGDhm-0o9SDbSGIUQPuff patient008_frame01_gt.nii\n",
      "Processing file 1RAE7sx2oIekTq2PNueHxDUphzPctMe8c patient008_frame01.nii\n",
      "Processing file 13THyzTTOvPAacn7VqEB0y1LIVxt_xQFj patient008_frame13_gt.nii\n",
      "Processing file 1kVahxaYS2EEAWZAjB4XRJ0JILRqX6Jxd patient008_frame13.nii\n",
      "Retrieving folder 1bTqjJmMzaslt3CFboYrghs-ZyqBMEY6O patient009\n",
      "Processing file 1KZ_xbKm5yVWynzG6QC5tkgbzs9g8ejTy Info.cfg\n",
      "Processing file 1hW2QX6YwrPA07zs97Sht893gZPzb4_6q MANDATORY_CITATION.md\n",
      "Processing file 1HsOLAakbKr_iPGRFsvtyH_0zDbTPa2zv patient009_4d.nii\n",
      "Processing file 1Jf0r0DHtYWFNbt7qc5djDZUgJOH0ckaX patient009_frame01_gt.nii\n",
      "Processing file 1fg0ck5IWFuiBBOAr1VXv-PKxxys5F0MD patient009_frame01.nii\n",
      "Processing file 1JaUtUUSrAdu89S11Tb2aroVP4m5VsFEm patient009_frame13_gt.nii\n",
      "Processing file 1UuEFiC5AkgQq_juM7G8NsJwvTY1yKcL3 patient009_frame13.nii\n",
      "Retrieving folder 15EiAlueBegxFlpmYivLX_hgQff03b-8c patient010\n",
      "Processing file 1neuG4gOM7BXUHAPyt1ph9BgwDfr2epyR Info.cfg\n",
      "Processing file 1dQRkfxdxTmCZXK6vHVqi4IBOvPrdJ_r_ MANDATORY_CITATION.md\n",
      "Processing file 1XuYYuH4kR7JmXcwd7kH2hKw2ThxKevK0 patient010_4d.nii\n",
      "Processing file 19xIVHomz5V3Doq-ElrobKNekA0P9YBUY patient010_frame01_gt.nii\n",
      "Processing file 16_UoKnG2t5lGGd18pOL4RqCEWbnalVde patient010_frame01.nii\n",
      "Processing file 1YkIiwDi7aizpDCrg2gYH_MJ16vcWQEeR patient010_frame13_gt.nii\n",
      "Processing file 1x3_RAVwqU1xLlzccaqmS7R7K9OGsYmzS patient010_frame13.nii\n",
      "Retrieving folder 1PeFZ9BwkXjnoZsNvCSUiZKmGfCb5pNXo patient011\n",
      "Processing file 12q3yJXZPUnmNtlhc4lWrO7RB9MpAHMHu Info.cfg\n",
      "Processing file 1bWpF_InQ_74CsaUvz3uQ6I_A-VU6KHuf MANDATORY_CITATION.md\n",
      "Processing file 1mDoRLIjwpJWo7QfM5g8fTzUbzUIv4ReQ patient011_4d.nii\n",
      "Processing file 1sDpRXL13JmsZ7xTh47osqiUGbIwPFQKg patient011_frame01_gt.nii\n",
      "Processing file 1EzLUM9bHI_K80sZsMIvUTzosoiNHx96q patient011_frame01.nii\n",
      "Processing file 1avbOS_Xsi3iXM1g7fza2d5jynjkn95th patient011_frame08_gt.nii\n",
      "Processing file 1wxV_zBlOa_0El2uhTZ8Gplbibvg8dR8r patient011_frame08.nii\n",
      "Retrieving folder 1mlqoYLU6ijVdH6TGAmBYgBPUNtXtb8XX patient012\n",
      "Processing file 1vDKjpUHY0HBV9ih2Xu1yyIYXsVYmYsU- Info.cfg\n",
      "Processing file 1PEd7KA_8FI_uVgUeU2eQcqwH6MZwyQOv MANDATORY_CITATION.md\n",
      "Processing file 189AAtFQQ62DIIuodPm4XwMGme_ooE7fS patient012_4d.nii\n",
      "Processing file 1PfGtQPHmDXjju_8mtIISM4lFwGDRXCmO patient012_frame01_gt.nii\n",
      "Processing file 1L8_-gzOesx9akKieHf8XS87bvwiLiax9 patient012_frame01.nii\n",
      "Processing file 1BuLFYchT5HCV4mlOq57J29sCa1N-OjNN patient012_frame13_gt.nii\n",
      "Processing file 1HG_rWx6QInK1IcUlIAQ2lKALlCeOAKkb patient012_frame13.nii\n",
      "Retrieving folder 187Rrc4XQXmfLBzHGB9-ycTa8DVeldn9h patient013\n",
      "Processing file 1nEpjBd48N3Jheu1dnNXwu35ZWyHryDfF Info.cfg\n",
      "Processing file 1PuoNyA1CPJS6O1rpe0NzWebUFjDUCMZX MANDATORY_CITATION.md\n",
      "Processing file 1X8PkVWGL420KVE5Jr_7e3yIoK16_cXRI patient013_4d.nii\n",
      "Processing file 1HIAqHke67vG8-MbKUUk-3S29BvUi9Yrh patient013_frame01_gt.nii\n",
      "Processing file 1Avwb7QHha--r-0AqsXq5ByFfEQVAAOq3 patient013_frame01.nii\n",
      "Processing file 1Uad8FhBYA-izVkAk5kQw8tDrGSdnhMkO patient013_frame14_gt.nii\n",
      "Processing file 1MBKR9Jps2N2BV2C5LkDDAuCmpMQF0cJD patient013_frame14.nii\n",
      "Retrieving folder 158IcdI2ziXA_xNftbIw-pE_IEUI7laU_ patient014\n",
      "Processing file 1m2g-igRuygHnkn2PXyhI-gDLryiXiQjn Info.cfg\n",
      "Processing file 1mSP6UfHzi_UmbX9_OFaKCIfC_uaYpqIF MANDATORY_CITATION.md\n",
      "Processing file 1Y2Tu9UIdNiDdNqKx03ji1YKNaZuPkOdX patient014_4d.nii\n",
      "Processing file 1Zt4ZpJZJ4-wj2aPIaNUoeCutQxjqGm2f patient014_frame01_gt.nii\n",
      "Processing file 1kYD0E3VdtQb08tvjV27QIos3yMOYVqst patient014_frame01.nii\n",
      "Processing file 1x1g2U6Z_tKcV-nEEGnV3HC2YyeCw6ZDg patient014_frame13_gt.nii\n",
      "Processing file 1UmIBk-D0fA3ZJhcAhdgNj4_Mec62MuDE patient014_frame13.nii\n",
      "Retrieving folder 1LP4XgHHmyXaGw4aC5vWeQAZPJohnDAMF patient015\n",
      "Processing file 1EMpH0SCsAn1iPvPa2UOAyB9bkm2NJfHP Info.cfg\n",
      "Processing file 16-c_ktJSB11s1eXo79Vpu57WLSvhH6PE MANDATORY_CITATION.md\n",
      "Processing file 10SU9Dngq0xriIeAOWg2UB1zFaB6NM8Yo patient015_4d.nii\n",
      "Processing file 14KDvb72o5O3e8UAgcqBGPQR7Xdy8CGJd patient015_frame01_gt.nii\n",
      "Processing file 14OMe-4qfmk7_amy4b_K_XWYTAs2Z4QSt patient015_frame01.nii\n",
      "Processing file 1P5PctrAqcmpPx0lSVtGF6AUJKgfmDuGE patient015_frame10_gt.nii\n",
      "Processing file 1p96_AhyycO7NLyUdV-qfe00BsNOxBzKR patient015_frame10.nii\n",
      "Retrieving folder 14Ea8m0RWjGo4WrDcQwWa82Z9wbU8Z5yC patient016\n",
      "Processing file 1dhzrq0aWavwJhFxsuaqxfAeKrxiKKgOv Info.cfg\n",
      "Processing file 1Vc0MMqshQeWRw9YVjnKPodns60e_ASgW MANDATORY_CITATION.md\n",
      "Processing file 1WnVt5HiWBgATWb5S9n99PD1Bqp1uXfH0 patient016_4d.nii\n",
      "Processing file 1Ug1TbKd9p0hRbluxwsREg41RNE7fj6Cu patient016_frame01_gt.nii\n",
      "Processing file 1FORlCG45jidpbIDptBSM6tcxneKOAY2l patient016_frame01.nii\n",
      "Processing file 17nlGc6B9pYeSon7uIMD7lWGxkhbAZb49 patient016_frame12_gt.nii\n",
      "Processing file 1VvwSPLjx9XP0VcVh6vvRMQ_nWorzjBz_ patient016_frame12.nii\n",
      "Retrieving folder 16W4xnYX7-ewfQ8lLXHlyClb0e-56l1ro patient017\n",
      "Processing file 1wG5dVF_2rNGcVZ0xvXrRrKmEZp4YLYIO Info.cfg\n",
      "Processing file 1Zm7cph3Tq7Thyf0UcCnmrg14_Ef4-KAJ MANDATORY_CITATION.md\n",
      "Processing file 1kH9ygAsGMUCyA6j_2zq8s51U00lxNoEr patient017_4d.nii\n",
      "Processing file 16Es2LAIb4UfjxloPAV3zbnuS0NvVYTew patient017_frame01_gt.nii\n",
      "Processing file 1D2UpjVGYcaU1cTQp7Z5SDVEtsewYBfOz patient017_frame01.nii\n",
      "Processing file 10HWDFbBNtDaBrG2JmToZ6pUQzrx6CWE- patient017_frame09_gt.nii\n",
      "Processing file 1HCeDUHvdZEXAdFWBqPPMi15TVonPGmfv patient017_frame09.nii\n",
      "Retrieving folder 1bAOratWlypV6Ugj4neTeNg9jKB-TeDbo patient018\n",
      "Processing file 1BlZR9wov48vsvNnz1amF3AUBqNOhESs2 Info.cfg\n",
      "Processing file 1b5YRAsLP_BOQLASa_COiyjE8B8gbG1Cz MANDATORY_CITATION.md\n",
      "Processing file 1m_-CiGWCGVi6kLXFs-p1o_GnTrKVcmJN patient018_4d.nii\n",
      "Processing file 1KFM9B1R-of9Zop-Hox2ufOEqKT3cb_G2 patient018_frame01_gt.nii\n",
      "Processing file 1d3bSQwWM_S7p2JIxiyvdNwsmL8ZBgc4X patient018_frame01.nii\n",
      "Processing file 1Jah0KzSQI1So4WcU6lfLF-eKkRz5vPLv patient018_frame10_gt.nii\n",
      "Processing file 1CmwDNIhvWXkwJ1WCT5_t8puGMjr5Rgr- patient018_frame10.nii\n",
      "Retrieving folder 1Pno-sNxdC941J0kZiUTJcSNVDe1gD8pI patient019\n",
      "Processing file 1Ogcoq_nje5yX06Gt14cS8rTOCG0yeU6T Info.cfg\n",
      "Processing file 1a3M0JRew-iP1d341Gz6yfGEDwLlQ1l_3 MANDATORY_CITATION.md\n",
      "Processing file 1c8HXwsXrC5GFXTaRZy_FhOtMaqzDhVSN patient019_4d.nii\n",
      "Processing file 1CQ4myLbZZRBT0wmfj9B8O_TERPftAN5a patient019_frame01_gt.nii\n",
      "Processing file 1sJsQKo8cgTvQ7hEdEzKM917f-ZxGgFg- patient019_frame01.nii\n",
      "Processing file 1LNCzQwnbQ12NMYAuaUj81AXDdgoSoLGI patient019_frame11_gt.nii\n",
      "Processing file 1Oe8gCYuNVvB0ElkrYLziTo3byeSKZjDV patient019_frame11.nii\n",
      "Retrieving folder 1RrhK_GGZIuR-W5RVWofH8fUr3fv6tN7e patient020\n",
      "Processing file 1tiayrSR43Q6pUaDZflrFpGu4Piz-tyWj Info.cfg\n",
      "Processing file 1Z6KicjU1YHzQCEZDGvGh5WjJpi8RDt3B MANDATORY_CITATION.md\n",
      "Processing file 1-ZGw0xyZVUhtMWrg3f2h6cmZ5oWjZtPf patient020_4d.nii\n",
      "Processing file 1iw_FaqfCGPQM1RP8buZ2jv-YWTamHy_C patient020_frame01_gt.nii\n",
      "Processing file 1enANJ6JugMwAuIUf7M5MTSXc6LVQvhy3 patient020_frame01.nii\n",
      "Processing file 14f-QBGXoArHjIlYjM-V98SCHEH9uQbZl patient020_frame11_gt.nii\n",
      "Processing file 1Y9vngIMH-bfKVCvSzhR7Y4-VUtNVq9o6 patient020_frame11.nii\n",
      "Retrieving folder 1qIkCw23Bm2n4CBwFRLf3L7Knj1jc_SlH patient021\n",
      "Processing file 112dReJ6v2pnKXmhqcVAAoihX7Y8nV3_V Info.cfg\n",
      "Processing file 1Rlo1J4VPFGq10DHx98L1jsriHesMhn2_ MANDATORY_CITATION.md\n",
      "Processing file 1X3X08Fcq8D-qYQtdQ7pSoqj6ljA50FJi patient021_4d.nii\n",
      "Processing file 1YRT02CrJHTaaihon21qAFbNoYva9O4iq patient021_frame01_gt.nii\n",
      "Processing file 105N3RHUIrbyVD8YLnY8DUm1-Vbo3yc-9 patient021_frame01.nii\n",
      "Processing file 1CGZ44bxVDIp3E7nSlwP5SHUgoDWl0SgP patient021_frame13_gt.nii\n",
      "Processing file 19LsGrKoPFm8tblgbd2SZ3lWO_4Y-9-DY patient021_frame13.nii\n",
      "Retrieving folder 1AGydUL12K-r3LqZJ7ixjtjFhjGa6SG25 patient022\n",
      "Processing file 1MaOZqvl-JwVZC0963E-YDfcW1sM5xxGC Info.cfg\n",
      "Processing file 15d3ckddVDHIjp_Whuw6SpDMbmTJxTy2V MANDATORY_CITATION.md\n",
      "Processing file 1XzG5NlUzKec7aiTDpUwOsBE_oXdpuYOj patient022_4d.nii\n",
      "Processing file 1-QJhYTJIH7loo7pWfoXXnr3JU7wZ50sD patient022_frame01_gt.nii\n",
      "Processing file 13tZ1juClTlUuVELvmQvIrmAYaQ2aZnXN patient022_frame01.nii\n",
      "Processing file 17JWbGCv_QuWCPIX_LMIBupIMmA_21_Fz patient022_frame11_gt.nii\n",
      "Processing file 1wNUiVsQHRNK0683ej9lW511lxstmwBme patient022_frame11.nii\n",
      "Retrieving folder 1Aen4Ec88hovqgqS8Z8GHk_Em88SgWEYE patient023\n",
      "Processing file 1feDG0EQ7Ov7SDVFpU2sTnHjqeM9kK5J8 Info.cfg\n",
      "Processing file 1a_CAPFCgxySJNyQfehtdrNC5iiJIg-U3 MANDATORY_CITATION.md\n",
      "Processing file 1h5blOhJfYcIjRinT7Qi8AjPeyR2BSKtB patient023_4d.nii\n",
      "Processing file 1DDsMdPzg47fiiHtUiipA7o1tzBv-LcIL patient023_frame01_gt.nii\n",
      "Processing file 1FjHFJBw2axA53_qAN80OLI3MBjPAI0JG patient023_frame01.nii\n",
      "Processing file 14jUUNh4AXazpwmh6rLQfkqaA1A5GD-TA patient023_frame09_gt.nii\n",
      "Processing file 1tKvjvUmBGCZtbUqQqbUXUZLrbRny6yvk patient023_frame09.nii\n",
      "Retrieving folder 1glXWDCPB-N8g3YvUHTbJkTLaqaeCEg0f patient024\n",
      "Processing file 13xCwrGEk8UPdSqWiIoI1CLIRUGKo2_Bu Info.cfg\n",
      "Processing file 17Mus-hGp9ePjhWPIOQimJvkMk6hy4odP MANDATORY_CITATION.md\n",
      "Processing file 1F0vLI73En__MCgZ6Ozieh6s9V6oLoAgQ patient024_4d.nii\n",
      "Processing file 1tcrf1FZvwpXfXmAP2hkOsw75sT9shKOB patient024_frame01_gt.nii\n",
      "Processing file 1h_3cn-J1Bh8weU41BxPvtS5czcvg2sYU patient024_frame01.nii\n",
      "Processing file 1VUBZ3AiC_EG4uIN79dJ-yGIfgJro3KFt patient024_frame09_gt.nii\n",
      "Processing file 1158FDlnOIEMIYVyRl_qhSKKG_6KYYKel patient024_frame09.nii\n",
      "Retrieving folder 1GA_-_8qtNMR8ndbsPXnabBrQokidv8ax patient025\n",
      "Processing file 1tFMSqxqIu_ww4TFvpQV7tVyCo0agATy5 Info.cfg\n",
      "Processing file 1o-wYtkuZwKWB0YVGjRp2txHY-d6iYAfX MANDATORY_CITATION.md\n",
      "Processing file 13rlGk_vCQIhDfKUSJum4pUWYOK4izp9J patient025_4d.nii\n",
      "Processing file 1FvwKj4XOh-UWJb6tjJWTIYxieJ-TTXUd patient025_frame01_gt.nii\n",
      "Processing file 1f2pfP_PmzhebVS3xSDqF_efP9IBqQuo4 patient025_frame01.nii\n",
      "Processing file 1XnCtAssPjVvOYUHTT_ZrXILPfdk711iL patient025_frame09_gt.nii\n",
      "Processing file 1IyXcPWblTriWxfo6Yngd5RjQ8sC_T0GY patient025_frame09.nii\n",
      "Retrieving folder 1Gmk8BjlPOxaONRj5nhtpboxKr7p87PGr patient026\n",
      "Processing file 1VWpMec58OHrCLRc7IezQwxQsMqjI-0R4 Info.cfg\n",
      "Processing file 1ySAFk0WV6eWZFLaSnlJuOxwMaCas9Fh0 MANDATORY_CITATION.md\n",
      "Processing file 12kirJdHvuqz9dyatJ7Bk3Uyl9XO4wi9Y patient026_4d.nii\n",
      "Processing file 1e4DYTTMaQn_TAgsEVhPOOYQNt1FcrRTL patient026_frame01_gt.nii\n",
      "Processing file 1ArZT1BZ3Ia-D3xkYuLqYPDdnknc_iw7M patient026_frame01.nii\n",
      "Processing file 1it5HJOGt1WYsnWulEGr-3lSwUHkGLsy5 patient026_frame12_gt.nii\n",
      "Processing file 12apZr9tj-KcaRLnzLyJJ1MF5Q5T2Qi65 patient026_frame12.nii\n",
      "Retrieving folder 1cSvAOe-o8YXc_mIQqn0pCyrUpYKYLMq1 patient027\n",
      "Processing file 1Cn2gL68nVJjv4c0e-mRhcS3T7-yqdH-R Info.cfg\n",
      "Processing file 1gsfxBPAzORli3DtIbwlQ_d8k_hEzDBMt MANDATORY_CITATION.md\n",
      "Processing file 1kz5DsSJJr67dAP2np5AxMWg2ZLuGawfk patient027_4d.nii\n",
      "Processing file 1f9GEtRfK5D9Y4M_-J2YPM0b279glDkQX patient027_frame01_gt.nii\n",
      "Processing file 1amtuV9eZ9agps-AELV_6KmeEVJZYL4w1 patient027_frame01.nii\n",
      "Processing file 1zft_2P1vylDLUnwzbeolsqELVlyA07k9 patient027_frame11_gt.nii\n",
      "Processing file 1BfGEgh4OOliTxmef8dJtmE7d5FrPu_An patient027_frame11.nii\n",
      "Retrieving folder 1DXdbRbd_EjltVLPsACHKJbcwMUBufvTM patient028\n",
      "Processing file 184QVjDYJ9UtrHTclLIAANqrib9GZQDVc Info.cfg\n",
      "Processing file 1NqjzdhEi6eOE4jHLwrE7SoBYer0MnwkR MANDATORY_CITATION.md\n",
      "Processing file 1mDVcWX78pgeZg9HFWaihX1ucjN6BFYqK patient028_4d.nii\n",
      "Processing file 1GVCZTZM7EZncPTefkx1z0xsqbv-Nu_ZO patient028_frame01_gt.nii\n",
      "Processing file 1bza5y-6YWJbSEop-Xs0VCcNgQNoEUS7D patient028_frame01.nii\n",
      "Processing file 1R5T1O-DdNa1SonKmt8j9OFWBaZ4nztJ6 patient028_frame09_gt.nii\n",
      "Processing file 1v3Tm9prxTF-Km_EhCnGnQr8uCE8gWqpO patient028_frame09.nii\n",
      "Retrieving folder 1KsTSO3Ng59kfqYRi8TqjLHXw4YiKoPeB patient029\n",
      "Processing file 1Q30o_RBaMs2PfRTl8hc6ePVexs59h361 Info.cfg\n",
      "Processing file 1eVeSXx-tK8it2Gj1bEBdDN-0AoBiqZ8x MANDATORY_CITATION.md\n",
      "Processing file 1OTeGo80S8_GVHuGsTGhcDyc_qJYETMZ_ patient029_4d.nii\n",
      "Processing file 1xt7kCOcnVDzmTJ52owICrUPq8L-5ci1j patient029_frame01_gt.nii\n",
      "Processing file 1aVrGEsaTihIuHPAMoSdaCfnt5M5SR-fD patient029_frame01.nii\n",
      "Processing file 1G8981B5woYMghS-JN1Z577QXs00qBmYZ patient029_frame12_gt.nii\n",
      "Processing file 1yYqhyyQL3jl7UNcFj2KY5EIAT_Sl0GDb patient029_frame12.nii\n",
      "Retrieving folder 1GLFIk_jH3ALfCO9GKxxwgRIFyOwMQyTr patient030\n",
      "Processing file 1jmfAoYeEIN6IwB3bMUZPjZIlq--u2S6x Info.cfg\n",
      "Processing file 12RvQZQJkLkNUKA1IXlRikm4DC_94qHpR MANDATORY_CITATION.md\n",
      "Processing file 1AHs1HivB_ajkDl9bwu1a2r0nhM2RGxWi patient030_4d.nii\n",
      "Processing file 1l-edvoSaq8wP_H_bdEMlTDQgSSFh--a0 patient030_frame01_gt.nii\n",
      "Processing file 1FPqWH5m_KKWnTz4nG1RU9DfKqP3iUpfD patient030_frame01.nii\n",
      "Processing file 1E40VpkpM9It2lBGdzGbNKi7zEE9a301m patient030_frame12_gt.nii\n",
      "Processing file 15G1lkU2V3qbH17gDPHehI6x3lyjVN-6I patient030_frame12.nii\n",
      "Retrieving folder 1RNuqWiUxoR31bU3KHv3UO1O8mandCJ5z patient031\n",
      "Processing file 1TRvUYTlh04ml0UAF-mXnXndDOQHkdLdF Info.cfg\n",
      "Processing file 1cKIvuIExuLfMzsRBPmabIrocnUgMAqqU MANDATORY_CITATION.md\n",
      "Processing file 1b_c374pxvMCPA6T9mDEpMajz7TQeqR6d patient031_4d.nii\n",
      "Processing file 1tbw00q7h-KiHzaU7Xx42v7aLqEJqIamD patient031_frame01_gt.nii\n",
      "Processing file 1zRyHQqAFsg-OCkj2kwUxjeuz7n-5kOf_ patient031_frame01.nii\n",
      "Processing file 1gsmSVR8vc016l9cDkyNKMN-QJ9i7DXZ6 patient031_frame10_gt.nii\n",
      "Processing file 1i6ixgrrSVMetJWtxrUclzGlrztXPp-Lv patient031_frame10.nii\n",
      "Retrieving folder 1FjphMYoKQ2RzOokBO80b3pcRqrdnVSZC patient032\n",
      "Processing file 1LP8fUhJ5MTHfDlRV-diYF3kJ2gxdr7gg Info.cfg\n",
      "Processing file 1IQT4t_ftd4iLGi_2EFN9ffMVF3H2x5X9 MANDATORY_CITATION.md\n",
      "Processing file 1ZdYEqxj805brDs5jC-T_-51TOpI7uEzM patient032_4d.nii\n",
      "Processing file 1WAW37lVYWqHVM2V12WxpB4_JIjRwNakK patient032_frame01_gt.nii\n",
      "Processing file 18WlVysVN6OeMAdIAY6rDhVC9iflJEB3X patient032_frame01.nii\n",
      "Processing file 1RuLP82o1t9SMigZwdcEKCyLcSYPttF4H patient032_frame12_gt.nii\n",
      "Processing file 12hAGx8XerIBSYo5ptSV36GV_2SuaqNLr patient032_frame12.nii\n",
      "Retrieving folder 1SlWHMXrVO9Od15WKMDxlfg-qSOgxPC6O patient033\n",
      "Processing file 1G8vxAqCKr6BTzkf0P4RGqLwpa0YywtQf Info.cfg\n",
      "Processing file 1fqEbG3sIpma594yVgioV0ASU4VmCFrj0 MANDATORY_CITATION.md\n",
      "Processing file 1MNlMawKde-0lKGlVTFVRMrRnmw-9Jtp4 patient033_4d.nii\n",
      "Processing file 1V_s_CYBNH-7k3EY4SIU4HiFznDgxwKnX patient033_frame01_gt.nii\n",
      "Processing file 1wV22R6T2V3S2LTTjpi3JPKJvidZF_Bl0 patient033_frame01.nii\n",
      "Processing file 1xJu78NCk5ZGNTLrUQHH90WOdD9kMaggr patient033_frame14_gt.nii\n",
      "Processing file 10qHKDoNWR2_3DG_jVhplDejxKVkDj-wZ patient033_frame14.nii\n",
      "Retrieving folder 1li0rYUnm958yT2thrkxtHTFi-lkBc9OY patient034\n",
      "Processing file 1ltzorfDw5m8KGzJfvaG0BPY-_LPTuOKf Info.cfg\n",
      "Processing file 1OLura1Vm198rzPe7ynwiNUrjc2w4RjcY MANDATORY_CITATION.md\n",
      "Processing file 18HhGDiy6x90XV2dN8Qkdsy8xnncPQkJP patient034_4d.nii\n",
      "Processing file 10v63nKEAIKH0FM5uOdZo-Sdu_-FBFtX5 patient034_frame01_gt.nii\n",
      "Processing file 13xyuNUhdp3lbz3lIhKsDwTR6F9CS46vo patient034_frame01.nii\n",
      "Processing file 1sOSUF8NclLrMdxx2llErjDqhu6i4dLMG patient034_frame16_gt.nii\n",
      "Processing file 1_p7E7hdEj38jAd47k8u0mIzNQEx07iIx patient034_frame16.nii\n",
      "Retrieving folder 1m2bnep7MGq5vHhgddhh2ChuOsUojzGSo patient035\n",
      "Processing file 1GyrVrSfh-qGtdT2aVpRRT5sJxtOUDTad Info.cfg\n",
      "Processing file 1AIgqL5swg16aKOAU5T_aE88b4dk6yvpV MANDATORY_CITATION.md\n",
      "Processing file 1ymM6XvbApqwr_X06okobPPATraO0ZvZH patient035_4d.nii\n",
      "Processing file 1198va6GSbHMNyloCD0FJevDT1Koy8I21 patient035_frame01_gt.nii\n",
      "Processing file 1yn1PfrXCEnP9WfImYa2MAmXDu8WkEgzF patient035_frame01.nii\n",
      "Processing file 1Ia7aRkJ1oVEHZ3mHytEcijao62Hv7IK3 patient035_frame11_gt.nii\n",
      "Processing file 18LIwOK19VYUya0A7BPA9z23q4NiaGR20 patient035_frame11.nii\n",
      "Retrieving folder 1ASMFgtgI-M3vSpYfDFceBfGBhTkjuWQu patient036\n",
      "Processing file 1mYKvq-640O55wUu_eg4LjTvjtesZrEip Info.cfg\n",
      "Processing file 1Y1yjqZx1kIib8aAB01NrBF1rpLp3oQcp MANDATORY_CITATION.md\n",
      "Processing file 1aq56DEBc9CMMat0rTs1bpAeto-yA8OCH patient036_4d.nii\n",
      "Processing file 1J7VH4W6jn0kDQeZ_groeDhKdm03pBP5S patient036_frame01_gt.nii\n",
      "Processing file 1xkglS3ABvS12Vmzq5OYDKxnU6SrvQHwb patient036_frame01.nii\n",
      "Processing file 1XZIv89AMQ_gLl01gxAAegeFrisqM2hkL patient036_frame12_gt.nii\n",
      "Processing file 1XCaXuW60H0EVYRPim_XONEW-HBjFlQxx patient036_frame12.nii\n",
      "Retrieving folder 1R6sfRvrtPkVSx7QQd1LGDL-FoFuB4OMU patient037\n",
      "Processing file 1Cs_k5-l8MppQnw54S_iI6d4G-rKGvZy4 Info.cfg\n",
      "Processing file 1kfoIyknT6wmZT2XiHAQeZyU4ZIpHTDXZ MANDATORY_CITATION.md\n",
      "Processing file 1hN0OKigQV4k09mbv_DUduaLP5JNUGqkc patient037_4d.nii\n",
      "Processing file 1OOM3KMzg1FZoKjB7U4o8NC1urTtzSwiJ patient037_frame01_gt.nii\n",
      "Processing file 14eZABM_2B4WxAXEXv2DAlhhlQPEJWxBw patient037_frame01.nii\n",
      "Processing file 1EKnJELZIex-JJ5EhaVCcie1Ll8kJiaIH patient037_frame12_gt.nii\n",
      "Processing file 1mUPJbJcDKSIJ82oN5eaI58AbA-6QXlFN patient037_frame12.nii\n",
      "Retrieving folder 1zVdiUiXjfKlAMfWziLy_AxTEV6N6fIrk patient038\n",
      "Processing file 1J7qJal6Gap87MGMWrPjMzxs9Jrec29MX Info.cfg\n",
      "Processing file 1dQ_XL5PvuJOl4XTCZQvU7v_2BZ48PwEG MANDATORY_CITATION.md\n",
      "Processing file 1Xnzne-rUYHHLjAtCYL4rS05_ZZpjglCM patient038_4d.nii\n",
      "Processing file 1DIqlq9ZwFYNOpurVvnWiMRP2eB0ev2xh patient038_frame01_gt.nii\n",
      "Processing file 1wqEonNzZ6ZYHRpiQL1vrvE5JejQpXbr_ patient038_frame01.nii\n",
      "Processing file 1bk22srwCCKhSo03RxuPhvJ6sUzGLUJEa patient038_frame11_gt.nii\n",
      "Processing file 1Gw5susi2Wm_d8O8ivprGwaTWzCsqL8Yk patient038_frame11.nii\n",
      "Retrieving folder 1tS0kclMKFLx8XhCGTHYTXYgFrm2purBX patient039\n",
      "Processing file 19qYJ1_o_pbNYLYvbvOKeuITbJ6bUCIjd Info.cfg\n",
      "Processing file 12DfQtBCwkwqhNssPQ_zCtC3zlr9dJqeo MANDATORY_CITATION.md\n",
      "Processing file 1n3SOnyPjfkd2FG1vshFx-nZKe2giU1uP patient039_4d.nii\n",
      "Processing file 14GguzN5ciLH6EoYmW694aFP_0Y60QSop patient039_frame01_gt.nii\n",
      "Processing file 1ojWTkjPQAPRKGtjzkEjGs6POyGyjTef8 patient039_frame01.nii\n",
      "Processing file 1Pn-pDIVRadUoaXTZtAnrNhkHfrZ288GQ patient039_frame10_gt.nii\n",
      "Processing file 1dN48OKBIUkhjVfaw4AzyR90kh53PgvZt patient039_frame10.nii\n",
      "Retrieving folder 1Klk-6diNgNpPSOFWgqLnDrmLPyigPnns patient040\n",
      "Processing file 11JiiCnEGUFkfeAIcjeqfV8Ni4MKzb0Z4 Info.cfg\n",
      "Processing file 1S5eAU4HDDs2KI1eyz7qqBxfTOStrpkOE MANDATORY_CITATION.md\n",
      "Processing file 1Ke4dLRRfhkCi-aZSGq-aFs7SMbg1LtCE patient040_4d.nii\n",
      "Processing file 1q42z-Kp9HGOIXB8flDkExz6Jdp9vPsGF patient040_frame01_gt.nii\n",
      "Processing file 1MQrv5F1VsqAXZUTW1eUtOupQMOPnhgi4 patient040_frame01.nii\n",
      "Processing file 1VZ2SdLcpB9oZsJLydui3JhWycU_lsvog patient040_frame13_gt.nii\n",
      "Processing file 1jTQiDPolzKMe1p6-MGtnvoqV9xoQCug1 patient040_frame13.nii\n",
      "Retrieving folder 1Cp1CJat8mpfWUEN89h-cipVMnB1J_mkR patient041\n",
      "Processing file 1i1BtV48PUqjbzuFkUkXbJDTRSka0by7s Info.cfg\n",
      "Processing file 1ElmIJ0vWoPPYGotAHiZcwrbigy2MGCZd MANDATORY_CITATION.md\n",
      "Processing file 1MQa_uc_wfB1NLV8DGw2RI5oxWwmyT4nb patient041_4d.nii\n",
      "Processing file 1m1nv15WcHNbsPFotmcPb_t33bwJfIcim patient041_frame01_gt.nii\n",
      "Processing file 1J6zZaMTi7WxjyntPLVWZNK6tyDR0s8-P patient041_frame01.nii\n",
      "Processing file 1ozkJbkrIB16wU2c8uNh6k1vnIeMAqCzQ patient041_frame11_gt.nii\n",
      "Processing file 1nv1cMuSD4E7NJQyDKBNLxacu8eM2VA33 patient041_frame11.nii\n",
      "Retrieving folder 1IbSnlJ06o-lOA__a5lkPxW8JOnXAY3ZZ patient042\n",
      "Processing file 1D3G24AI-wAf-aL5jG_xr89OF1SzcrImD Info.cfg\n",
      "Processing file 1Ydhe9HCT0eV7lgpNvux9tw6zJnLRHR9a MANDATORY_CITATION.md\n",
      "Processing file 1PExAOFiBX6h-CqUhOrhtcYo2dTKcUprc patient042_4d.nii\n",
      "Processing file 1LSRjfiGa3Ni_OJgzIQzl4v6WVSlL_Rlk patient042_frame01_gt.nii\n",
      "Processing file 1P8PtnEJ1wxYWLjRsb2Upo3y5hfr9n5kn patient042_frame01.nii\n",
      "Processing file 1yS5NoNhLgfZUkY7L7GOpGzskUr8zF0t6 patient042_frame16_gt.nii\n",
      "Processing file 1vjS-P3Gk-wz9crIIfLICGdP8d-I0LiaC patient042_frame16.nii\n",
      "Retrieving folder 1z4k6XdsTUKoui0Lu7awLu8CrU_i1F9ie patient043\n",
      "Processing file 1qAFuiUmYSXnJiO2_eeoDMFAeb4SbarQ5 Info.cfg\n",
      "Processing file 1qPlpdiKCKqzWjvSONGkKWGKG2iul59SA MANDATORY_CITATION.md\n",
      "Processing file 18ZLBrgXhsKt82oA4MUJ-rDfH7S6cOoF0 patient043_4d.nii\n",
      "Processing file 1iMzaWei_DmEstiyr-s7pVS10kMkhlflv patient043_frame01_gt.nii\n",
      "Processing file 1WXUung_oyQQqBSKF1G_PCNo6YQUDLIlJ patient043_frame01.nii\n",
      "Processing file 170l4xeb17dRFYSxrfQUCfxg9pkMj8q73 patient043_frame07_gt.nii\n",
      "Processing file 1FjnnBAFrUrnsbYBfZOZqmncLgzv9fo8S patient043_frame07.nii\n",
      "Retrieving folder 1DcQDwTzONSYnuPxUyHghiAMDGZsa5YMT patient044\n",
      "Processing file 1WPPUzINNbij39G2qU-Pn5cbpS1GRoQjY Info.cfg\n",
      "Processing file 1XAiyZLac00m1w1oRbBqs6VUoqrCbBMnG MANDATORY_CITATION.md\n",
      "Processing file 1sdGGWY1E2Kkg1I9YFZldBMiR_1dbgybQ patient044_4d.nii\n",
      "Processing file 12o-ww_sd_f0I6zVLnjliiuV926sqkqOV patient044_frame01_gt.nii\n",
      "Processing file 1Asfboi2QHnBexfUfc5dNWAXmsGMqcnsF patient044_frame01.nii\n",
      "Processing file 1DfqGe-jrUUjGj027zjRrfrhuEIMDF5DI patient044_frame11_gt.nii\n",
      "Processing file 1GA9lOqbI1lrbaDjHrW1q88-sa_58TB_J patient044_frame11.nii\n",
      "Retrieving folder 1PofZM3gC3JS1EBfvZBzCxcb4xHEQJf5Y patient045\n",
      "Processing file 10b1-xyR7zs9JlIEibO1Px3M6vQGnMo_x Info.cfg\n",
      "Processing file 13izZ_9OrBu3qV0x5eTp-epCCk1_vBnnj MANDATORY_CITATION.md\n",
      "Processing file 1DmaVCJ24FpoJknQnSlt7iJcB5RNcCo5S patient045_4d.nii\n",
      "Processing file 16y15m7oGHreGElS7pDClcBaHgSXaOgeU patient045_frame01_gt.nii\n",
      "Processing file 1YYkyGRFv_sy2Gl3nLHDUHqdGHpRtsNJJ patient045_frame01.nii\n",
      "Processing file 1l8Cawypzi9ERZTya6qwuTBoayCnJl6Rg patient045_frame13_gt.nii\n",
      "Processing file 1R973pdZupOyUvaJ4GcmLuVyV96OzYjqB patient045_frame13.nii\n",
      "Retrieving folder 1aRyaSYrujqZjfm7YK7_paHgrcJJQV9Qa patient046\n",
      "Processing file 1w_vomBtTgIb2HeA1WA8lkg_RiyqZz_Hu Info.cfg\n",
      "Processing file 1JUn0PKcf1EQBNJTjiGE7b70AbCJM2COC MANDATORY_CITATION.md\n",
      "Processing file 19MQOEoshOGbhmdhIDpqb8CzEynREUR2K patient046_4d.nii\n",
      "Processing file 1TY7GeAe-HZetmbeh3QJszdwbVspMUmyX patient046_frame01_gt.nii\n",
      "Processing file 1KKVSorwoMaFt-eXl-bfd69dv_QzyQmrq patient046_frame01.nii\n",
      "Processing file 1C5rtqd-sl6Px0y2K6JYhyBF9_Jsywk6c patient046_frame10_gt.nii\n",
      "Processing file 1D3HTcX82MxwnE9JWWZkhplX8Xty0j-YA patient046_frame10.nii\n",
      "Retrieving folder 1Rthnct8vbI4b0xU5npYzK1yIHCt05wLk patient047\n",
      "Processing file 18OiWQ1-VHwRrfaIINx8W5ASeaJ73qkAz Info.cfg\n",
      "Processing file 1016VSyoCck-wzWZkI9aPVCyYKvtuGFbW MANDATORY_CITATION.md\n",
      "Processing file 1ylaw1SXHGXUU0LnbkT2eONEbdbXstLWM patient047_4d.nii\n",
      "Processing file 1QusGiJ-GLvXJj853EtZd53Q78NDDwef9 patient047_frame01_gt.nii\n",
      "Processing file 1CXmdKhQDH0PtrVF1NFYZ-kfkhqcto2-Q patient047_frame01.nii\n",
      "Processing file 1ksT4PUUgNXXyfBt0B8BY00O-ej-4P41Y patient047_frame09_gt.nii\n",
      "Processing file 1rb7n-wgfdoDRpPn4flD9tQzkprb4doSS patient047_frame09.nii\n",
      "Retrieving folder 1n9sTZyFtlcOodyvNItT340wMsnzz_HQH patient048\n",
      "Processing file 1KYjk0gN2T01wZd35VYQ6pJNN9vJ5cCub Info.cfg\n",
      "Processing file 1t5pTpxl1HCyeSI4uNoX89C9FcmrD28Xm MANDATORY_CITATION.md\n",
      "Processing file 1rlYuBwgSNC0Rp5I7erUwZi8VVGIOapQw patient048_4d.nii\n",
      "Processing file 108dZGLFgswFyeSM3QzKJvfw7xfyb7wst patient048_frame01_gt.nii\n",
      "Processing file 15zGEeKlWkkvBsO2yGxrzc4TCIMibBglm patient048_frame01.nii\n",
      "Processing file 1q4ujEY-gFvsn_fYBQuJFy-O2EBDwakv5 patient048_frame08_gt.nii\n",
      "Processing file 1ONgPvh70ri8aJ6zvXZHU4dn65zfN15gh patient048_frame08.nii\n",
      "Retrieving folder 1YT40UPnDZ150KXjZXTZaXtKd1Gjc618y patient049\n",
      "Processing file 1-DL-xc3LGxZIo2yokoI4M82zwSw2y4R1 Info.cfg\n",
      "Processing file 1TJgfi-xUXm_KEk6XPoE4sv8bKWPJNKX8 MANDATORY_CITATION.md\n",
      "Processing file 1gzXrzRYb7TCpeLP4sDN10LDzg1XWy2bM patient049_4d.nii\n",
      "Processing file 1cz31LFPZx-eTcDroVrVvmZSPreRdZLWM patient049_frame01_gt.nii\n",
      "Processing file 1GlgCy62iZX7RMQX5q1xA0WvCeUQ73lGB patient049_frame01.nii\n",
      "Processing file 1qMIl2uQWEqrz1OdukhJdNIul7-ttGcJz patient049_frame11_gt.nii\n",
      "Processing file 1zgYfMWtrBUhER8b3-n4qgLWOIFqyO1bk patient049_frame11.nii\n",
      "Retrieving folder 1m0zxKyrTg1jy0VAar_dhe1UhD3-C9qUW patient050\n",
      "Processing file 1QkYdP0OY4X0asQmPBDVeaPqZe5yhB78c Info.cfg\n",
      "Processing file 1x8sQ1g9yE0-tKpFs48gfnBau9lpJKIQO MANDATORY_CITATION.md\n",
      "Processing file 1J_tsnNGRZQBFpNKrpRcOIXTJ5fG6rqYK patient050_4d.nii\n",
      "Processing file 1Sj3C4sHH-qwXI7sJqzaQVQehrZDJwhwR patient050_frame01_gt.nii\n",
      "Processing file 1qj-UHd1pwzY0cQVgql4TFWBCch-XTC8q patient050_frame01.nii\n",
      "Processing file 15UyqS2K_VmKstuhQff6QViH10PXee2dY patient050_frame12_gt.nii\n",
      "Processing file 197tNsOLsExrGrqUAMvJSC7MlvNaP5kSK patient050_frame12.nii\n",
      "Processing file 1Yq4lX9dehdWVcnYESGdLaWfd6xOSCPF5 MANDATORY_CITATION.md\n",
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1qXKKh4DNxwiJA5zgqQDkWxIGloALUhA6\n",
      "To: /content/data/ACDC/testing/patient101/Info.cfg\n",
      "100% 63.0/63.0 [00:00<00:00, 325kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1TCvYlYh2P4HCPJUsOebas1llY_gd0rQU\n",
      "To: /content/data/ACDC/testing/patient101/MANDATORY_CITATION.md\n",
      "100% 361/361 [00:00<00:00, 2.09MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ihjf_DqLe4ywtefurEZE9rFq9_WyKaK8\n",
      "To: /content/data/ACDC/testing/patient101/patient101_4d.nii\n",
      "100% 71.3M/71.3M [00:00<00:00, 118MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1zLfzi4BU7E2b38P1owG9Aczey5uaFSHY\n",
      "To: /content/data/ACDC/testing/patient101/patient101_frame01_gt.nii\n",
      "100% 594k/594k [00:00<00:00, 173MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1G8GNBb-6h9pkHLsJrzIsE0sWFzFEubSr\n",
      "To: /content/data/ACDC/testing/patient101/patient101_frame01.nii\n",
      "100% 2.38M/2.38M [00:00<00:00, 243MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1IFCp8jhgswbqJ5kn0OmKiOuXr81kk9ks\n",
      "To: /content/data/ACDC/testing/patient101/patient101_frame14_gt.nii\n",
      "100% 594k/594k [00:00<00:00, 123MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1mzq3gThA3Ft4J86BAD4MIe5-y_mLiF-T\n",
      "To: /content/data/ACDC/testing/patient101/patient101_frame14.nii\n",
      "100% 2.38M/2.38M [00:00<00:00, 124MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=12YOhsYZ7EIcPhVo8grzXMDvvYwNOPaN-\n",
      "To: /content/data/ACDC/testing/patient102/Info.cfg\n",
      "100% 63.0/63.0 [00:00<00:00, 356kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1QgJak3ZukdD37ddJGEWjce0Rw7ZGSGuB\n",
      "To: /content/data/ACDC/testing/patient102/MANDATORY_CITATION.md\n",
      "100% 361/361 [00:00<00:00, 2.56MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1kGI4cLS2Sr3k0Br8VlEfar1YxDOD3wpt\n",
      "To: /content/data/ACDC/testing/patient102/patient102_4d.nii\n",
      "100% 53.1M/53.1M [00:00<00:00, 132MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1G72AKLtBVsr_U_Xy1hKsekasfjPDzN6J\n",
      "To: /content/data/ACDC/testing/patient102/patient102_frame01_gt.nii\n",
      "100% 443k/443k [00:00<00:00, 140MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1tq2fOT0QllW62-Y5XlvPq2fuMtRGyiTr\n",
      "To: /content/data/ACDC/testing/patient102/patient102_frame01.nii\n",
      "100% 1.77M/1.77M [00:00<00:00, 227MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1AcVh9eqHdSYHPsd5c5dO3_Sq958avIXb\n",
      "To: /content/data/ACDC/testing/patient102/patient102_frame13_gt.nii\n",
      "100% 443k/443k [00:00<00:00, 122MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1tOpf9qxflqzKlj1VsoIyCEtRLcgbJvos\n",
      "To: /content/data/ACDC/testing/patient102/patient102_frame13.nii\n",
      "100% 1.77M/1.77M [00:00<00:00, 217MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1w6zb0qlYduiKnlHjryuE_CVnAzZCEU9a\n",
      "To: /content/data/ACDC/testing/patient103/Info.cfg\n",
      "100% 65.0/65.0 [00:00<00:00, 459kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1r65W4mw7OL04SBZBeM19B7CnyDHrPAc7\n",
      "To: /content/data/ACDC/testing/patient103/MANDATORY_CITATION.md\n",
      "100% 361/361 [00:00<00:00, 2.55MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1CJo2zcC9cnG0R9HXjVDTt066TcuKkRjx\n",
      "To: /content/data/ACDC/testing/patient103/patient103_4d.nii\n",
      "100% 59.7M/59.7M [00:00<00:00, 123MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ba9q0hZRVnXDzfbMg96P1semPTlJSl2g\n",
      "To: /content/data/ACDC/testing/patient103/patient103_frame01_gt.nii\n",
      "100% 498k/498k [00:00<00:00, 126MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=19NBx1ikSWBCk8Zf8yiPe5y2UHHIh-5XD\n",
      "To: /content/data/ACDC/testing/patient103/patient103_frame01.nii\n",
      "100% 1.99M/1.99M [00:00<00:00, 212MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1HmvI_BZsZrDckRUl3iqMz4hAGfQ5jQbt\n",
      "To: /content/data/ACDC/testing/patient103/patient103_frame11_gt.nii\n",
      "100% 498k/498k [00:00<00:00, 164MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1fnD-bY6oaJsw11Cm2Ef5HJiuYl8E4Vki\n",
      "To: /content/data/ACDC/testing/patient103/patient103_frame11.nii\n",
      "100% 1.99M/1.99M [00:00<00:00, 219MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Xg8mNp2Filx6xaOTok5z8nx02FYWhrZ8\n",
      "To: /content/data/ACDC/testing/patient104/Info.cfg\n",
      "100% 63.0/63.0 [00:00<00:00, 408kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1niliOZ_VIgDbAtJ2k5UVuryYsMB-euui\n",
      "To: /content/data/ACDC/testing/patient104/MANDATORY_CITATION.md\n",
      "100% 361/361 [00:00<00:00, 2.56MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1krx_yH28mPDJ-_CYEhf6-_tplsPXYd8z\n",
      "To: /content/data/ACDC/testing/patient104/patient104_4d.nii\n",
      "100% 57.5M/57.5M [00:00<00:00, 78.9MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1_Lr0s0RE1wZnnhRpkeyizLI3-_R6oMOS\n",
      "To: /content/data/ACDC/testing/patient104/patient104_frame01_gt.nii\n",
      "100% 480k/480k [00:00<00:00, 75.3MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1MxUn5DRTz9Tvbz6vz48QqRrJk1buN_Z2\n",
      "To: /content/data/ACDC/testing/patient104/patient104_frame01.nii\n",
      "100% 1.92M/1.92M [00:00<00:00, 217MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1yHXxdFZ71hgA4WNa_74ILRVJEYegTXHe\n",
      "To: /content/data/ACDC/testing/patient104/patient104_frame11_gt.nii\n",
      "100% 480k/480k [00:00<00:00, 172MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1epGnZEuRoZz6GE2v4DpW06O-oPK7GsCY\n",
      "To: /content/data/ACDC/testing/patient104/patient104_frame11.nii\n",
      "100% 1.92M/1.92M [00:00<00:00, 228MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1xIR4XJVdQkVmGP_ecq85QUF8VJqZyKxS\n",
      "To: /content/data/ACDC/testing/patient105/Info.cfg\n",
      "100% 63.0/63.0 [00:00<00:00, 402kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1lZMZugDD7hk2LD5DHNO_auLmftzggizi\n",
      "To: /content/data/ACDC/testing/patient105/MANDATORY_CITATION.md\n",
      "100% 361/361 [00:00<00:00, 2.56MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1h1FPMss1Vxuy71xadBvGU-Fa5qGt4eV3\n",
      "To: /content/data/ACDC/testing/patient105/patient105_4d.nii\n",
      "100% 71.3M/71.3M [00:00<00:00, 117MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-SpcfC4diGxnDXfxZQfinOoSQsgLHQNB\n",
      "To: /content/data/ACDC/testing/patient105/patient105_frame01_gt.nii\n",
      "100% 594k/594k [00:00<00:00, 163MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1sQc_A3rMRsGhNDGmPfBTGH-cb5HzXFJY\n",
      "To: /content/data/ACDC/testing/patient105/patient105_frame01.nii\n",
      "100% 2.38M/2.38M [00:00<00:00, 177MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=18gAFZjSpXor8s7OSKJZjHjSGRsbCfNAO\n",
      "To: /content/data/ACDC/testing/patient105/patient105_frame10_gt.nii\n",
      "100% 594k/594k [00:00<00:00, 161MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ryJR9hdxiREq60vFg4-s_jDUlqmiaeUH\n",
      "To: /content/data/ACDC/testing/patient105/patient105_frame10.nii\n",
      "100% 2.38M/2.38M [00:00<00:00, 33.6MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1CnVugMN54w-2X-fN0MMLophnDnRFAVMc\n",
      "To: /content/data/ACDC/testing/patient106/Info.cfg\n",
      "100% 63.0/63.0 [00:00<00:00, 265kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1QD9ooO-8l-CcLGCiLvwaMzK8YzRIqyET\n",
      "To: /content/data/ACDC/testing/patient106/MANDATORY_CITATION.md\n",
      "100% 361/361 [00:00<00:00, 2.66MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1OUICoJiBc003nAO_QWM4xSGfSLrhlDY0\n",
      "To: /content/data/ACDC/testing/patient106/patient106_4d.nii\n",
      "100% 59.7M/59.7M [00:00<00:00, 116MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1hMTBtL00RCxxuhKGlKnwhoFHUBNgiGqe\n",
      "To: /content/data/ACDC/testing/patient106/patient106_frame01_gt.nii\n",
      "100% 498k/498k [00:00<00:00, 144MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1EVN8RV-5DW8ZDUWPxFAhI67nkX9nKj67\n",
      "To: /content/data/ACDC/testing/patient106/patient106_frame01.nii\n",
      "100% 1.99M/1.99M [00:00<00:00, 226MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1tzWWfr4bPrk-cIIa5t5LL6H_YRqAcLfR\n",
      "To: /content/data/ACDC/testing/patient106/patient106_frame13_gt.nii\n",
      "100% 498k/498k [00:00<00:00, 114MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=12CJB7aq1TqO2VN-KYR0GhuAYE5FQ-IiS\n",
      "To: /content/data/ACDC/testing/patient106/patient106_frame13.nii\n",
      "100% 1.99M/1.99M [00:00<00:00, 208MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1sEDfO3Bf5Q5lRs8JCaUqzgnidM7xOQ4a\n",
      "To: /content/data/ACDC/testing/patient107/Info.cfg\n",
      "100% 63.0/63.0 [00:00<00:00, 445kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1fGHin9Mwp3QkC0GOc4GW-iLguH_dXD2K\n",
      "To: /content/data/ACDC/testing/patient107/MANDATORY_CITATION.md\n",
      "100% 361/361 [00:00<00:00, 2.30MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ZEcU593NXhgRImJAF2sWljOO9-lsszoL\n",
      "To: /content/data/ACDC/testing/patient107/patient107_4d.nii\n",
      "100% 59.7M/59.7M [00:00<00:00, 110MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1VNvQhH8CuBqAUMox7PXSGNnpd7ErbvB_\n",
      "To: /content/data/ACDC/testing/patient107/patient107_frame01_gt.nii\n",
      "100% 498k/498k [00:00<00:00, 150MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1IxaiyVwwaFY0fF-ht0auLfql7DCrgYtm\n",
      "To: /content/data/ACDC/testing/patient107/patient107_frame01.nii\n",
      "100% 1.99M/1.99M [00:00<00:00, 213MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=16kd8ipm6CNStjCl0D7m4OXKZAsi2lxSF\n",
      "To: /content/data/ACDC/testing/patient107/patient107_frame10_gt.nii\n",
      "100% 498k/498k [00:00<00:00, 141MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1TL_hLhiy6yt6tpdLlT1FDu9sIPIYhutn\n",
      "To: /content/data/ACDC/testing/patient107/patient107_frame10.nii\n",
      "100% 1.99M/1.99M [00:00<00:00, 234MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1I5xcA8MHuS4WfXZI857m-gjEgOJ4vuvL\n",
      "To: /content/data/ACDC/testing/patient108/Info.cfg\n",
      "100% 62.0/62.0 [00:00<00:00, 425kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1KSHJQT_ky6v2_anz8GRPBWEAq5OaL2jN\n",
      "To: /content/data/ACDC/testing/patient108/MANDATORY_CITATION.md\n",
      "100% 361/361 [00:00<00:00, 2.37MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1RDa3dueMtEJRuGe7hlZz2UVGA8DrOPXZ\n",
      "To: /content/data/ACDC/testing/patient108/patient108_4d.nii\n",
      "100% 73.7M/73.7M [00:00<00:00, 97.6MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=16HfB99vsCeKsw5et7GC1w-9k5orB30sY\n",
      "To: /content/data/ACDC/testing/patient108/patient108_frame01_gt.nii\n",
      "100% 615k/615k [00:00<00:00, 179MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=16UzN3qQltBCItOrJtsHY7Xn1UWxHAwwU\n",
      "To: /content/data/ACDC/testing/patient108/patient108_frame01.nii\n",
      "100% 2.46M/2.46M [00:00<00:00, 29.3MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1zUso9_ieW68ww0H_lBlHau1Wxmoo0_9p\n",
      "To: /content/data/ACDC/testing/patient108/patient108_frame09_gt.nii\n",
      "100% 615k/615k [00:00<00:00, 169MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Wg8Q2fJSb9uOVWFY1PKusmVdM2PJf2do\n",
      "To: /content/data/ACDC/testing/patient108/patient108_frame09.nii\n",
      "100% 2.46M/2.46M [00:00<00:00, 254MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1B6fZA0hpI9BtUXtdJFNabUYdkqj8QRtS\n",
      "To: /content/data/ACDC/testing/patient109/Info.cfg\n",
      "100% 62.0/62.0 [00:00<00:00, 363kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1h8Uc9phkfbcoTbjQlv9H1v1V_fRJ4LpX\n",
      "To: /content/data/ACDC/testing/patient109/MANDATORY_CITATION.md\n",
      "100% 361/361 [00:00<00:00, 2.34MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1wMImPdaPjnui2VCYbC_ePWNJSDXEBgWw\n",
      "To: /content/data/ACDC/testing/patient109/patient109_4d.nii\n",
      "100% 38.7M/38.7M [00:00<00:00, 124MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=14VQUjboBjH_Uim-rueL4A-br4wbS2Zxg\n",
      "To: /content/data/ACDC/testing/patient109/patient109_frame01_gt.nii\n",
      "100% 323k/323k [00:00<00:00, 145MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1FIKmpeoabunMZPkvYPWlxX8E5cpBE3w6\n",
      "To: /content/data/ACDC/testing/patient109/patient109_frame01.nii\n",
      "100% 1.29M/1.29M [00:00<00:00, 173MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1zS9U-aikphyH-8zdStVqOnUUILWuSzTP\n",
      "To: /content/data/ACDC/testing/patient109/patient109_frame10_gt.nii\n",
      "100% 323k/323k [00:00<00:00, 118MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1uPbzOqG-3U9dne_C6lQ3-C51pOw6Yunq\n",
      "To: /content/data/ACDC/testing/patient109/patient109_frame10.nii\n",
      "100% 1.29M/1.29M [00:00<00:00, 213MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1vl3T0cVIsRRO6CCY0hmOXUIMqjFxdQyS\n",
      "To: /content/data/ACDC/testing/patient110/Info.cfg\n",
      "100% 64.0/64.0 [00:00<00:00, 442kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1mR8VFSDiFygapGoS5r6mmDa9XQEKmKRe\n",
      "To: /content/data/ACDC/testing/patient110/MANDATORY_CITATION.md\n",
      "100% 361/361 [00:00<00:00, 1.85MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1E8wF7kbxRn7Q30SyufF2NfcBt8Bmmeib\n",
      "To: /content/data/ACDC/testing/patient110/patient110_4d.nii\n",
      "100% 59.7M/59.7M [00:00<00:00, 171MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1YQi1mau23yeUkDQmZqglf_ct3XgY4IWB\n",
      "To: /content/data/ACDC/testing/patient110/patient110_frame01_gt.nii\n",
      "100% 498k/498k [00:00<00:00, 171MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1bJn5BSpY-3j3vtkjl8fUxoFACsPiv0XD\n",
      "To: /content/data/ACDC/testing/patient110/patient110_frame01.nii\n",
      "100% 1.99M/1.99M [00:00<00:00, 243MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1lkiSS7mCGDgm3pSjym46_-_-lT4KuPJX\n",
      "To: /content/data/ACDC/testing/patient110/patient110_frame11_gt.nii\n",
      "100% 498k/498k [00:00<00:00, 160MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1EKwuyxAcelyUgOpMKEVbKprzAJBneCYR\n",
      "To: /content/data/ACDC/testing/patient110/patient110_frame11.nii\n",
      "100% 1.99M/1.99M [00:00<00:00, 257MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1aIFA2P4GUOwpB6O-4-FDKaJpPmsW-tIy\n",
      "To: /content/data/ACDC/testing/patient111/Info.cfg\n",
      "100% 62.0/62.0 [00:00<00:00, 431kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-COIdSMqjRAEsilcPrrqPdzQCPnZapvM\n",
      "To: /content/data/ACDC/testing/patient111/MANDATORY_CITATION.md\n",
      "100% 361/361 [00:00<00:00, 2.47MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1NVhtKWcAgDMzmVnexxeZxJ-qJ3gghMYI\n",
      "To: /content/data/ACDC/testing/patient111/patient111_4d.nii\n",
      "100% 19.7M/19.7M [00:00<00:00, 75.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --folder \"https://drive.google.com/drive/folders/{DRIVE_FOLDER_ID}\" -O {RAW_DATA_DIR} --remaining-ok\n",
    "print(\"âœ… Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa89e59",
   "metadata": {},
   "source": [
    "### 3.3 Preprocess (NIfTI â†’ NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import configparser\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "\n",
    "def normalize_intensity(image):\n",
    "    \n",
    "    # Clip outliers\n",
    "    p05 = np.percentile(image, 0.5)\n",
    "    p995 = np.percentile(image, 99.5)\n",
    "    image = np.clip(image, p05, p995)\n",
    "    \n",
    "    # Z-score\n",
    "    mean = np.mean(image)\n",
    "    std = np.std(image)\n",
    "    if std > 0:\n",
    "        return (image - mean) / std\n",
    "    return image\n",
    "\n",
    "def preprocess_single_patient_acdc(patient_path, output_dir, target_size=(224, 224)):\n",
    "    \n",
    "    patient_folder = os.path.basename(patient_path)\n",
    "    info_cfg_path = os.path.join(patient_path, 'Info.cfg')\n",
    "    \n",
    "    # Táº¡o folder output cho slice\n",
    "    img_save_dir = os.path.join(output_dir, 'images')\n",
    "    mask_save_dir = os.path.join(output_dir, 'masks')\n",
    "    os.makedirs(img_save_dir, exist_ok=True)\n",
    "    os.makedirs(mask_save_dir, exist_ok=True)\n",
    "    \n",
    "    # Äá»c config Ä‘á»ƒ biáº¿t frame nÃ o lÃ  ED, frame nÃ o lÃ  ES\n",
    "    if not os.path.exists(info_cfg_path):\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        parser = configparser.ConfigParser()\n",
    "        with open(info_cfg_path, 'r') as f:\n",
    "            config_string = '[DEFAULT]\\n' + f.read()\n",
    "        parser.read_string(config_string)\n",
    "        ed_frame = int(parser['DEFAULT']['ED'])\n",
    "        es_frame = int(parser['DEFAULT']['ES'])\n",
    "    except Exception as e:\n",
    "        print(f\"  Error reading Info.cfg for {patient_folder}: {e}\")\n",
    "        return 0\n",
    "    \n",
    "    slices_saved = 0\n",
    "    \n",
    "    for frame_num, frame_name in [(ed_frame, 'ED'), (es_frame, 'ES')]:\n",
    "        img_filename = f'{patient_folder}_frame{frame_num:02d}.nii.gz'\n",
    "        mask_filename = f'{patient_folder}_frame{frame_num:02d}_gt.nii.gz'\n",
    "        \n",
    "        # TÃ¬m file (support cáº£ .nii vÃ  .nii.gz)\n",
    "        img_path = None\n",
    "        mask_path = None\n",
    "        for suffix in ['.gz', '']:\n",
    "            test_img = os.path.join(patient_path, img_filename.replace('.gz', '') if suffix == '' else img_filename)\n",
    "            test_mask = os.path.join(patient_path, mask_filename.replace('.gz', '') if suffix == '' else mask_filename)\n",
    "            if os.path.exists(test_img):\n",
    "                img_path = test_img\n",
    "                mask_path = test_mask\n",
    "                break\n",
    "        \n",
    "        if img_path is None or not os.path.exists(img_path):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Load NIfTI\n",
    "            img_nii = nib.load(img_path)\n",
    "            img_data = img_nii.get_fdata() # (H, W, D)\n",
    "            \n",
    "            mask_data = None\n",
    "            if os.path.exists(mask_path):\n",
    "                mask_data = nib.load(mask_path).get_fdata()\n",
    "            else:\n",
    "                continue # Bá» qua náº¿u khÃ´ng cÃ³ mask\n",
    "            \n",
    "            # 1. Normalize Intensity TRÆ¯á»šC khi resize (tÃ­nh trÃªn toÃ n volume 3D)\n",
    "            img_data = normalize_intensity(img_data)\n",
    "            \n",
    "            num_slices = img_data.shape[2]\n",
    "            \n",
    "            # 2. Xá»­ lÃ½ tá»«ng slice vÃ  lÆ°u ngay láº­p tá»©c\n",
    "            for i in range(num_slices):\n",
    "                slice_img = img_data[:, :, i]\n",
    "                slice_mask = mask_data[:, :, i]\n",
    "                \n",
    "                # Bá» qua slice Ä‘en thui (khÃ´ng cÃ³ thÃ´ng tin) Ä‘á»ƒ trÃ¡nh nhiá»…u training\n",
    "                if np.sum(slice_img) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Resize (LÆ°u Ã½: resize cá»§a skimage range input=output, Ä‘Ã£ normalize thÃ¬ váº«n giá»¯ range)\n",
    "                slice_img_resized = resize(\n",
    "                    slice_img, target_size, order=1, preserve_range=True, anti_aliasing=True, mode='reflect'\n",
    "                ).astype(np.float32)\n",
    "                \n",
    "                slice_mask_resized = resize(\n",
    "                    slice_mask, target_size, order=0, preserve_range=True, anti_aliasing=False, mode='reflect'\n",
    "                ).astype(np.uint8) # Mask pháº£i lÃ  int\n",
    "                \n",
    "                # Táº¡o tÃªn file: patient001_ED_slice005.npy\n",
    "                file_id = f\"{patient_folder}_{frame_name}_slice{i:03d}\"\n",
    "                \n",
    "                np.save(os.path.join(img_save_dir, f\"{file_id}.npy\"), slice_img_resized)\n",
    "                np.save(os.path.join(mask_save_dir, f\"{file_id}.npy\"), slice_mask_resized)\n",
    "                \n",
    "                slices_saved += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {patient_folder} frame {frame_num}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    return slices_saved\n",
    "\n",
    "def preprocess_acdc_dataset(input_dir, output_dir, target_size=(224, 224)):\n",
    "    \n",
    "    # Láº¥y danh sÃ¡ch patient\n",
    "    patient_folders = sorted([\n",
    "        os.path.join(input_dir, d) \n",
    "        for d in os.listdir(input_dir) \n",
    "        if os.path.isdir(os.path.join(input_dir, d)) and d.startswith('patient')\n",
    "    ])\n",
    "    \n",
    "    print(f\"Found {len(patient_folders)} patients. Outputting 2D slices to {output_dir}\")\n",
    "    \n",
    "    total_slices = 0\n",
    "    \n",
    "    for patient_path in tqdm(patient_folders, desc=\"Processing ACDC\"):\n",
    "        slices = preprocess_single_patient_acdc(patient_path, output_dir, target_size)\n",
    "        total_slices += slices\n",
    "        \n",
    "    print(f\"\\nCompleted! Saved {total_slices} slices total.\")\n",
    "    print(f\"Images: {os.path.join(output_dir, 'images')}\")\n",
    "    print(f\"Masks:  {os.path.join(output_dir, 'masks')}\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input', type=str, required=True, help='Path to ACDC data folder')\n",
    "    parser.add_argument('--output', type=str, required=True, help='Path to save .npy slices')\n",
    "    parser.add_argument('--size', type=int, default=224, help='Target size (e.g., 224)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    preprocess_acdc_dataset(args.input, args.output, (args.size, args.size))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269610de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing\n",
    "from pathlib import Path\n",
    "config = PreprocessConfig(\n",
    "    input_dir=RAW_DATA_DIR,\n",
    "    output_dir=PREPROCESSED_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE)\n",
    ")\n",
    "preprocessor = ACDCPreprocessor(config)\n",
    "result = preprocessor.process_dataset()\n",
    "print(f\"âœ… Preprocessed {result.total_volumes} volumes, {result.total_slices} slices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7e6cd",
   "metadata": {},
   "source": [
    "### 3.4 Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8876d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ACDCDataset(Dataset):\n",
    "    def __init__(self, data_dir, split='train', min_foreground=50):\n",
    "        self.slices = []\n",
    "        with open(os.path.join(data_dir, 'metadata.json'), 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        volume_ids = metadata.get(f'{split}_volumes', [])\n",
    "        for vol_id in volume_ids:\n",
    "            vol_path = os.path.join(data_dir, 'volumes', f'{vol_id}.npy')\n",
    "            mask_path = os.path.join(data_dir, 'masks', f'{vol_id}.npy')\n",
    "            if os.path.exists(vol_path) and os.path.exists(mask_path):\n",
    "                mask = np.load(mask_path)\n",
    "                for i in range(mask.shape[2]):\n",
    "                    if np.sum(mask[:,:,i] > 0) >= min_foreground:\n",
    "                        self.slices.append((vol_path, mask_path, i))\n",
    "        print(f\"   {split.upper()}: {len(self.slices)} slices\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        vol_path, mask_path, i = self.slices[idx]\n",
    "        img = np.load(vol_path, mmap_mode='r')[:,:,i].copy()\n",
    "        seg = np.load(mask_path, mmap_mode='r')[:,:,i].copy()\n",
    "        return torch.from_numpy(img).unsqueeze(0).float(), torch.from_numpy(seg).long()\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_dataset = ACDCDataset(PREPROCESSED_DIR, 'train')\n",
    "val_dataset = ACDCDataset(PREPROCESSED_DIR, 'val')\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(f\"âœ… Train: {len(train_dataset)}, Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d238cbc",
   "metadata": {},
   "source": [
    "---\n",
    "## 4ï¸âƒ£ Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9b806",
   "metadata": {},
   "source": [
    "### 4.1 Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EGMNet(in_channels=1, num_classes=NUM_CLASSES, img_size=IMG_SIZE).to(device)\n",
    "print(f\"âœ… EGM-Net: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b160f",
   "metadata": {},
   "source": [
    "### 4.2 Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d48bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SpectralDualLoss(spatial_weight=1.0, freq_weight=0.1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a9d3f",
   "metadata": {},
   "source": [
    "### 4.3 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45942c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "best_dice = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "        loss = criterion(pred, masks)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_dice = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "            pred_cls = pred.argmax(dim=1)\n",
    "            for c in range(1, NUM_CLASSES):\n",
    "                intersection = ((pred_cls == c) & (masks == c)).sum()\n",
    "                union = (pred_cls == c).sum() + (masks == c).sum()\n",
    "                val_dice += (2 * intersection / (union + 1e-5)).item()\n",
    "    val_dice /= len(val_loader) * (NUM_CLASSES - 1)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss={train_loss/len(train_loader):.4f}, Dice={val_dice:.4f}\")\n",
    "    if val_dice > best_dice:\n",
    "        best_dice = val_dice\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"   âœ… Best model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4614a502",
   "metadata": {},
   "source": [
    "---\n",
    "## 5ï¸âƒ£ Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "images, masks = next(iter(val_loader))\n",
    "images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "    pred_cls = pred.argmax(dim=1)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "for i in range(4):\n",
    "    axes[0, i].imshow(images[i, 0].cpu(), cmap='gray')\n",
    "    axes[0, i].set_title('Input')\n",
    "    axes[1, i].imshow(masks[i].cpu(), cmap='jet', vmin=0, vmax=NUM_CLASSES-1)\n",
    "    axes[1, i].set_title('Ground Truth')\n",
    "    axes[2, i].imshow(pred_cls[i].cpu(), cmap='jet', vmin=0, vmax=NUM_CLASSES-1)\n",
    "    axes[2, i].set_title('Prediction')\n",
    "for ax in axes.flatten():\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
