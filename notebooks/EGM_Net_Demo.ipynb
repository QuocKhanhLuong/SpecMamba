{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4d6004",
   "metadata": {},
   "source": [
    "# ðŸ§  EGM-Net: Energy-Gated Gabor Mamba Network\n",
    "\n",
    "**Medical Image Segmentation with Implicit Neural Representations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879d899",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install mamba-ssm -q torch torchvision numpy matplotlib tqdm gdown nibabel scikit-image monai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307547ca",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Model Architecture (Standalone SOTA Implementation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2401869c",
   "metadata": {},
   "source": [
    "Full implementation of EGM-Net, HRNetV2-Mamba, and Physics-based Layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b8855",
   "metadata": {},
   "source": [
    "### data_utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data utilities for medical image segmentation with Monogenic Signal support.\n",
    "\n",
    "Includes:\n",
    "- JointVectorRotation: Augmentation that rotates images and Riesz vectors synchronously\n",
    "- MonogenicDataset: Dataset with pre-computed Monogenic Signal components\n",
    "- MedicalImageSegmentationDataset: Base dataset class\n",
    "- MetricsCalculator: Segmentation metrics computation\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Tuple, Optional, Dict, List, Union\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Augmentation Transforms\n",
    "# =============================================================================\n",
    "\n",
    "class JointVectorRotation:\n",
    "    \"\"\"\n",
    "    Synchronously rotate image, Riesz vectors, and mask.\n",
    "    \n",
    "    When rotating an image, the Riesz vector field (Rx, Ry) must be:\n",
    "    1. Geometrically rotated (pixel positions move)\n",
    "    2. Vector-rotated (direction at each pixel changes)\n",
    "    \n",
    "    This avoids expensive FFT re-computation during augmentation.\n",
    "    \n",
    "    Mathematical basis:\n",
    "        If image rotates by angle Î¸, then at each pixel:\n",
    "        [Rx']   [cos(Î¸)  -sin(Î¸)] [Rx]\n",
    "        [Ry'] = [sin(Î¸)   cos(Î¸)] [Ry]\n",
    "    \n",
    "    Args:\n",
    "        angle_range: Tuple of (min_angle, max_angle) in degrees\n",
    "        p: Probability of applying rotation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, angle_range: Tuple[float, float] = (-180, 180), p: float = 1.0):\n",
    "        self.angle_range = angle_range\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self, image: torch.Tensor, riesz_vec: torch.Tensor, \n",
    "                 mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Apply synchronized rotation.\n",
    "        \n",
    "        Args:\n",
    "            image: Intensity image (1, H, W) or (C, H, W)\n",
    "            riesz_vec: Riesz components (2, H, W) containing [Rx, Ry]\n",
    "            mask: Segmentation mask (H, W) or (1, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (rotated_image, rotated_riesz, rotated_mask)\n",
    "        \"\"\"\n",
    "        if random.random() > self.p:\n",
    "            return image, riesz_vec, mask\n",
    "        \n",
    "        angle = random.uniform(self.angle_range[0], self.angle_range[1])\n",
    "        \n",
    "        # 1. Geometric Rotation (pixel positions)\n",
    "        image = TF.rotate(image, angle, interpolation=TF.InterpolationMode.BILINEAR)\n",
    "        mask = TF.rotate(mask.unsqueeze(0) if mask.dim() == 2 else mask, \n",
    "                        angle, interpolation=TF.InterpolationMode.NEAREST)\n",
    "        if mask.dim() == 3 and mask.shape[0] == 1:\n",
    "            mask = mask.squeeze(0)\n",
    "        riesz_vec = TF.rotate(riesz_vec, angle, interpolation=TF.InterpolationMode.BILINEAR)\n",
    "        \n",
    "        # 2. Vector Re-orientation (direction at each pixel)\n",
    "        # Negative angle because image coordinate y-axis is inverted\n",
    "        theta = torch.tensor(-angle * math.pi / 180.0, dtype=riesz_vec.dtype)\n",
    "        cos_a = torch.cos(theta)\n",
    "        sin_a = torch.sin(theta)\n",
    "        \n",
    "        rx, ry = riesz_vec[0], riesz_vec[1]\n",
    "        \n",
    "        # 2D rotation matrix applied element-wise\n",
    "        new_rx = rx * cos_a - ry * sin_a\n",
    "        new_ry = rx * sin_a + ry * cos_a\n",
    "        \n",
    "        new_riesz = torch.stack([new_rx, new_ry], dim=0)\n",
    "        \n",
    "        return image, new_riesz, mask\n",
    "\n",
    "\n",
    "class JointRandomFlip:\n",
    "    \"\"\"\n",
    "    Synchronously flip image, Riesz vectors, and mask.\n",
    "    \n",
    "    When flipping, Riesz vector components must also be negated appropriately:\n",
    "    - Horizontal flip: Rx -> -Rx\n",
    "    - Vertical flip: Ry -> -Ry\n",
    "    \n",
    "    Args:\n",
    "        p_horizontal: Probability of horizontal flip\n",
    "        p_vertical: Probability of vertical flip\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, p_horizontal: float = 0.5, p_vertical: float = 0.5):\n",
    "        self.p_horizontal = p_horizontal\n",
    "        self.p_vertical = p_vertical\n",
    "    \n",
    "    def __call__(self, image: torch.Tensor, riesz_vec: torch.Tensor,\n",
    "                 mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Apply synchronized flip.\"\"\"\n",
    "        \n",
    "        # Horizontal flip\n",
    "        if random.random() < self.p_horizontal:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask.unsqueeze(0) if mask.dim() == 2 else mask)\n",
    "            if mask.dim() == 3 and mask.shape[0] == 1:\n",
    "                mask = mask.squeeze(0)\n",
    "            riesz_vec = TF.hflip(riesz_vec)\n",
    "            # Negate Rx component\n",
    "            riesz_vec[0] = -riesz_vec[0]\n",
    "        \n",
    "        # Vertical flip\n",
    "        if random.random() < self.p_vertical:\n",
    "            image = TF.vflip(image)\n",
    "            mask = TF.vflip(mask.unsqueeze(0) if mask.dim() == 2 else mask)\n",
    "            if mask.dim() == 3 and mask.shape[0] == 1:\n",
    "                mask = mask.squeeze(0)\n",
    "            riesz_vec = TF.vflip(riesz_vec)\n",
    "            # Negate Ry component\n",
    "            riesz_vec[1] = -riesz_vec[1]\n",
    "        \n",
    "        return image, riesz_vec, mask\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset Classes\n",
    "# =============================================================================\n",
    "\n",
    "class MonogenicDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset with pre-computed Monogenic Signal components.\n",
    "    \n",
    "    Expects data in .npy format with structure:\n",
    "    - intensity: (H, W) grayscale image\n",
    "    - riesz_x: (H, W) Riesz x-component\n",
    "    - riesz_y: (H, W) Riesz y-component\n",
    "    - mask: (H, W) segmentation mask\n",
    "    \n",
    "    The Riesz components should be pre-computed offline to avoid FFT overhead.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing .npy files\n",
    "        img_size: Target image size for resizing\n",
    "        augment: Whether to apply data augmentation\n",
    "        normalize: Whether to normalize intensity to [0, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: Union[str, Path], img_size: int = 256,\n",
    "                 augment: bool = True, normalize: bool = True):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        # Find all sample files\n",
    "        self.samples = sorted(list(self.data_dir.glob(\"*.npy\")))\n",
    "        \n",
    "        if len(self.samples) == 0:\n",
    "            raise ValueError(f\"No .npy files found in {data_dir}\")\n",
    "        \n",
    "        # Augmentation transforms\n",
    "        if augment:\n",
    "            self.rotation = JointVectorRotation(angle_range=(-30, 30), p=0.5)\n",
    "            self.flip = JointRandomFlip(p_horizontal=0.5, p_vertical=0.5)\n",
    "        else:\n",
    "            self.rotation = None\n",
    "            self.flip = None\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def _load_sample(self, path: Path) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Load a sample from .npy file.\"\"\"\n",
    "        data = np.load(path, allow_pickle=True).item()\n",
    "        return data\n",
    "    \n",
    "    def _preprocess(self, data: Dict[str, np.ndarray]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Preprocess loaded data.\"\"\"\n",
    "        # Extract components\n",
    "        intensity = torch.from_numpy(data['intensity']).float()\n",
    "        riesz_x = torch.from_numpy(data['riesz_x']).float()\n",
    "        riesz_y = torch.from_numpy(data['riesz_y']).float()\n",
    "        mask = torch.from_numpy(data['mask']).long()\n",
    "        \n",
    "        # Add channel dimension to intensity if needed\n",
    "        if intensity.dim() == 2:\n",
    "            intensity = intensity.unsqueeze(0)  # (1, H, W)\n",
    "        \n",
    "        # Stack Riesz components\n",
    "        riesz_vec = torch.stack([riesz_x, riesz_y], dim=0)  # (2, H, W)\n",
    "        \n",
    "        # Normalize intensity\n",
    "        if self.normalize:\n",
    "            i_min, i_max = intensity.min(), intensity.max()\n",
    "            if i_max > i_min:\n",
    "                intensity = (intensity - i_min) / (i_max - i_min)\n",
    "        \n",
    "        return intensity, riesz_vec, mask\n",
    "    \n",
    "    def _resize(self, intensity: torch.Tensor, riesz_vec: torch.Tensor, \n",
    "                mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Resize to target size.\"\"\"\n",
    "        if intensity.shape[-1] != self.img_size or intensity.shape[-2] != self.img_size:\n",
    "            intensity = F.interpolate(\n",
    "                intensity.unsqueeze(0), size=(self.img_size, self.img_size),\n",
    "                mode='bilinear', align_corners=True\n",
    "            ).squeeze(0)\n",
    "            \n",
    "            riesz_vec = F.interpolate(\n",
    "                riesz_vec.unsqueeze(0), size=(self.img_size, self.img_size),\n",
    "                mode='bilinear', align_corners=True\n",
    "            ).squeeze(0)\n",
    "            \n",
    "            mask = F.interpolate(\n",
    "                mask.float().unsqueeze(0).unsqueeze(0),\n",
    "                size=(self.img_size, self.img_size),\n",
    "                mode='nearest'\n",
    "            ).squeeze(0).squeeze(0).long()\n",
    "        \n",
    "        return intensity, riesz_vec, mask\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get a sample.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "            - 'input': Combined input (3, H, W) = [Intensity, Rx, Ry]\n",
    "            - 'intensity': (1, H, W)\n",
    "            - 'riesz': (2, H, W)\n",
    "            - 'mask': (H, W)\n",
    "            - 'energy': (1, H, W) computed from Riesz components\n",
    "        \"\"\"\n",
    "        # Load and preprocess\n",
    "        data = self._load_sample(self.samples[idx])\n",
    "        intensity, riesz_vec, mask = self._preprocess(data)\n",
    "        \n",
    "        # Resize\n",
    "        intensity, riesz_vec, mask = self._resize(intensity, riesz_vec, mask)\n",
    "        \n",
    "        # Augmentation (AFTER loading, with vector rotation)\n",
    "        if self.augment:\n",
    "            if self.rotation is not None:\n",
    "                intensity, riesz_vec, mask = self.rotation(intensity, riesz_vec, mask)\n",
    "            if self.flip is not None:\n",
    "                intensity, riesz_vec, mask = self.flip(intensity, riesz_vec, mask)\n",
    "        \n",
    "        # Compute energy from monogenic components\n",
    "        # E = sqrt(I^2 + Rx^2 + Ry^2)\n",
    "        energy = torch.sqrt(\n",
    "            intensity ** 2 + riesz_vec[0:1] ** 2 + riesz_vec[1:2] ** 2 + 1e-8\n",
    "        )\n",
    "        # Normalize energy to [0, 1]\n",
    "        energy = (energy - energy.min()) / (energy.max() - energy.min() + 1e-8)\n",
    "        \n",
    "        # Combine into 3-channel input\n",
    "        combined_input = torch.cat([intensity, riesz_vec], dim=0)  # (3, H, W)\n",
    "        \n",
    "        return {\n",
    "            'input': combined_input,\n",
    "            'intensity': intensity,\n",
    "            'riesz': riesz_vec,\n",
    "            'mask': mask,\n",
    "            'energy': energy\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MedicalImageSegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Base dataset class for medical image segmentation.\n",
    "    Can be extended for specific data formats (NIfTI, DICOM, PNG, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, images: np.ndarray, masks: np.ndarray,\n",
    "                 img_size: int = 256, normalize: bool = True,\n",
    "                 augment: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Args:\n",
    "            images: Array of shape (N, H, W) or (N, C, H, W)\n",
    "            masks: Array of shape (N, H, W) with class labels\n",
    "            img_size: Target image size for resizing\n",
    "            normalize: Whether to normalize images (0-1 or standardize)\n",
    "            augment: Whether to apply data augmentation\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.img_size = img_size\n",
    "        self.normalize = normalize\n",
    "        self.augment = augment\n",
    "        \n",
    "        assert len(images) == len(masks), \"Images and masks must have same length\"\n",
    "        \n",
    "        # Ensure 4D shape (N, C, H, W)\n",
    "        if self.images.ndim == 3:\n",
    "            self.images = np.expand_dims(self.images, axis=1)\n",
    "        \n",
    "        # Preprocess\n",
    "        self.images = torch.from_numpy(self.images).float()\n",
    "        self.masks = torch.from_numpy(self.masks).long()\n",
    "        \n",
    "        if self.normalize:\n",
    "            self._normalize_images()\n",
    "    \n",
    "    def _normalize_images(self):\n",
    "        \"\"\"Normalize images to 0-1 range or standardize.\"\"\"\n",
    "        # Normalize per image to 0-1\n",
    "        for i in range(len(self.images)):\n",
    "            img = self.images[i]\n",
    "            img_min = img.min()\n",
    "            img_max = img.max()\n",
    "            if img_max > img_min:\n",
    "                self.images[i] = (img - img_min) / (img_max - img_min)\n",
    "    \n",
    "    def _resize_if_needed(self, image: torch.Tensor, mask: torch.Tensor) -> Tuple:\n",
    "        \"\"\"Resize image and mask if needed.\"\"\"\n",
    "        if image.shape[-1] != self.img_size or image.shape[-2] != self.img_size:\n",
    "            image = F.interpolate(\n",
    "                image.unsqueeze(0), size=(self.img_size, self.img_size),\n",
    "                mode='bilinear', align_corners=True\n",
    "            ).squeeze(0)\n",
    "            mask = F.interpolate(\n",
    "                mask.float().unsqueeze(0).unsqueeze(0),\n",
    "                size=(self.img_size, self.img_size),\n",
    "                mode='nearest'\n",
    "            ).squeeze(0).squeeze(0).long()\n",
    "        return image, mask\n",
    "    \n",
    "    def _augment_data(self, image: torch.Tensor, mask: torch.Tensor) -> Tuple:\n",
    "        \"\"\"Apply basic data augmentation.\"\"\"\n",
    "        # Random horizontal flip\n",
    "        if torch.rand(1).item() > 0.5:\n",
    "            image = torch.flip(image, dims=[-1])\n",
    "            mask = torch.flip(mask, dims=[-1])\n",
    "        \n",
    "        # Random vertical flip\n",
    "        if torch.rand(1).item() > 0.5:\n",
    "            image = torch.flip(image, dims=[-2])\n",
    "            mask = torch.flip(mask, dims=[-2])\n",
    "        \n",
    "        # Random rotation (0, 90, 180, 270)\n",
    "        k = torch.randint(0, 4, (1,)).item()\n",
    "        image = torch.rot90(image, k=k, dims=[-2, -1])\n",
    "        mask = torch.rot90(mask, k=k, dims=[-2, -1])\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        \n",
    "        # Resize if needed\n",
    "        image, mask = self._resize_if_needed(image, mask)\n",
    "        \n",
    "        # Augmentation\n",
    "        if self.augment:\n",
    "            image, mask = self._augment_data(image, mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class MetricsCalculator:\n",
    "    \"\"\"Calculate segmentation metrics.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def dice_score(pred: torch.Tensor, target: torch.Tensor, \n",
    "                   smooth: float = 1e-5) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Dice coefficient.\n",
    "        \n",
    "        Args:\n",
    "            pred: Predictions of shape (B, C, H, W) or (B, H, W)\n",
    "            target: Ground truth of shape (B, H, W)\n",
    "            smooth: Smoothing constant\n",
    "            \n",
    "        Returns:\n",
    "            Dice score (0-1)\n",
    "        \"\"\"\n",
    "        if pred.ndim == 4:\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "        \n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        intersection = (pred == target).sum().float()\n",
    "        union = pred.numel()\n",
    "        \n",
    "        dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "        return dice.item()\n",
    "    \n",
    "    @staticmethod\n",
    "    def iou_score(pred: torch.Tensor, target: torch.Tensor,\n",
    "                  num_classes: int = 3, smooth: float = 1e-5) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate Intersection over Union (IoU) for each class.\n",
    "        \n",
    "        Args:\n",
    "            pred: Predictions of shape (B, C, H, W) or (B, H, W)\n",
    "            target: Ground truth of shape (B, H, W)\n",
    "            num_classes: Number of classes\n",
    "            smooth: Smoothing constant\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with per-class and mean IoU\n",
    "        \"\"\"\n",
    "        if pred.ndim == 4:\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "        \n",
    "        iou_scores = {}\n",
    "        mean_iou = 0.0\n",
    "        \n",
    "        for cls in range(num_classes):\n",
    "            pred_mask = (pred == cls)\n",
    "            target_mask = (target == cls)\n",
    "            \n",
    "            intersection = (pred_mask & target_mask).sum().float()\n",
    "            union = (pred_mask | target_mask).sum().float()\n",
    "            \n",
    "            iou = (intersection + smooth) / (union + smooth)\n",
    "            iou_scores[f\"class_{cls}\"] = iou.item()\n",
    "            mean_iou += iou.item()\n",
    "        \n",
    "        iou_scores[\"mean\"] = mean_iou / num_classes\n",
    "        return iou_scores\n",
    "    \n",
    "    @staticmethod\n",
    "    def hausdorff_distance(pred: torch.Tensor, target: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Hausdorff distance (boundary metric).\n",
    "        \n",
    "        Args:\n",
    "            pred: Predictions of shape (B, H, W)\n",
    "            target: Ground truth of shape (B, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Hausdorff distance\n",
    "        \"\"\"\n",
    "        # Simple implementation using max of minimum distances\n",
    "        pred = pred.float().view(-1, 1)\n",
    "        target = target.float().view(-1, 1)\n",
    "        \n",
    "        # Distance from pred to target\n",
    "        dist_pt = torch.cdist(pred, target).min(dim=1)[0]\n",
    "        max_dist_pt = dist_pt.max()\n",
    "        \n",
    "        # Distance from target to pred\n",
    "        dist_tp = torch.cdist(target, pred).min(dim=1)[0]\n",
    "        max_dist_tp = dist_tp.max()\n",
    "        \n",
    "        hd = max(max_dist_pt.item(), max_dist_tp.item())\n",
    "        return hd\n",
    "    \n",
    "    @staticmethod\n",
    "    def sensitivity_specificity(pred: torch.Tensor, target: torch.Tensor,\n",
    "                               num_classes: int = 2) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate sensitivity and specificity for binary/multi-class.\n",
    "        \n",
    "        Args:\n",
    "            pred: Predictions of shape (B, C, H, W) or (B, H, W)\n",
    "            target: Ground truth of shape (B, H, W)\n",
    "            num_classes: Number of classes\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with sensitivity and specificity per class\n",
    "        \"\"\"\n",
    "        if pred.ndim == 4:\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        for cls in range(1, num_classes):  # Skip background class 0\n",
    "            pred_pos = (pred == cls)\n",
    "            pred_neg = (pred != cls)\n",
    "            target_pos = (target == cls)\n",
    "            target_neg = (target != cls)\n",
    "            \n",
    "            tp = (pred_pos & target_pos).sum().float().item()\n",
    "            tn = (pred_neg & target_neg).sum().float().item()\n",
    "            fp = (pred_pos & target_neg).sum().float().item()\n",
    "            fn = (pred_neg & target_pos).sum().float().item()\n",
    "            \n",
    "            sensitivity = tp / (tp + fn + 1e-5)\n",
    "            specificity = tn / (tn + fp + 1e-5)\n",
    "            \n",
    "            metrics[f\"class_{cls}\"] = {\n",
    "                \"sensitivity\": sensitivity,\n",
    "                \"specificity\": specificity\n",
    "            }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c4fe4",
   "metadata": {},
   "source": [
    "### spectral_layers.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67181770",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Spectral Layers Module\n",
    "Implements frequency domain processing using FFT-based gating.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class SpectralGating(nn.Module):\n",
    "    \"\"\"\n",
    "    Spectral Gating Module using FFT for frequency domain filtering.\n",
    "    \n",
    "    Applies learnable frequency filtering in the frequency domain to enhance\n",
    "    edge sharpness and remove noise by modulating frequency components.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels: int, height: int, width: int, \n",
    "                 threshold: float = 0.1, complex_init: str = \"kaiming\"):\n",
    "        \"\"\"\n",
    "        Initialize SpectralGating module.\n",
    "        \n",
    "        Args:\n",
    "            channels: Number of input channels\n",
    "            height: Input height (should be divisible by some factor)\n",
    "            width: Input width (should be divisible by some factor)\n",
    "            threshold: Hard thresholding value for amplitude (0 to disable)\n",
    "            complex_init: Initialization strategy (\"kaiming\" or \"identity\")\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # Create learnable complex weights for frequency domain\n",
    "        # Shape: (channels, height, width//2 + 1) for rfft2\n",
    "        self.register_buffer(\n",
    "            \"freq_shape\",\n",
    "            torch.tensor([channels, height, width // 2 + 1], dtype=torch.long)\n",
    "        )\n",
    "        \n",
    "        # Real and Imaginary parts of complex weights\n",
    "        self.weight_real = nn.Parameter(\n",
    "            torch.zeros(channels, height, width // 2 + 1)\n",
    "        )\n",
    "        self.weight_imag = nn.Parameter(\n",
    "            torch.zeros(channels, height, width // 2 + 1)\n",
    "        )\n",
    "        \n",
    "        self._init_weights(complex_init)\n",
    "        \n",
    "    def _init_weights(self, strategy: str = \"kaiming\"):\n",
    "        \"\"\"Initialize complex weights.\"\"\"\n",
    "        if strategy == \"identity\":\n",
    "            # Initialize close to identity (magnitude ~1, phase ~0)\n",
    "            nn.init.ones_(self.weight_real)\n",
    "            nn.init.zeros_(self.weight_imag)\n",
    "        elif strategy == \"kaiming\":\n",
    "            # Kaiming initialization adapted for complex numbers\n",
    "            fan_in = self.height * (self.width // 2 + 1)\n",
    "            std = (2.0 / fan_in) ** 0.5\n",
    "            nn.init.normal_(self.weight_real, 0, std)\n",
    "            nn.init.normal_(self.weight_imag, 0, std)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown init strategy: {strategy}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply spectral gating to input.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (B, C, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Filtered output tensor of shape (B, C, H, W)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Apply FFT to convert to frequency domain\n",
    "        # rfft2 returns complex tensor\n",
    "        x_freq = torch.fft.rfft2(x, dim=(-2, -1), norm=\"ortho\")\n",
    "        \n",
    "        # Create complex weight matrix: weight_real + 1j * weight_imag\n",
    "        # Reshape to (1, C, H, W//2+1) for broadcasting\n",
    "        complex_weight = (\n",
    "            self.weight_real.unsqueeze(0) + \n",
    "            1j * self.weight_imag.unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        # Apply channel-wise multiplication in frequency domain\n",
    "        # Shape: (B, C, H, W//2+1) * (1, C, H, W//2+1) -> (B, C, H, W//2+1)\n",
    "        x_filtered = x_freq * complex_weight\n",
    "        \n",
    "        # Optional: Hard thresholding to remove low-amplitude noise\n",
    "        if self.threshold > 0:\n",
    "            magnitude = torch.abs(x_filtered)\n",
    "            mask = magnitude > self.threshold\n",
    "            x_filtered = x_filtered * mask.float()\n",
    "        \n",
    "        # Apply inverse FFT to return to spatial domain\n",
    "        output = torch.fft.irfft2(x_filtered, s=(H, W), dim=(-2, -1), norm=\"ortho\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class FrequencyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Frequency domain loss for enforcing edge sharpness.\n",
    "    Computes L2 distance between FFT of prediction and ground truth.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, weight: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize FrequencyLoss.\n",
    "        \n",
    "        Args:\n",
    "            weight: Weight factor for frequency loss in combined loss\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute frequency domain loss.\n",
    "        \n",
    "        Args:\n",
    "            pred: Prediction tensor of shape (B, C, H, W)\n",
    "            target: Ground truth tensor of shape (B, C, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Scalar loss value\n",
    "        \"\"\"\n",
    "        # Apply FFT\n",
    "        pred_freq = torch.fft.rfft2(pred, dim=(-2, -1), norm=\"ortho\")\n",
    "        target_freq = torch.fft.rfft2(target, dim=(-2, -1), norm=\"ortho\")\n",
    "        \n",
    "        # Compute L2 distance in frequency domain\n",
    "        # Using both magnitude and phase information\n",
    "        loss_real = F.mse_loss(pred_freq.real, target_freq.real)\n",
    "        loss_imag = F.mse_loss(pred_freq.imag, target_freq.imag)\n",
    "        \n",
    "        return loss_real + loss_imag\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01471f6",
   "metadata": {},
   "source": [
    "### mamba_block.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10914e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mamba-like Block Module (Lightweight VSS Block Implementation).\n",
    "\n",
    "Pure PyTorch implementation of Visual State Space (VSS) blocks for 2D image\n",
    "processing without external SSM libraries. Uses GRU-based sequential scanning\n",
    "to simulate selective state space dynamics.\n",
    "\n",
    "Architecture:\n",
    "    - DirectionalScanner: Multi-directional sequential scanning (4 directions)\n",
    "    - VSSBlock: Core Mamba-like component with residual connections\n",
    "    - MambaBlockStack: Hierarchical stacking of VSS blocks\n",
    "\n",
    "References:\n",
    "    [1] Gu & Dao, \"Mamba: Linear-Time Sequence Modeling with Selective\n",
    "        State Spaces,\" arXiv, 2023.\n",
    "    [2] Liu et al., \"VMamba: Visual State Space Model,\" arXiv, 2024.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "\n",
    "class DepthwiseSeparableConv2d(nn.Module):\n",
    "    \"\"\"Depthwise separable convolution block.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2, groups=in_channels, bias=False\n",
    "        )\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=True)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DirectionalScanner(nn.Module):\n",
    "    \"\"\"\n",
    "    Scans 2D feature maps in multiple directions to simulate 2D-SS2D.\n",
    "    Implements sequential scanning in 4 directions: right, down, left, up.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels: int, scan_dim: int = 64):\n",
    "        \"\"\"\n",
    "        Initialize DirectionalScanner.\n",
    "        \n",
    "        Args:\n",
    "            channels: Number of input channels\n",
    "            scan_dim: Hidden dimension for sequential processing\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.scan_dim = scan_dim\n",
    "        \n",
    "        # Learnable projection to scan_dim for each direction\n",
    "        self.proj_in = nn.Linear(channels, scan_dim)\n",
    "        \n",
    "        # GRU cell for sequential state processing (simulates SSM)\n",
    "        self.gru_cell = nn.GRUCell(scan_dim, scan_dim)\n",
    "        \n",
    "        # Project back to original channels\n",
    "        self.proj_out = nn.Linear(scan_dim, channels)\n",
    "        \n",
    "    def _scan_direction(self, x: torch.Tensor, direction: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Scan feature map in a specific direction.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (B, C, H, W)\n",
    "            direction: One of ['right', 'down', 'left', 'up']\n",
    "            \n",
    "        Returns:\n",
    "            Scanned tensor of shape (B, C, H, W)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Prepare sequence based on direction\n",
    "        if direction == \"right\":\n",
    "            # Scan left-to-right: (B, H*W, C) after reshape\n",
    "            x = x.permute(0, 2, 3, 1).reshape(B * H, W, C)  # (B*H, W, C)\n",
    "        elif direction == \"down\":\n",
    "            # Scan top-to-bottom\n",
    "            x = x.permute(0, 3, 2, 1).reshape(B * W, H, C)  # (B*W, H, C)\n",
    "        elif direction == \"left\":\n",
    "            # Scan right-to-left (reverse)\n",
    "            x = x.permute(0, 2, 3, 1).flip(1).reshape(B * H, W, C)  # (B*H, W, C)\n",
    "        elif direction == \"up\":\n",
    "            # Scan bottom-to-top (reverse)\n",
    "            x = x.permute(0, 3, 2, 1).flip(1).reshape(B * W, H, C)  # (B*W, H, C)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown direction: {direction}\")\n",
    "        \n",
    "        # Project to scan dimension\n",
    "        x = self.proj_in(x)  # (*, W/H, scan_dim)\n",
    "        \n",
    "        # Apply GRU cell sequentially (simulates SSM forward pass)\n",
    "        outputs = []\n",
    "        h = torch.zeros(x.shape[0], self.scan_dim, device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        for t in range(x.shape[1]):\n",
    "            h = self.gru_cell(x[:, t], h)  # GRU step\n",
    "            outputs.append(h)\n",
    "        \n",
    "        x = torch.stack(outputs, dim=1)  # (*, W/H, scan_dim)\n",
    "        \n",
    "        # Project back to original channels\n",
    "        x = self.proj_out(x)  # (*, W/H, C)\n",
    "        \n",
    "        # Reshape back to (B, C, H, W)\n",
    "        if direction == \"right\":\n",
    "            x = x.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        elif direction == \"down\":\n",
    "            x = x.reshape(B, W, H, C).permute(0, 3, 2, 1)\n",
    "        elif direction == \"left\":\n",
    "            x = x.reshape(B, H, W, C).permute(0, 3, 1, 2).flip(-1)\n",
    "        elif direction == \"up\":\n",
    "            x = x.reshape(B, W, H, C).permute(0, 3, 2, 1).flip(-2)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply directional scanning in 4 directions and aggregate.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (B, C, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Aggregated output tensor of shape (B, C, H, W)\n",
    "        \"\"\"\n",
    "        # Scan in all 4 directions\n",
    "        scan_right = self._scan_direction(x, \"right\")\n",
    "        scan_down = self._scan_direction(x, \"down\")\n",
    "        scan_left = self._scan_direction(x, \"left\")\n",
    "        scan_up = self._scan_direction(x, \"up\")\n",
    "        \n",
    "        # Aggregate by averaging\n",
    "        output = (scan_right + scan_down + scan_left + scan_up) / 4.0\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class VSSBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Visual State Space Block - Core Mamba-like component.\n",
    "    Combines convolutional preprocessing with multi-directional scanning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels: int, hidden_dim: Optional[int] = None, \n",
    "                 scan_dim: int = 64, expansion_ratio: float = 2.0):\n",
    "        \"\"\"\n",
    "        Initialize VSSBlock.\n",
    "        \n",
    "        Args:\n",
    "            channels: Number of input channels\n",
    "            hidden_dim: Hidden dimension (default: channels * expansion_ratio)\n",
    "            scan_dim: Hidden dimension for directional scanner\n",
    "            expansion_ratio: Channel expansion ratio for internal processing\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        hidden_dim = hidden_dim or int(channels * expansion_ratio)\n",
    "        \n",
    "        # Preprocessing: expand channels\n",
    "        self.norm1 = nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6)\n",
    "        self.conv_expand = nn.Conv2d(channels, hidden_dim, kernel_size=1, bias=True)\n",
    "        \n",
    "        # Directional scanning\n",
    "        self.scanner = DirectionalScanner(hidden_dim, scan_dim=scan_dim)\n",
    "        \n",
    "        # Postprocessing: contract channels back\n",
    "        self.norm2 = nn.GroupNorm(num_groups=32, num_channels=hidden_dim, eps=1e-6)\n",
    "        self.conv_contract = nn.Conv2d(hidden_dim, channels, kernel_size=1, bias=True)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with residual connection.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (B, C, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (B, C, H, W) with residual\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        \n",
    "        # Preprocessing\n",
    "        x = self.norm1(x)\n",
    "        x = self.conv_expand(x)\n",
    "        x = F.gelu(x)\n",
    "        \n",
    "        # Directional scanning (core SSM-like operation)\n",
    "        x = self.scanner(x)\n",
    "        \n",
    "        # Postprocessing\n",
    "        x = self.norm2(x)\n",
    "        x = self.conv_contract(x)\n",
    "        \n",
    "        # Residual connection\n",
    "        output = x + residual\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class MambaBlockStack(nn.Module):\n",
    "    \"\"\"Stack of multiple VSS blocks for hierarchical processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, channels: int, depth: int = 2, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize stack of VSS blocks.\n",
    "        \n",
    "        Args:\n",
    "            channels: Number of channels\n",
    "            depth: Number of VSS blocks to stack\n",
    "            **kwargs: Additional arguments passed to VSSBlock\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            VSSBlock(channels, **kwargs) for _ in range(depth)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44d086",
   "metadata": {},
   "source": [
    "### constellation_head.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RBF Constellation Head for Semantic Segmentation.\n",
    "\n",
    "Implements a Gaussian/RBF classifier inspired by digital modulation (PSK/QAM),\n",
    "projecting features to a 2D I/Q space and measuring distance to fixed prototypes.\n",
    "\n",
    "Key Concepts:\n",
    "    1. Project features to 2D embedding space (I/Q plane)\n",
    "    2. Fix class prototypes at constellation points (PSK/QPSK/8-PSK)\n",
    "    3. Classify based on RBF kernel: P(y|x) âˆ exp(-Î³||z - c||Â²)\n",
    "    4. Output logits = -Î³ Ã— squared_distance (for CrossEntropyLoss)\n",
    "\n",
    "Advantages over standard Conv1x1 classifier:\n",
    "    - Maximizes noise margin (decision boundaries equidistant from prototypes)\n",
    "    - Geometric interpretation of class relationships\n",
    "    - Robust to gradient explosion (bounded embedding space via Tanh)\n",
    "\n",
    "References:\n",
    "    [1] Proakis, \"Digital Communications\" - PSK/QAM constellation theory\n",
    "    [2] RBF Networks - Gaussian kernel classifiers\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "\n",
    "class RBFConstellationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    RBF Constellation Head with Gaussian Classifier.\n",
    "    \n",
    "    Projects feature map to 2D latent space (I/Q plane) and computes\n",
    "    class probabilities based on distance to fixed constellation prototypes.\n",
    "    \n",
    "    Architecture:\n",
    "        1. Projector: Conv 3x3 â†’ SiLU â†’ Conv 1x1 â†’ Tanh (â†’ [-1, 1])\n",
    "        2. Prototypes: Fixed PSK constellation points (non-trainable)\n",
    "        3. RBF Kernel: exp(-Î³||z - c||Â²)\n",
    "        4. Logits: -Î³ Ã— ||z - c||Â² (for CrossEntropyLoss)\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Number of input feature channels\n",
    "        num_classes: Number of output classes (determines constellation type)\n",
    "        embedding_dim: Dimension of I/Q embedding (default 2 for 2D plane)\n",
    "        init_gamma: Initial temperature parameter (Î³ = 1/2ÏƒÂ²)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, num_classes: int = 4,\n",
    "                 embedding_dim: int = 2, init_gamma: float = 1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Feature projector: maps to 2D I/Q space\n",
    "        mid_channels = in_channels // 2\n",
    "        # Ensure num_groups divides num_channels (using GCD with 32)\n",
    "        num_groups = math.gcd(mid_channels, 32)\n",
    "        \n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(num_groups, mid_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels // 2, embedding_dim, kernel_size=1, bias=True),\n",
    "            nn.Tanh()  # Constrain to [-1, 1] for stable training\n",
    "        )\n",
    "        \n",
    "        # Fixed constellation prototypes (non-trainable)\n",
    "        prototypes = self._generate_constellation_points(num_classes, embedding_dim)\n",
    "        self.register_buffer('prototypes', prototypes)\n",
    "        \n",
    "        # Learnable temperature (Î³ = 1 / 2ÏƒÂ²)\n",
    "        # Higher Î³ = narrower Gaussian = harder boundaries\n",
    "        self.gamma = nn.Parameter(torch.tensor(init_gamma))\n",
    "    \n",
    "    def _generate_constellation_points(self, num_classes: int, \n",
    "                                        embedding_dim: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate constellation points based on number of classes.\n",
    "        \n",
    "        Uses M-PSK (Phase Shift Keying) for 2D:\n",
    "            - 2 classes: BPSK (Â±1)\n",
    "            - 4 classes: QPSK (Â±1, Â±j)\n",
    "            - 8 classes: 8-PSK (unit circle, 45Â° spacing)\n",
    "            - Other: uniform on unit circle\n",
    "        \n",
    "        Args:\n",
    "            num_classes: Number of constellation points\n",
    "            embedding_dim: Embedding dimension (2 for I/Q)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape (embedding_dim, num_classes)\n",
    "        \"\"\"\n",
    "        if embedding_dim != 2:\n",
    "            # For higher dimensions, use random unit vectors\n",
    "            points = torch.randn(embedding_dim, num_classes)\n",
    "            points = F.normalize(points, dim=0)\n",
    "            return points\n",
    "        \n",
    "        # Generate M-PSK constellation (2D)\n",
    "        angles = torch.linspace(0, 2 * math.pi, num_classes + 1)[:-1]\n",
    "        \n",
    "        # Unit circle points\n",
    "        points = torch.stack([\n",
    "            torch.cos(angles),  # I component\n",
    "            torch.sin(angles)   # Q component\n",
    "        ], dim=0)  # (2, num_classes)\n",
    "        \n",
    "        return points\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Feature map (B, C, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of:\n",
    "            - logits: Class logits (B, num_classes, H, W)\n",
    "            - embeddings: 2D embeddings (B, 2, H, W)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Project to 2D embedding space\n",
    "        z = self.projector(x)  # (B, 2, H, W)\n",
    "        \n",
    "        # Reshape for distance computation\n",
    "        # z: (B, 2, H, W) â†’ (B, H, W, 2, 1)\n",
    "        z_flat = z.permute(0, 2, 3, 1).unsqueeze(-1)  # (B, H, W, 2, 1)\n",
    "        \n",
    "        # prototypes: (2, num_classes) â†’ (1, 1, 1, 2, num_classes)\n",
    "        proto = self.prototypes.view(1, 1, 1, self.embedding_dim, self.num_classes)\n",
    "        \n",
    "        # Compute squared Euclidean distance: ||z - c||Â²\n",
    "        # (B, H, W, 2, 1) - (1, 1, 1, 2, M) â†’ (B, H, W, 2, M)\n",
    "        diff = z_flat - proto\n",
    "        squared_dist = torch.sum(diff ** 2, dim=3)  # (B, H, W, num_classes)\n",
    "        \n",
    "        # RBF/Gaussian logits: -Î³ Ã— ||z - c||Â²\n",
    "        # Higher Î³ = stricter classification\n",
    "        gamma = F.softplus(self.gamma)  # Ensure positive\n",
    "        logits = -gamma * squared_dist  # (B, H, W, num_classes)\n",
    "        \n",
    "        # Rearrange to (B, num_classes, H, W)\n",
    "        logits = logits.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return logits, z\n",
    "    \n",
    "    def get_noise_margin(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute minimum distance between constellation points (noise margin).\n",
    "        \n",
    "        Returns:\n",
    "            Minimum Euclidean distance between any two prototypes\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # prototypes: (2, M)\n",
    "            proto = self.prototypes.T  # (M, 2)\n",
    "            \n",
    "            # Pairwise distances\n",
    "            dist_matrix = torch.cdist(proto, proto)\n",
    "            \n",
    "            # Mask diagonal\n",
    "            mask = torch.eye(self.num_classes, device=proto.device).bool()\n",
    "            dist_matrix = dist_matrix.masked_fill(mask, float('inf'))\n",
    "            \n",
    "            min_dist = dist_matrix.min().item()\n",
    "            \n",
    "        return min_dist\n",
    "\n",
    "\n",
    "class ConstellationVisualization:\n",
    "    \"\"\"Helper class for visualizing constellation embeddings.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_constellation(embeddings: torch.Tensor, labels: torch.Tensor,\n",
    "                          prototypes: torch.Tensor, save_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Plot 2D constellation with class prototypes.\n",
    "        \n",
    "        Args:\n",
    "            embeddings: Tensor of shape (N, 2) with embedded points\n",
    "            labels: Tensor of shape (N,) with class labels\n",
    "            prototypes: Tensor of shape (2, num_classes) with prototypes\n",
    "            save_path: Optional path to save figure\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "        except ImportError:\n",
    "            print(\"matplotlib not available for visualization\")\n",
    "            return\n",
    "        \n",
    "        embeddings = embeddings.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        prototypes = prototypes.detach().cpu().numpy().T  # (num_classes, 2)\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        \n",
    "        # Plot embeddings colored by class\n",
    "        num_classes = prototypes.shape[0]\n",
    "        colors = plt.cm.tab10(range(num_classes))\n",
    "        \n",
    "        for cls in range(num_classes):\n",
    "            mask = labels == cls\n",
    "            if mask.sum() > 0:\n",
    "                ax.scatter(embeddings[mask, 0], embeddings[mask, 1],\n",
    "                          c=[colors[cls]], alpha=0.5, s=10, label=f'Class {cls}')\n",
    "        \n",
    "        # Plot prototypes as larger markers\n",
    "        for cls in range(num_classes):\n",
    "            ax.scatter(prototypes[cls, 0], prototypes[cls, 1],\n",
    "                      c=[colors[cls]], s=200, marker='*', edgecolors='black',\n",
    "                      linewidth=2)\n",
    "        \n",
    "        # Draw unit circle (constellation reference)\n",
    "        theta = torch.linspace(0, 2 * math.pi, 100)\n",
    "        ax.plot(torch.cos(theta).numpy(), torch.sin(theta).numpy(),\n",
    "               'k--', alpha=0.3, label='Unit Circle')\n",
    "        \n",
    "        ax.set_xlim(-1.5, 1.5)\n",
    "        ax.set_ylim(-1.5, 1.5)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlabel('I (In-Phase)')\n",
    "        ax.set_ylabel('Q (Quadrature)')\n",
    "        ax.set_title('Constellation Diagram')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Testing\n",
    "# =============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299664ee",
   "metadata": {},
   "source": [
    "### gabor_implicit.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gabor Implicit Neural Representation Module\n",
    "\n",
    "Replaces standard Fourier Features with Gabor Basis for implicit neural representations.\n",
    "\n",
    "Key insight: \n",
    "- Fourier (sin/cos): Oscillates infinitely â†’ causes Gibbs ringing artifacts\n",
    "- Gabor (Gaussian Ã— sin): Localized oscillation â†’ clean boundaries without ringing\n",
    "\n",
    "This implements the \"Fine Branch\" of the EGM-Net architecture.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "\n",
    "class GaborBasis(nn.Module):\n",
    "    \"\"\"\n",
    "    Gabor Basis Functions for coordinate encoding.\n",
    "    \n",
    "    Gabor function: g(x) = exp(-xÂ²/2ÏƒÂ²) Ã— cos(2Ï€fx + Ï†)\n",
    "    \n",
    "    Unlike Fourier features which oscillate infinitely, Gabor wavelets\n",
    "    are spatially localized, preventing Gibbs phenomenon (ringing artifacts).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 2, num_frequencies: int = 64,\n",
    "                 sigma_range: Tuple[float, float] = (0.1, 2.0),\n",
    "                 freq_range: Tuple[float, float] = (1.0, 10.0),\n",
    "                 learnable: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize Gabor Basis.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension of input coordinates (2 for images)\n",
    "            num_frequencies: Number of Gabor basis functions\n",
    "            sigma_range: Range of Gaussian envelope widths\n",
    "            freq_range: Range of oscillation frequencies\n",
    "            learnable: Whether parameters are learnable\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.output_dim = num_frequencies * 2  # sin and cos components\n",
    "        \n",
    "        # Initialize frequencies uniformly in log space\n",
    "        log_freqs = torch.linspace(\n",
    "            math.log(freq_range[0]), \n",
    "            math.log(freq_range[1]), \n",
    "            num_frequencies\n",
    "        )\n",
    "        freqs = torch.exp(log_freqs)\n",
    "        \n",
    "        # Initialize sigmas (Gaussian envelope widths)\n",
    "        sigmas = torch.linspace(sigma_range[0], sigma_range[1], num_frequencies)\n",
    "        \n",
    "        # Random orientations for 2D\n",
    "        orientations = torch.rand(num_frequencies) * 2 * math.pi\n",
    "        \n",
    "        # Random phases\n",
    "        phases = torch.rand(num_frequencies) * 2 * math.pi\n",
    "        \n",
    "        # Create direction vectors from orientations\n",
    "        directions = torch.stack([\n",
    "            torch.cos(orientations),\n",
    "            torch.sin(orientations)\n",
    "        ], dim=-1)  # (num_freq, 2)\n",
    "        \n",
    "        if learnable:\n",
    "            self.freqs = nn.Parameter(freqs)\n",
    "            self.sigmas = nn.Parameter(sigmas)\n",
    "            self.directions = nn.Parameter(directions)\n",
    "            self.phases = nn.Parameter(phases)\n",
    "        else:\n",
    "            self.register_buffer('freqs', freqs)\n",
    "            self.register_buffer('sigmas', sigmas)\n",
    "            self.register_buffer('directions', directions)\n",
    "            self.register_buffer('phases', phases)\n",
    "    \n",
    "    def forward(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode coordinates using Gabor basis.\n",
    "        \n",
    "        Args:\n",
    "            coords: Coordinate tensor of shape (..., input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Gabor features of shape (..., output_dim)\n",
    "        \"\"\"\n",
    "        # Normalize directions\n",
    "        directions = F.normalize(self.directions, dim=-1)  # (num_freq, 2)\n",
    "        \n",
    "        # Project coordinates onto directions\n",
    "        # coords: (..., 2), directions: (num_freq, 2)\n",
    "        proj = torch.matmul(coords, directions.T)  # (..., num_freq)\n",
    "        \n",
    "        # Compute Gaussian envelope\n",
    "        # exp(-projÂ² / (2ÏƒÂ²))\n",
    "        sigmas = torch.abs(self.sigmas) + 0.01  # Ensure positive\n",
    "        gaussian = torch.exp(-proj**2 / (2 * sigmas**2 + 1e-8))\n",
    "        \n",
    "        # Compute oscillatory component\n",
    "        # cos(2Ï€fÂ·proj + Ï†), sin(2Ï€fÂ·proj + Ï†)\n",
    "        freqs = torch.abs(self.freqs) + 0.1  # Ensure positive\n",
    "        arg = 2 * math.pi * freqs * proj + self.phases\n",
    "        \n",
    "        cos_comp = gaussian * torch.cos(arg)\n",
    "        sin_comp = gaussian * torch.sin(arg)\n",
    "        \n",
    "        # Concatenate sin and cos\n",
    "        gabor_features = torch.cat([cos_comp, sin_comp], dim=-1)\n",
    "        \n",
    "        return gabor_features\n",
    "\n",
    "\n",
    "class FourierFeatures(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Fourier Features for comparison.\n",
    "    \n",
    "    From \"Fourier Features Let Networks Learn High Frequency Functions\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 2, num_frequencies: int = 64,\n",
    "                 scale: float = 10.0, learnable: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize Fourier Features.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension of input coordinates\n",
    "            num_frequencies: Number of frequency bands\n",
    "            scale: Standard deviation for frequency sampling\n",
    "            learnable: Whether B matrix is learnable\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.output_dim = num_frequencies * 2\n",
    "        \n",
    "        # Random frequency matrix\n",
    "        B = torch.randn(input_dim, num_frequencies) * scale\n",
    "        \n",
    "        if learnable:\n",
    "            self.B = nn.Parameter(B)\n",
    "        else:\n",
    "            self.register_buffer('B', B)\n",
    "    \n",
    "    def forward(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode coordinates using Fourier features.\n",
    "        \n",
    "        Args:\n",
    "            coords: Coordinate tensor of shape (..., input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Fourier features of shape (..., output_dim)\n",
    "        \"\"\"\n",
    "        # Project: coords @ B\n",
    "        proj = 2 * math.pi * torch.matmul(coords, self.B)  # (..., num_freq)\n",
    "        \n",
    "        # Sin and cos\n",
    "        return torch.cat([torch.cos(proj), torch.sin(proj)], dim=-1)\n",
    "\n",
    "\n",
    "class SIRENLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    SIREN (Sinusoidal Representation Networks) layer.\n",
    "    \n",
    "    Uses periodic sine activation instead of ReLU.\n",
    "    From \"Implicit Neural Representations with Periodic Activation Functions\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, \n",
    "                 omega_0: float = 30.0, is_first: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize SIREN layer.\n",
    "        \n",
    "        Args:\n",
    "            in_features: Number of input features\n",
    "            out_features: Number of output features\n",
    "            omega_0: Frequency multiplier (Ï‰â‚€ in paper)\n",
    "            is_first: Whether this is the first layer (uses different init)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.omega_0 = omega_0\n",
    "        self.is_first = is_first\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"SIREN-specific weight initialization.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            if self.is_first:\n",
    "                # First layer: uniform in [-1/n, 1/n]\n",
    "                self.linear.weight.uniform_(-1 / self.in_features, \n",
    "                                            1 / self.in_features)\n",
    "            else:\n",
    "                # Other layers: uniform in [-sqrt(6/n)/Ï‰â‚€, sqrt(6/n)/Ï‰â‚€]\n",
    "                bound = math.sqrt(6 / self.in_features) / self.omega_0\n",
    "                self.linear.weight.uniform_(-bound, bound)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sin(self.omega_0 * self.linear(x))\n",
    "\n",
    "\n",
    "class GaborNet(nn.Module):\n",
    "    \"\"\"\n",
    "    GaborNet: MLP with Gabor basis input encoding.\n",
    "    \n",
    "    Architecture: Gabor Encoding â†’ SIREN Layers â†’ Output\n",
    "    \n",
    "    This replaces standard MLP/KAN for implicit neural representations,\n",
    "    providing better stability and localized high-frequency learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, coord_dim: int = 2, feature_dim: int = 256,\n",
    "                 hidden_dim: int = 256, output_dim: int = 1,\n",
    "                 num_layers: int = 4, num_frequencies: int = 64,\n",
    "                 use_gabor: bool = True, omega_0: float = 30.0):\n",
    "        \"\"\"\n",
    "        Initialize GaborNet.\n",
    "        \n",
    "        Args:\n",
    "            coord_dim: Dimension of input coordinates (2 for images)\n",
    "            feature_dim: Dimension of input features from encoder\n",
    "            hidden_dim: Hidden layer dimension\n",
    "            output_dim: Output dimension (num_classes for segmentation)\n",
    "            num_layers: Number of SIREN layers\n",
    "            num_frequencies: Number of Gabor/Fourier frequencies\n",
    "            use_gabor: Use Gabor (True) or Fourier (False) features\n",
    "            omega_0: SIREN frequency parameter\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Coordinate encoding\n",
    "        if use_gabor:\n",
    "            self.coord_encoder = GaborBasis(\n",
    "                input_dim=coord_dim,\n",
    "                num_frequencies=num_frequencies,\n",
    "                learnable=True\n",
    "            )\n",
    "        else:\n",
    "            self.coord_encoder = FourierFeatures(\n",
    "                input_dim=coord_dim,\n",
    "                num_frequencies=num_frequencies,\n",
    "                learnable=False\n",
    "            )\n",
    "        \n",
    "        coord_encoded_dim = self.coord_encoder.output_dim\n",
    "        \n",
    "        # Input dimension: encoded coords + features\n",
    "        input_dim = coord_encoded_dim + feature_dim\n",
    "        \n",
    "        # Build SIREN network\n",
    "        layers = []\n",
    "        \n",
    "        # First layer\n",
    "        layers.append(SIRENLayer(input_dim, hidden_dim, omega_0, is_first=True))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(SIRENLayer(hidden_dim, hidden_dim, omega_0, is_first=False))\n",
    "        \n",
    "        # Final layer (linear, no sine activation)\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, coords: torch.Tensor, features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            coords: Normalized coordinates of shape (B, N, 2) in [-1, 1]\n",
    "            features: Features from encoder of shape (B, N, feature_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Output of shape (B, N, output_dim)\n",
    "        \"\"\"\n",
    "        # Encode coordinates\n",
    "        coord_encoded = self.coord_encoder(coords)  # (B, N, coord_encoded_dim)\n",
    "        \n",
    "        # Concatenate with features\n",
    "        x = torch.cat([coord_encoded, features], dim=-1)  # (B, N, input_dim)\n",
    "        \n",
    "        # Pass through network\n",
    "        output = self.network(x)  # (B, N, output_dim)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class ImplicitSegmentationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Implicit Segmentation Head for continuous boundary representation.\n",
    "    \n",
    "    Takes feature maps from encoder and outputs continuous segmentation\n",
    "    at arbitrary resolution using Gabor-based implicit representation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_channels: int = 64, num_classes: int = 2,\n",
    "                 hidden_dim: int = 256, num_layers: int = 4,\n",
    "                 num_frequencies: int = 64, use_gabor: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize Implicit Segmentation Head.\n",
    "        \n",
    "        Args:\n",
    "            feature_channels: Number of channels in input feature map\n",
    "            num_classes: Number of segmentation classes\n",
    "            hidden_dim: Hidden dimension of GaborNet\n",
    "            num_layers: Number of layers in GaborNet\n",
    "            num_frequencies: Number of Gabor frequencies\n",
    "            use_gabor: Use Gabor (True) or Fourier (False) basis\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_channels = feature_channels\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Feature projector (reduce channel dimension)\n",
    "        self.feature_proj = nn.Sequential(\n",
    "            nn.Conv2d(feature_channels, hidden_dim, kernel_size=1),\n",
    "            nn.GroupNorm(32, hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Implicit decoder\n",
    "        self.implicit_decoder = GaborNet(\n",
    "            coord_dim=2,\n",
    "            feature_dim=hidden_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=num_classes,\n",
    "            num_layers=num_layers,\n",
    "            num_frequencies=num_frequencies,\n",
    "            use_gabor=use_gabor\n",
    "        )\n",
    "    \n",
    "    def sample_features(self, feature_map: torch.Tensor, \n",
    "                       coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample features at given coordinates using bilinear interpolation.\n",
    "        \n",
    "        Args:\n",
    "            feature_map: Feature tensor of shape (B, C, H, W)\n",
    "            coords: Coordinates of shape (B, N, 2) in [-1, 1]\n",
    "            \n",
    "        Returns:\n",
    "            Sampled features of shape (B, N, C)\n",
    "        \"\"\"\n",
    "        B, C, H, W = feature_map.shape\n",
    "        N = coords.shape[1]\n",
    "        \n",
    "        # Reshape coords for grid_sample: (B, N, 1, 2) -> (B, 1, N, 2)\n",
    "        # grid_sample expects (B, H, W, 2) where last dim is (x, y)\n",
    "        grid = coords.view(B, 1, N, 2)\n",
    "        \n",
    "        # Sample using bilinear interpolation\n",
    "        # feature_map: (B, C, H, W), grid: (B, 1, N, 2)\n",
    "        # output: (B, C, 1, N)\n",
    "        sampled = F.grid_sample(\n",
    "            feature_map, grid,\n",
    "            mode='bilinear',\n",
    "            padding_mode='border',\n",
    "            align_corners=True\n",
    "        )\n",
    "        \n",
    "        # Reshape: (B, C, 1, N) -> (B, N, C)\n",
    "        sampled = sampled.squeeze(2).permute(0, 2, 1)\n",
    "        \n",
    "        return sampled\n",
    "    \n",
    "    def forward(self, feature_map: torch.Tensor, \n",
    "                coords: Optional[torch.Tensor] = None,\n",
    "                output_size: Optional[Tuple[int, int]] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            feature_map: Feature tensor of shape (B, C, H, W)\n",
    "            coords: Optional query coordinates of shape (B, N, 2) in [-1, 1]\n",
    "                   If None, generates grid based on output_size\n",
    "            output_size: Optional (H, W) for output resolution\n",
    "                        If None and coords is None, uses feature_map size * 4\n",
    "            \n",
    "        Returns:\n",
    "            Segmentation logits of shape (B, num_classes, H_out, W_out) or (B, N, num_classes)\n",
    "        \"\"\"\n",
    "        B, C, H_feat, W_feat = feature_map.shape\n",
    "        device = feature_map.device\n",
    "        \n",
    "        # Project features\n",
    "        feature_map = self.feature_proj(feature_map)  # (B, hidden_dim, H, W)\n",
    "        \n",
    "        # Generate coordinates if not provided\n",
    "        if coords is None:\n",
    "            if output_size is None:\n",
    "                output_size = (H_feat * 4, W_feat * 4)\n",
    "            \n",
    "            H_out, W_out = output_size\n",
    "            \n",
    "            # Create normalized coordinate grid [-1, 1]\n",
    "            y = torch.linspace(-1, 1, H_out, device=device)\n",
    "            x = torch.linspace(-1, 1, W_out, device=device)\n",
    "            yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "            coords = torch.stack([xx, yy], dim=-1)  # (H_out, W_out, 2)\n",
    "            coords = coords.view(1, -1, 2).expand(B, -1, -1)  # (B, H*W, 2)\n",
    "            \n",
    "            reshape_output = True\n",
    "        else:\n",
    "            reshape_output = False\n",
    "            H_out, W_out = None, None\n",
    "        \n",
    "        # Sample features at coordinates\n",
    "        features = self.sample_features(feature_map, coords)  # (B, N, hidden_dim)\n",
    "        \n",
    "        # Implicit decoding\n",
    "        logits = self.implicit_decoder(coords, features)  # (B, N, num_classes)\n",
    "        \n",
    "        # Reshape to image if using grid\n",
    "        if reshape_output:\n",
    "            logits = logits.view(B, H_out, W_out, self.num_classes)\n",
    "            logits = logits.permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class FiLMLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature-wise Linear Modulation (FiLM) Layer.\n",
    "    \n",
    "    Applies affine transformation conditioned on input features:\n",
    "        output = Î³(features) Ã— input + Î²(features)\n",
    "    \n",
    "    This allows the network to dynamically adjust activations based on context.\n",
    "    \n",
    "    Args:\n",
    "        feature_dim: Dimension of conditioning features\n",
    "        modulation_dim: Dimension of signal to modulate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim: int, modulation_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Predict scale (Î³) and shift (Î²) from features\n",
    "        self.gamma_proj = nn.Linear(feature_dim, modulation_dim)\n",
    "        self.beta_proj = nn.Linear(feature_dim, modulation_dim)\n",
    "        \n",
    "        # Initialize to identity transform\n",
    "        nn.init.ones_(self.gamma_proj.weight.data[:modulation_dim, :modulation_dim // feature_dim + 1])\n",
    "        nn.init.zeros_(self.gamma_proj.bias.data)\n",
    "        nn.init.zeros_(self.beta_proj.weight.data)\n",
    "        nn.init.zeros_(self.beta_proj.bias.data)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, condition: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply FiLM modulation.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor to modulate (..., modulation_dim)\n",
    "            condition: Conditioning features (..., feature_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Modulated tensor (..., modulation_dim)\n",
    "        \"\"\"\n",
    "        gamma = self.gamma_proj(condition)  # Scale\n",
    "        beta = self.beta_proj(condition)    # Shift\n",
    "        \n",
    "        return gamma * x + beta\n",
    "\n",
    "\n",
    "class EnergyGatedGaborImplicit(nn.Module):\n",
    "    \"\"\"\n",
    "    Energy-Gated Gabor Implicit Neural Representation for Fine Segmentation.\n",
    "    \n",
    "    Key features:\n",
    "    1. Gabor/Raised Cosine basis encoding (prevents Gibbs ringing)\n",
    "    2. FiLM conditioning (feature-dependent modulation)\n",
    "    3. Physics Gating (output Ã— energy_map to suppress flat regions)\n",
    "    \n",
    "    The energy gating ensures:\n",
    "    - High energy (boundaries): Full implicit output for sharp details\n",
    "    - Low energy (flat): Suppressed output to avoid artifacts\n",
    "    \n",
    "    Architecture:\n",
    "        coords â†’ Gabor Encoding â†’ FiLM(features) â†’ MLP â†’ Ã— Energy â†’ Output\n",
    "    \n",
    "    Args:\n",
    "        feature_dim: Dimension of input features\n",
    "        num_classes: Number of output classes\n",
    "        hidden_dim: Hidden layer dimension\n",
    "        num_layers: Number of MLP layers\n",
    "        num_frequencies: Number of Gabor basis functions\n",
    "        use_gabor: Use Gabor (True) or Fourier (False) basis\n",
    "        omega_0: SIREN frequency parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim: int, num_classes: int,\n",
    "                 hidden_dim: int = 256, num_layers: int = 4,\n",
    "                 num_frequencies: int = 64, use_gabor: bool = True,\n",
    "                 omega_0: float = 30.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Coordinate encoder (Gabor or Fourier)\n",
    "        if use_gabor:\n",
    "            self.coord_encoder = GaborBasis(\n",
    "                input_dim=2, num_frequencies=num_frequencies, learnable=True\n",
    "            )\n",
    "        else:\n",
    "            self.coord_encoder = FourierFeatures(\n",
    "                input_dim=2, num_frequencies=num_frequencies, learnable=False\n",
    "            )\n",
    "        \n",
    "        coord_encoded_dim = self.coord_encoder.output_dim\n",
    "        \n",
    "        # FiLM layers for feature conditioning\n",
    "        self.film_layers = nn.ModuleList([\n",
    "            FiLMLayer(feature_dim, hidden_dim)\n",
    "            for _ in range(num_layers - 1)\n",
    "        ])\n",
    "        \n",
    "        # MLP decoder with SIREN layers\n",
    "        self.input_proj = SIRENLayer(coord_encoded_dim, hidden_dim, omega_0, is_first=True)\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            SIRENLayer(hidden_dim, hidden_dim, omega_0, is_first=False)\n",
    "            for _ in range(num_layers - 2)\n",
    "        ])\n",
    "        \n",
    "        # Output layer (linear, no sine)\n",
    "        self.output_proj = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        # Energy gating parameters\n",
    "        self.gate_scale = nn.Parameter(torch.ones(1))\n",
    "        self.gate_bias = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, coords: torch.Tensor, features: torch.Tensor,\n",
    "                energy: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with FiLM conditioning and energy gating.\n",
    "        \n",
    "        Args:\n",
    "            coords: Query coordinates (B, N, 2) in [-1, 1]\n",
    "            features: Conditioning features (B, N, feature_dim)\n",
    "            energy: Energy values at query points (B, N, 1) in [0, 1]\n",
    "            \n",
    "        Returns:\n",
    "            Gated output logits (B, N, num_classes)\n",
    "        \"\"\"\n",
    "        # 1. Encode coordinates with Gabor basis\n",
    "        coord_enc = self.coord_encoder(coords)  # (B, N, coord_encoded_dim)\n",
    "        \n",
    "        # 2. Initial projection\n",
    "        x = self.input_proj(coord_enc)  # (B, N, hidden_dim)\n",
    "        \n",
    "        # 3. Process through FiLM-modulated layers\n",
    "        for i, (hidden_layer, film_layer) in enumerate(\n",
    "            zip(self.hidden_layers, self.film_layers[:-1])\n",
    "        ):\n",
    "            # Apply SIREN layer\n",
    "            x = hidden_layer(x)\n",
    "            # Apply FiLM modulation\n",
    "            x = film_layer(x, features)\n",
    "        \n",
    "        # Final FiLM modulation\n",
    "        if len(self.film_layers) > 0:\n",
    "            x = self.film_layers[-1](x, features)\n",
    "        \n",
    "        # 4. Output projection\n",
    "        logits = self.output_proj(x)  # (B, N, num_classes)\n",
    "        \n",
    "        # 5. Physics Gating\n",
    "        # Scale energy with learnable parameters\n",
    "        gate = torch.sigmoid(energy * self.gate_scale + self.gate_bias)  # (B, N, 1)\n",
    "        \n",
    "        # Apply gating\n",
    "        gated_logits = logits * gate  # (B, N, num_classes)\n",
    "        \n",
    "        return gated_logits\n",
    "\n",
    "\n",
    "class EnergyGatedImplicitHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Energy-Gated Implicit Segmentation Head.\n",
    "    \n",
    "    Wraps EnergyGatedGaborImplicit with feature sampling from encoder output.\n",
    "    \n",
    "    Args:\n",
    "        feature_channels: Number of channels in input feature map\n",
    "        num_classes: Number of output classes\n",
    "        hidden_dim: Hidden dimension\n",
    "        num_layers: Number of MLP layers\n",
    "        num_frequencies: Number of Gabor frequencies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_channels: int, num_classes: int,\n",
    "                 hidden_dim: int = 256, num_layers: int = 4,\n",
    "                 num_frequencies: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Feature projection\n",
    "        self.feature_proj = nn.Sequential(\n",
    "            nn.Conv2d(feature_channels, hidden_dim, kernel_size=1),\n",
    "            nn.GroupNorm(min(32, hidden_dim), hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Implicit decoder with energy gating\n",
    "        self.implicit_decoder = EnergyGatedGaborImplicit(\n",
    "            feature_dim=hidden_dim,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_frequencies=num_frequencies,\n",
    "            use_gabor=True\n",
    "        )\n",
    "    \n",
    "    def sample_at_coords(self, feature_map: torch.Tensor,\n",
    "                        energy_map: torch.Tensor,\n",
    "                        coords: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Sample features and energy at given coordinates.\n",
    "        \n",
    "        Args:\n",
    "            feature_map: (B, C, H, W)\n",
    "            energy_map: (B, 1, H, W)\n",
    "            coords: (B, N, 2) in [-1, 1]\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (features, energy) both of shape (B, N, C)\n",
    "        \"\"\"\n",
    "        B, C, H, W = feature_map.shape\n",
    "        N = coords.shape[1]\n",
    "        \n",
    "        grid = coords.view(B, 1, N, 2)\n",
    "        \n",
    "        # Sample features\n",
    "        features = F.grid_sample(\n",
    "            feature_map, grid, mode='bilinear',\n",
    "            padding_mode='border', align_corners=True\n",
    "        ).squeeze(2).permute(0, 2, 1)  # (B, N, C)\n",
    "        \n",
    "        # Sample energy\n",
    "        energy = F.grid_sample(\n",
    "            energy_map, grid, mode='bilinear',\n",
    "            padding_mode='border', align_corners=True\n",
    "        ).squeeze(2).permute(0, 2, 1)  # (B, N, 1)\n",
    "        \n",
    "        return features, energy\n",
    "    \n",
    "    def forward(self, feature_map: torch.Tensor,\n",
    "                energy_map: torch.Tensor,\n",
    "                coords: Optional[torch.Tensor] = None,\n",
    "                output_size: Optional[Tuple[int, int]] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            feature_map: Feature tensor (B, C, H, W)\n",
    "            energy_map: Energy map (B, 1, H, W)\n",
    "            coords: Optional query coordinates (B, N, 2)\n",
    "            output_size: Optional output size (H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Segmentation logits (B, num_classes, H, W) or (B, N, num_classes)\n",
    "        \"\"\"\n",
    "        B, C, H_feat, W_feat = feature_map.shape\n",
    "        device = feature_map.device\n",
    "        \n",
    "        # Project features\n",
    "        feature_map = self.feature_proj(feature_map)\n",
    "        \n",
    "        # Generate grid if no coords provided\n",
    "        if coords is None:\n",
    "            if output_size is None:\n",
    "                output_size = (H_feat * 4, W_feat * 4)\n",
    "            \n",
    "            H_out, W_out = output_size\n",
    "            \n",
    "            y = torch.linspace(-1, 1, H_out, device=device)\n",
    "            x = torch.linspace(-1, 1, W_out, device=device)\n",
    "            yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "            coords = torch.stack([xx, yy], dim=-1).view(1, -1, 2).expand(B, -1, -1)\n",
    "            \n",
    "            reshape_output = True\n",
    "        else:\n",
    "            reshape_output = False\n",
    "            H_out, W_out = None, None\n",
    "        \n",
    "        # Sample features and energy\n",
    "        features, energy = self.sample_at_coords(feature_map, energy_map, coords)\n",
    "        \n",
    "        # Implicit decoding with energy gating\n",
    "        logits = self.implicit_decoder(coords, features, energy)\n",
    "        \n",
    "        # Reshape if needed\n",
    "        if reshape_output:\n",
    "            logits = logits.view(B, H_out, W_out, self.num_classes)\n",
    "            logits = logits.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Testing Gabor Implicit Modules...\")\n",
    "    \n",
    "    # Test Gabor Basis\n",
    "    print(\"\\n[1] Testing GaborBasis...\")\n",
    "    gabor = GaborBasis(input_dim=2, num_frequencies=32)\n",
    "    coords = torch.randn(4, 100, 2)  # (B, N, 2)\n",
    "    encoded = gabor(coords)\n",
    "    print(f\"Input coords: {coords.shape}\")\n",
    "    print(f\"Gabor encoded: {encoded.shape}\")\n",
    "    \n",
    "    # Test GaborNet\n",
    "    print(\"\\n[2] Testing GaborNet...\")\n",
    "    net = GaborNet(coord_dim=2, feature_dim=64, hidden_dim=128, \n",
    "                   output_dim=3, num_layers=3, num_frequencies=32)\n",
    "    features = torch.randn(4, 100, 64)\n",
    "    output = net(coords, features)\n",
    "    print(f\"GaborNet output: {output.shape}\")\n",
    "    \n",
    "    # Test ImplicitSegmentationHead\n",
    "    print(\"\\n[3] Testing ImplicitSegmentationHead...\")\n",
    "    head = ImplicitSegmentationHead(\n",
    "        feature_channels=64, num_classes=3,\n",
    "        hidden_dim=128, num_layers=3, num_frequencies=32\n",
    "    )\n",
    "    feature_map = torch.randn(2, 64, 32, 32)\n",
    "    \n",
    "    # Test with automatic grid\n",
    "    seg_output = head(feature_map, output_size=(128, 128))\n",
    "    print(f\"Feature map: {feature_map.shape}\")\n",
    "    print(f\"Segmentation output (grid): {seg_output.shape}\")\n",
    "    \n",
    "    # Test with custom coordinates\n",
    "    custom_coords = torch.rand(2, 500, 2) * 2 - 1  # Random points in [-1, 1]\n",
    "    seg_points = head(feature_map, coords=custom_coords)\n",
    "    print(f\"Segmentation output (points): {seg_points.shape}\")\n",
    "    \n",
    "    print(\"\\nâœ“ All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae7a0be",
   "metadata": {},
   "source": [
    "### hrnet_mamba.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958276bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HRNetV2-Mamba Backbone for EGM-Net.\n",
    "\n",
    "A dual-stream architecture that maintains high-resolution features throughout,\n",
    "replacing downsampling/upsampling with parallel multi-resolution streams.\n",
    "\n",
    "Architecture Overview:\n",
    "    1. Two parallel streams: High-Res (H/4) and Low-Res (H/8 â†’ H/16)\n",
    "    2. SpectralVSSBlock: Combines Mamba (spatial) + FFT (spectral) processing\n",
    "    3. MultiScaleFusion: Periodic exchange of information between streams\n",
    "    4. Aggregation: Upsample all to highest resolution and concatenate\n",
    "\n",
    "Key Features:\n",
    "    - High-res stream preserves boundary details (Energy, Frequency signals)\n",
    "    - Low-res stream captures semantic context (Intensity)\n",
    "    - No information loss from deep downsampling\n",
    "    - Spectral Mamba blocks overcome spectral bias\n",
    "\n",
    "References:\n",
    "    [1] Wang et al., \"Deep High-Resolution Representation Learning,\" CVPR 2020.\n",
    "    [2] Liu et al., \"VMamba: Visual State Space Model,\" arXiv 2024.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "import math\n",
    "\n",
    "# Handle imports for both package and standalone usage\n",
    "try:\n",
    "except (ImportError, ValueError):\n",
    "    try:\n",
    "    except ImportError:\n",
    "        try:\n",
    "            import sys\n",
    "            import os\n",
    "            # Add parent directory of 'models' (i.e., 'src') to path\n",
    "            current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "            src_dir = os.path.dirname(current_dir)\n",
    "            if src_dir not in sys.path:\n",
    "                sys.path.append(src_dir)\n",
    "            \n",
    "        except ImportError as e:\n",
    "            raise ImportError(f\"Could not import required modules (mamba_block, spectral_layers): {e}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Spectral VSS Block: Mamba + FFT Dual-Path Processing\n",
    "# =============================================================================\n",
    "\n",
    "class SpectralVSSBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Spectral VSS Block combining spatial and frequency domain processing.\n",
    "    \n",
    "    Two parallel branches:\n",
    "        A) Spatial Branch: Mamba (SS2D) for global context with O(N) complexity\n",
    "        B) Spectral Branch: FFT â†’ Learnable Gating â†’ IFFT to overcome spectral bias\n",
    "    \n",
    "    The outputs are blended with a learnable weight.\n",
    "    \n",
    "    Args:\n",
    "        channels: Number of input/output channels\n",
    "        height: Feature map height (for spectral weights)\n",
    "        width: Feature map width (for spectral weights)\n",
    "        mamba_depth: Number of stacked Mamba blocks\n",
    "        expansion_ratio: Channel expansion in Mamba blocks\n",
    "        spectral_threshold: Threshold for spectral gating\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels: int, height: int, width: int,\n",
    "                 mamba_depth: int = 2, expansion_ratio: float = 2.0,\n",
    "                 spectral_threshold: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # Branch A: Spatial path (VSS/Mamba Blocks)\n",
    "        self.vss_blocks = MambaBlockStack(\n",
    "            channels, \n",
    "            depth=mamba_depth,\n",
    "            expansion_ratio=expansion_ratio,\n",
    "            scan_dim=min(64, channels)\n",
    "        )\n",
    "        \n",
    "        # Branch B: Spectral path (FFT-based filtering)\n",
    "        self.spectral_gate = SpectralGating(\n",
    "            channels, height, width,\n",
    "            threshold=spectral_threshold,\n",
    "            complex_init=\"kaiming\"\n",
    "        )\n",
    "        \n",
    "        # Learnable fusion weight (Ïƒ(w) blends spatial and spectral)\n",
    "        self.fusion_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        \n",
    "        # Optional: Layer norm for stability\n",
    "        self.norm = nn.GroupNorm(min(32, channels), channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with dual-path processing.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (B, C, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor (B, C, H, W)\n",
    "        \"\"\"\n",
    "        # Branch A: Spatial context via Mamba\n",
    "        spatial_out = self.vss_blocks(x)\n",
    "        \n",
    "        # Branch B: Frequency filtering via FFT\n",
    "        spectral_out = self.spectral_gate(x)\n",
    "        \n",
    "        # Learnable fusion\n",
    "        weight = torch.sigmoid(self.fusion_weight)\n",
    "        output = weight * spatial_out + (1 - weight) * spectral_out\n",
    "        \n",
    "        # Normalize\n",
    "        output = self.norm(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Multi-Scale Fusion Module\n",
    "# =============================================================================\n",
    "\n",
    "class MultiScaleFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-scale fusion between high-resolution and low-resolution streams.\n",
    "    \n",
    "    Enables bidirectional information flow:\n",
    "        - High â†’ Low: Strided Conv to downsample\n",
    "        - Low â†’ High: Bilinear Upsample + 1x1 Conv\n",
    "    \n",
    "    This allows:\n",
    "        - Low-res stream to know \"where\" (from high-res)\n",
    "        - High-res stream to know \"what\" (from low-res)\n",
    "    \n",
    "    Args:\n",
    "        high_channels: Channels in high-res stream\n",
    "        low_channels: Channels in low-res stream\n",
    "        scale_factor: Spatial scale ratio between streams (e.g., 2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, high_channels: int, low_channels: int, scale_factor: int = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "        # High â†’ Low: Downsample path\n",
    "        self.high_to_low = nn.Sequential(\n",
    "            nn.Conv2d(high_channels, low_channels, kernel_size=3, \n",
    "                     stride=scale_factor, padding=1, bias=False),\n",
    "            nn.GroupNorm(min(32, low_channels), low_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Low â†’ High: Upsample path\n",
    "        self.low_to_high = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(low_channels, high_channels, kernel_size=1, bias=False),\n",
    "            nn.GroupNorm(min(32, high_channels), high_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Fusion weights (learnable gating)\n",
    "        self.high_gate = nn.Parameter(torch.ones(1) * 0.5)\n",
    "        self.low_gate = nn.Parameter(torch.ones(1) * 0.5)\n",
    "    \n",
    "    def forward(self, high_feat: torch.Tensor, \n",
    "                low_feat: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Fuse features from both streams.\n",
    "        \n",
    "        Args:\n",
    "            high_feat: High-res features (B, C_h, H, W)\n",
    "            low_feat: Low-res features (B, C_l, H/s, W/s)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (updated_high, updated_low)\n",
    "        \"\"\"\n",
    "        # High â†’ Low contribution\n",
    "        high_to_low = self.high_to_low(high_feat)\n",
    "        \n",
    "        # Low â†’ High contribution\n",
    "        low_to_high = self.low_to_high(low_feat)\n",
    "        \n",
    "        # Gated fusion\n",
    "        h_gate = torch.sigmoid(self.high_gate)\n",
    "        l_gate = torch.sigmoid(self.low_gate)\n",
    "        \n",
    "        # Update streams\n",
    "        new_high = high_feat + h_gate * low_to_high\n",
    "        new_low = low_feat + l_gate * high_to_low\n",
    "        \n",
    "        return new_high, new_low\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# HRNet Stage: Single Resolution Processing Stage\n",
    "# =============================================================================\n",
    "\n",
    "class HRNetStage(nn.Module):\n",
    "    \"\"\"\n",
    "    Single stage of HRNet processing at one resolution.\n",
    "    \n",
    "    Contains multiple SpectralVSSBlocks for deep feature extraction.\n",
    "    \n",
    "    Args:\n",
    "        channels: Number of channels\n",
    "        height: Feature map height\n",
    "        width: Feature map width\n",
    "        num_blocks: Number of SpectralVSSBlocks\n",
    "        mamba_depth: Depth of Mamba blocks within each SpectralVSSBlock\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels: int, height: int, width: int,\n",
    "                 num_blocks: int = 2, mamba_depth: int = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            SpectralVSSBlock(channels, height, width, mamba_depth=mamba_depth)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Process through all blocks.\"\"\"\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Stem: Initial Feature Extraction\n",
    "# =============================================================================\n",
    "\n",
    "class HRNetStem(nn.Module):\n",
    "    \"\"\"\n",
    "    Initial feature extraction stem.\n",
    "    \n",
    "    Converts input image to initial feature representation with\n",
    "    strided convolutions for efficiency.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Number of input channels (3 for Intensity + Riesz)\n",
    "        out_channels: Output feature channels\n",
    "        stride: Total stride of stem (typically 4)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 3, out_channels: int = 64, stride: int = 4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Two-stage striding: 2x each\n",
    "        mid_channels = out_channels // 2\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, \n",
    "                              stride=2, padding=1, bias=False)\n",
    "        self.norm1 = nn.GroupNorm(min(32, mid_channels), mid_channels)\n",
    "        self.act1 = nn.GELU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=3,\n",
    "                              stride=2, padding=1, bias=False)\n",
    "        self.norm2 = nn.GroupNorm(min(32, out_channels), out_channels)\n",
    "        self.act2 = nn.GELU()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Extract initial features with stride-4 reduction.\"\"\"\n",
    "        x = self.act1(self.norm1(self.conv1(x)))\n",
    "        x = self.act2(self.norm2(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Aggregation Layer: Combine All Resolutions\n",
    "# =============================================================================\n",
    "\n",
    "class AggregationLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Aggregate multi-resolution features for output.\n",
    "    \n",
    "    Following HRNetV2: Upsample all resolutions to highest resolution\n",
    "    and concatenate along channel dimension.\n",
    "    \n",
    "    Args:\n",
    "        high_channels: Channels from high-res stream\n",
    "        low_channels: Channels from low-res stream\n",
    "        out_channels: Output channels after aggregation\n",
    "        scale_factor: Scale factor to upsample low-res\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, high_channels: int, low_channels: int, \n",
    "                 out_channels: int, scale_factor: int = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "        # Upsample low-res to match high-res\n",
    "        self.upsample_low = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(low_channels, low_channels, kernel_size=1, bias=False),\n",
    "            nn.GroupNorm(min(32, low_channels), low_channels)\n",
    "        )\n",
    "        \n",
    "        # Combine and project\n",
    "        total_channels = high_channels + low_channels\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(total_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.GroupNorm(min(32, out_channels), out_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, high_feat: torch.Tensor, \n",
    "                low_feat: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Aggregate features from both streams.\n",
    "        \n",
    "        Args:\n",
    "            high_feat: High-res features (B, C_h, H, W)\n",
    "            low_feat: Low-res features (B, C_l, H/s, W/s)\n",
    "            \n",
    "        Returns:\n",
    "            Aggregated features (B, out_channels, H, W)\n",
    "        \"\"\"\n",
    "        # Upsample low-res to match high-res\n",
    "        low_up = self.upsample_low(low_feat)\n",
    "        \n",
    "        # Concatenate\n",
    "        concat = torch.cat([high_feat, low_up], dim=1)\n",
    "        \n",
    "        # Project to output channels\n",
    "        output = self.projection(concat)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# HRNetV2-Mamba Backbone: Complete Dual-Stream Architecture\n",
    "# =============================================================================\n",
    "\n",
    "class HRNetV2MambaBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    HRNetV2 Backbone with Mamba (SS2D) and Spectral Gating.\n",
    "    \n",
    "    Dual-stream architecture maintaining high-resolution features throughout:\n",
    "    \n",
    "    Stream 1 (High-Res): H/4 resolution\n",
    "        - Receives: Energy and Frequency signals (physics features)\n",
    "        - Purpose: Preserve boundary details\n",
    "        \n",
    "    Stream 2 (Low-Res): H/8 resolution\n",
    "        - Receives: Intensity signal (semantic features)\n",
    "        - Purpose: Capture global context\n",
    "    \n",
    "    The streams are periodically fused via MultiScaleFusion modules.\n",
    "    Output is aggregated from all resolutions at the highest resolution.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Input channels (3 for Intensity + Rx + Ry)\n",
    "        base_channels: Base channel count\n",
    "        num_stages: Number of processing stages\n",
    "        blocks_per_stage: Number of SpectralVSSBlocks per stage\n",
    "        mamba_depth: Depth within each SpectralVSSBlock\n",
    "        img_size: Input image size (for spectral weight initialization)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 3, base_channels: int = 64,\n",
    "                 num_stages: int = 4, blocks_per_stage: int = 2,\n",
    "                 mamba_depth: int = 2, img_size: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.base_channels = base_channels\n",
    "        self.num_stages = num_stages\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Initial stem: stride 4 reduction\n",
    "        self.stem = HRNetStem(in_channels, base_channels, stride=4)\n",
    "        \n",
    "        # Initial feature sizes\n",
    "        high_res_size = img_size // 4  # H/4\n",
    "        low_res_size = img_size // 8   # H/8\n",
    "        \n",
    "        high_channels = base_channels\n",
    "        low_channels = base_channels * 2\n",
    "        \n",
    "        # Create initial low-res stream from high-res\n",
    "        self.create_low_stream = nn.Sequential(\n",
    "            nn.Conv2d(high_channels, low_channels, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.GroupNorm(min(32, low_channels), low_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # High-resolution stream stages\n",
    "        self.high_res_stages = nn.ModuleList()\n",
    "        \n",
    "        # Low-resolution stream stages\n",
    "        self.low_res_stages = nn.ModuleList()\n",
    "        \n",
    "        # Fusion modules (after each stage)\n",
    "        self.fusion_modules = nn.ModuleList()\n",
    "        \n",
    "        # Build stages\n",
    "        for stage_idx in range(num_stages):\n",
    "            # High-res stage\n",
    "            self.high_res_stages.append(\n",
    "                HRNetStage(\n",
    "                    channels=high_channels,\n",
    "                    height=high_res_size,\n",
    "                    width=high_res_size,\n",
    "                    num_blocks=blocks_per_stage,\n",
    "                    mamba_depth=mamba_depth\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Low-res stage\n",
    "            self.low_res_stages.append(\n",
    "                HRNetStage(\n",
    "                    channels=low_channels,\n",
    "                    height=low_res_size,\n",
    "                    width=low_res_size,\n",
    "                    num_blocks=blocks_per_stage,\n",
    "                    mamba_depth=mamba_depth\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Fusion module\n",
    "            self.fusion_modules.append(\n",
    "                MultiScaleFusion(\n",
    "                    high_channels=high_channels,\n",
    "                    low_channels=low_channels,\n",
    "                    scale_factor=2\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Aggregation layer\n",
    "        self.aggregation = AggregationLayer(\n",
    "            high_channels=high_channels,\n",
    "            low_channels=low_channels,\n",
    "            out_channels=high_channels + low_channels,\n",
    "            scale_factor=2\n",
    "        )\n",
    "        \n",
    "        # Store output channels\n",
    "        self.out_channels = high_channels + low_channels\n",
    "        self.feature_size = high_res_size\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through dual-stream backbone.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (B, in_channels, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "            - 'features': Aggregated features (B, out_channels, H/4, W/4)\n",
    "            - 'high_res': High-res stream output (B, C_h, H/4, W/4)\n",
    "            - 'low_res': Low-res stream output (B, C_l, H/8, W/8)\n",
    "        \"\"\"\n",
    "        # Initial stem\n",
    "        high = self.stem(x)  # (B, base_channels, H/4, W/4)\n",
    "        \n",
    "        # Create low-res stream\n",
    "        low = self.create_low_stream(high)  # (B, base_channels*2, H/8, W/8)\n",
    "        \n",
    "        # Process through stages with fusion\n",
    "        for stage_idx in range(self.num_stages):\n",
    "            # Process each stream\n",
    "            high = self.high_res_stages[stage_idx](high)\n",
    "            low = self.low_res_stages[stage_idx](low)\n",
    "            \n",
    "            # Fuse streams\n",
    "            high, low = self.fusion_modules[stage_idx](high, low)\n",
    "        \n",
    "        # Aggregate for output\n",
    "        features = self.aggregation(high, low)\n",
    "        \n",
    "        return {\n",
    "            'features': features,\n",
    "            'high_res': high,\n",
    "            'low_res': low\n",
    "        }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Testing\n",
    "# =============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2551c38e",
   "metadata": {},
   "source": [
    "### egm_net.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce16a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Energy-Gated Gabor Mamba Network (EGM-Net).\n",
    "\n",
    "A hybrid architecture for medical image segmentation combining physics-based\n",
    "signal processing with deep learning for artifact-free boundary delineation.\n",
    "\n",
    "Architecture Components:\n",
    "    1. HRNetV2-Mamba Backbone: Dual-stream with high-res preservation\n",
    "    2. Monogenic Signal Processing: Physics-based edge detection via Riesz transform\n",
    "    3. RBF Constellation Head: Gaussian classifier for coarse segmentation\n",
    "    4. Energy-Gated Gabor Implicit: FiLM-conditioned fine segmentation\n",
    "    5. Energy-Gated Fusion: Selective boundary refinement\n",
    "\n",
    "Key Innovations:\n",
    "    - Dual-path decoding: Coarse (RBF) + Fine (Implicit) branches\n",
    "    - Energy gating: Suppress artifacts in flat/homogeneous regions\n",
    "    - Gabor basis: Localized oscillations prevent Gibbs ringing artifacts\n",
    "    - Vector rotation augmentation: Maintains Riesz consistency under rotation\n",
    "\n",
    "References:\n",
    "    [1] Felsberg & Sommer, \"The Monogenic Signal,\" IEEE TSP, 2001.\n",
    "    [2] Wang et al., \"Deep High-Resolution Representation Learning,\" CVPR 2020.\n",
    "    [3] Liu et al., \"VMamba: Visual State Space Model,\" arXiv, 2024.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple, Dict\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple, Dict\n",
    "import math\n",
    "\n",
    "# Handle imports for both package and standalone usage\n",
    "try:\n",
    "except ImportError:\n",
    "    try:\n",
    "        # Standalone or different path structure fallback\n",
    "        import sys\n",
    "        sys.path.append(\"..\")\n",
    "    except ImportError:\n",
    "        # Testing fallback\n",
    "        pass\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Energy-Gated Fusion Module\n",
    "# =============================================================================\n",
    "\n",
    "class EnergyGatedFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Energy-Gated Fusion Module.\n",
    "    \n",
    "    Uses monogenic energy map to gate the implicit (fine) branch:\n",
    "    - High energy (edges): Activate fine branch for sharp boundaries\n",
    "    - Low energy (flat): Suppress fine branch to avoid artifacts\n",
    "    \n",
    "    Final output = Coarse + Energy Ã— (Fine - Coarse)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, temperature: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize Energy-Gated Fusion.\n",
    "        \n",
    "        Args:\n",
    "            temperature: Softness of gating (lower = harder gating)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.gate_scale = nn.Parameter(torch.ones(1))\n",
    "        self.gate_bias = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, coarse: torch.Tensor, fine: torch.Tensor, \n",
    "                energy: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Fuse coarse and fine predictions using energy gating.\n",
    "        \n",
    "        Args:\n",
    "            coarse: Coarse prediction (B, C, H, W)\n",
    "            fine: Fine prediction (B, C, H, W) \n",
    "            energy: Energy map (B, 1, H, W) in [0, 1]\n",
    "            \n",
    "        Returns:\n",
    "            Fused prediction (B, C, H, W)\n",
    "        \"\"\"\n",
    "        # Resize energy to match prediction size\n",
    "        if energy.shape[-2:] != coarse.shape[-2:]:\n",
    "            energy = F.interpolate(energy, size=coarse.shape[-2:], \n",
    "                                   mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # Resize fine to match coarse if needed\n",
    "        if fine.shape[-2:] != coarse.shape[-2:]:\n",
    "            fine = F.interpolate(fine, size=coarse.shape[-2:],\n",
    "                                mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # Apply learnable scaling and temperature\n",
    "        gate = torch.sigmoid((energy * self.gate_scale + self.gate_bias) / self.temperature)\n",
    "        \n",
    "        # Blend: high energy â†’ use fine, low energy â†’ use coarse\n",
    "        output = coarse + gate * (fine - coarse)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EGM-Net: Main Model\n",
    "# =============================================================================\n",
    "\n",
    "class EGMNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Energy-Gated Gabor Mamba Network (EGM-Net).\n",
    "    \n",
    "    Full architecture with HRNetV2-Mamba backbone:\n",
    "    \n",
    "    1. Input: 3 channels (Intensity, Rx, Ry) or 1 channel (Intensity only)\n",
    "    2. Backbone: HRNetV2-Mamba (dual-stream, maintains high-res throughout)\n",
    "    3. Coarse Head: RBF Constellation (Gaussian classifier with PSK prototypes)\n",
    "    4. Fine Head: Energy-Gated Gabor Implicit (FiLM-conditioned)\n",
    "    5. Fusion: Energy-gated blending of coarse and fine\n",
    "    \n",
    "    The energy map (from Monogenic Signal) gates the fine branch:\n",
    "    - High energy regions (boundaries): Use fine branch for sharp edges\n",
    "    - Low energy regions (flat): Use coarse branch to avoid artifacts\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Number of input channels (1 or 3)\n",
    "        num_classes: Number of segmentation classes\n",
    "        img_size: Input image size\n",
    "        base_channels: Base channel count for backbone\n",
    "        num_stages: Number of backbone stages\n",
    "        use_hrnet: Use HRNetV2 backbone (True) or simpler encoder (False)\n",
    "        implicit_hidden: Hidden dim for implicit decoder\n",
    "        implicit_layers: Number of implicit decoder layers\n",
    "        num_frequencies: Number of Gabor frequencies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 3, num_classes: int = 4,\n",
    "                 img_size: int = 256, base_channels: int = 64,\n",
    "                 num_stages: int = 4, use_hrnet: bool = True,\n",
    "                 implicit_hidden: int = 256, implicit_layers: int = 4,\n",
    "                 num_frequencies: int = 64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.use_hrnet = use_hrnet\n",
    "        \n",
    "        # 1. Monogenic Energy Extractor (fixed, non-trainable)\n",
    "        # For computing energy maps from input images\n",
    "        self.energy_extractor = EnergyMap(normalize=True, smoothing_sigma=1.0)\n",
    "        \n",
    "        # 2. Backbone\n",
    "        if use_hrnet:\n",
    "            self.backbone = HRNetV2MambaBackbone(\n",
    "                in_channels=in_channels,\n",
    "                base_channels=base_channels,\n",
    "                num_stages=num_stages,\n",
    "                blocks_per_stage=2,\n",
    "                mamba_depth=2,\n",
    "                img_size=img_size\n",
    "            )\n",
    "            backbone_channels = self.backbone.out_channels\n",
    "        else:\n",
    "            # Fallback to simpler encoder for testing\n",
    "            self.backbone = SimpleEncoder(\n",
    "                in_channels=in_channels,\n",
    "                base_channels=base_channels,\n",
    "                num_stages=num_stages\n",
    "            )\n",
    "            backbone_channels = base_channels * (2 ** (num_stages - 1))\n",
    "        \n",
    "        self.backbone_channels = backbone_channels\n",
    "        \n",
    "        # 3. Coarse Head: RBF Constellation\n",
    "        self.coarse_head = RBFConstellationHead(\n",
    "            in_channels=backbone_channels,\n",
    "            num_classes=num_classes,\n",
    "            embedding_dim=2,\n",
    "            init_gamma=1.0\n",
    "        )\n",
    "        \n",
    "        # 4. Fine Head: Energy-Gated Gabor Implicit\n",
    "        self.fine_head = EnergyGatedImplicitHead(\n",
    "            feature_channels=backbone_channels,\n",
    "            num_classes=num_classes,\n",
    "            hidden_dim=implicit_hidden,\n",
    "            num_layers=implicit_layers,\n",
    "            num_frequencies=num_frequencies\n",
    "        )\n",
    "        \n",
    "        # 5. Energy-Gated Fusion\n",
    "        self.fusion = EnergyGatedFusion(temperature=1.0)\n",
    "        \n",
    "        # Store feature size for output upsampling\n",
    "        if use_hrnet:\n",
    "            self.feature_size = img_size // 4\n",
    "        else:\n",
    "            self.feature_size = img_size // (2 ** num_stages)\n",
    "    \n",
    "    def _compute_energy(self, x: torch.Tensor) -> Tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Compute energy map from input.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (B, C, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (energy_map, monogenic_outputs)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Use first channel (intensity) for energy computation\n",
    "            x_gray = x[:, 0:1] if x.shape[1] > 1 else x\n",
    "            energy, mono_out = self.energy_extractor(x_gray)\n",
    "        return energy, mono_out\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, \n",
    "                output_size: Optional[Tuple[int, int]] = None,\n",
    "                return_intermediates: bool = True) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input image (B, C, H, W)\n",
    "               - If C=3: Expected [Intensity, Rx, Ry]\n",
    "               - If C=1: Intensity only, energy computed online\n",
    "            output_size: Optional output resolution (default: input size)\n",
    "            return_intermediates: Whether to return coarse/fine/energy separately\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "            - 'output': Final fused segmentation (B, num_classes, H, W)\n",
    "            - 'coarse': Coarse branch output (B, num_classes, H, W)\n",
    "            - 'fine': Fine branch output (B, num_classes, H, W)\n",
    "            - 'energy': Energy map (B, 1, H, W)\n",
    "            - 'embeddings': 2D constellation embeddings (B, 2, H, W) [optional]\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        if output_size is None:\n",
    "            output_size = (H, W)\n",
    "        \n",
    "        # 1. Compute or extract energy map\n",
    "        if C >= 3:\n",
    "            # If Riesz components are provided, compute energy from them\n",
    "            intensity = x[:, 0:1]  # (B, 1, H, W)\n",
    "            riesz_x = x[:, 1:2]    # (B, 1, H, W)  \n",
    "            riesz_y = x[:, 2:3]    # (B, 1, H, W)\n",
    "            \n",
    "            # Energy = sqrt(I^2 + Rx^2 + Ry^2)\n",
    "            energy = torch.sqrt(intensity**2 + riesz_x**2 + riesz_y**2 + 1e-8)\n",
    "            # Normalize to [0, 1]\n",
    "            energy = (energy - energy.min()) / (energy.max() - energy.min() + 1e-8)\n",
    "        else:\n",
    "            # Compute energy online using Monogenic Signal\n",
    "            energy, _ = self._compute_energy(x)\n",
    "        \n",
    "        # 2. Backbone feature extraction\n",
    "        if self.use_hrnet:\n",
    "            backbone_out = self.backbone(x)\n",
    "            features = backbone_out['features']\n",
    "        else:\n",
    "            features = self.backbone(x)\n",
    "        \n",
    "        # 3. Coarse Head (RBF Constellation)\n",
    "        coarse_logits, embeddings = self.coarse_head(features)\n",
    "        \n",
    "        # Upsample coarse to output size\n",
    "        coarse = F.interpolate(coarse_logits, size=output_size,\n",
    "                              mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # 4. Fine Head (Energy-Gated Gabor Implicit)\n",
    "        # Resize energy to feature size for implicit head\n",
    "        energy_for_fine = F.interpolate(energy, size=features.shape[-2:],\n",
    "                                        mode='bilinear', align_corners=True)\n",
    "        \n",
    "        fine_logits = self.fine_head(features, energy_for_fine, output_size=output_size)\n",
    "        \n",
    "        # Ensure fine is same size as coarse\n",
    "        if fine_logits.shape[-2:] != output_size:\n",
    "            fine_logits = F.interpolate(fine_logits, size=output_size,\n",
    "                                       mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # 5. Energy-Gated Fusion\n",
    "        output = self.fusion(coarse, fine_logits, energy)\n",
    "        \n",
    "        if return_intermediates:\n",
    "            return {\n",
    "                'output': output,\n",
    "                'coarse': coarse,\n",
    "                'fine': fine_logits,\n",
    "                'energy': energy,\n",
    "                'embeddings': embeddings\n",
    "            }\n",
    "        else:\n",
    "            return {'output': output}\n",
    "    \n",
    "    def inference(self, x: torch.Tensor, \n",
    "                  output_size: Optional[Tuple[int, int]] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Simplified inference returning only final output.\n",
    "        \n",
    "        Args:\n",
    "            x: Input image (B, C, H, W)\n",
    "            output_size: Optional output resolution\n",
    "            \n",
    "        Returns:\n",
    "            Segmentation logits (B, num_classes, H, W)\n",
    "        \"\"\"\n",
    "        return self.forward(x, output_size, return_intermediates=False)['output']\n",
    "    \n",
    "    def sample_points(self, coarse_logits: torch.Tensor, \n",
    "                      energy_map: torch.Tensor, \n",
    "                      num_samples: int = 4096) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample points based on Uncertainty + Energy logic.\n",
    "        \n",
    "        Logic:\n",
    "           1. Compute Uncertainty Map (Entropy or Margin) from coarse predictions.\n",
    "           2. Combine with Energy Map: P_sample âˆ (Uncertainty + Energy)\n",
    "           3. Sample coordinates from this distribution.\n",
    "           \n",
    "        Args:\n",
    "            coarse_logits: Coarse segmentation logits (B, num_classes, H, W)\n",
    "            energy_map: Monogenic energy map (B, 1, H, W)\n",
    "            num_samples: Number of points to sample per image\n",
    "            \n",
    "        Returns:\n",
    "            Sampled coordinates (B, N, 2) in [-1, 1] range\n",
    "        \"\"\"\n",
    "        B, C, H, W = coarse_logits.shape\n",
    "        device = coarse_logits.device\n",
    "        \n",
    "        # 1. Compute Uncertainty (1 - max_prob)\n",
    "        # Softmax probabilities\n",
    "        probs = F.softmax(coarse_logits, dim=1)\n",
    "        # Max probability per pixel\n",
    "        max_prob, _ = probs.max(dim=1, keepdim=True)  # (B, 1, H, W)\n",
    "        # Uncertainty: High where max_prob is low (near 1/num_classes)\n",
    "        uncertainty = 1.0 - max_prob \n",
    "        \n",
    "        # 2. Resizing Energy Map to match coarse logits\n",
    "        if energy_map.shape[-2:] != (H, W):\n",
    "            energy_map = F.interpolate(\n",
    "                energy_map, size=(H, W), mode='bilinear', align_corners=True\n",
    "            )\n",
    "            \n",
    "        # 3. Combine Uncertainty + Energy for Sampling Probability\n",
    "        # Normalize both to [0, 1] for balanced combination\n",
    "        uncertainty = (uncertainty - uncertainty.min()) / (uncertainty.max() - uncertainty.min() + 1e-8)\n",
    "        # Weighting: Bias strongly towards uncertainty (boundaries) and high energy\n",
    "        sample_weights = uncertainty + 0.5 * energy_map\n",
    "        \n",
    "        # Flatten for sampling\n",
    "        sample_weights_flat = sample_weights.view(B, -1)  # (B, H*W)\n",
    "        \n",
    "        # 4. Multinomial Sampling\n",
    "        # Sample indices based on weights\n",
    "        indices = torch.multinomial(sample_weights_flat, num_samples, replacement=True)  # (B, N)\n",
    "        \n",
    "        # Convert indices to coordinates [-1, 1]\n",
    "        # x_idx = index % W, y_idx = index // W\n",
    "        x_idx = indices % W\n",
    "        y_idx = indices // W\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        # (x + 0.5) / W * 2 - 1  (using +0.5 to sample pixel centers)\n",
    "        x_norm = (x_idx.float() + 0.5) / W * 2.0 - 1.0\n",
    "        y_norm = (y_idx.float() + 0.5) / H * 2.0 - 1.0\n",
    "        \n",
    "        coords = torch.stack([x_norm, y_norm], dim=-1)  # (B, N, 2)\n",
    "        \n",
    "        return coords\n",
    "    \n",
    "    def query_points(self, x: torch.Tensor, \n",
    "                     coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Query segmentation at arbitrary coordinates (implicit representation).\n",
    "        \n",
    "        This enables resolution-free inference - zoom into boundaries at any\n",
    "        resolution without pixelation.\n",
    "        \n",
    "        Args:\n",
    "            x: Input image (B, C, H, W)\n",
    "            coords: Query coordinates (B, N, 2) normalized to [-1, 1]\n",
    "            \n",
    "        Returns:\n",
    "            Segmentation logits at query points (B, N, num_classes)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Compute energy\n",
    "        if C >= 3:\n",
    "            intensity = x[:, 0:1]\n",
    "            riesz_x = x[:, 1:2]\n",
    "            riesz_y = x[:, 2:3]\n",
    "            energy = torch.sqrt(intensity**2 + riesz_x**2 + riesz_y**2 + 1e-8)\n",
    "            energy = (energy - energy.min()) / (energy.max() - energy.min() + 1e-8)\n",
    "        else:\n",
    "            energy, _ = self._compute_energy(x)\n",
    "        \n",
    "        # Get backbone features\n",
    "        if self.use_hrnet:\n",
    "            backbone_out = self.backbone(x)\n",
    "            features = backbone_out['features']\n",
    "        else:\n",
    "            features = self.backbone(x)\n",
    "        \n",
    "        # Sample features and energy at query coordinates\n",
    "        N = coords.shape[1]\n",
    "        grid = coords.view(B, 1, N, 2)\n",
    "        \n",
    "        # Project features\n",
    "        features_proj = self.fine_head.feature_proj(features)\n",
    "        \n",
    "        # Sample at coordinates\n",
    "        feat_sampled = F.grid_sample(\n",
    "            features_proj, grid, mode='bilinear',\n",
    "            padding_mode='border', align_corners=True\n",
    "        ).squeeze(2).permute(0, 2, 1)  # (B, N, C)\n",
    "        \n",
    "        energy_sampled = F.grid_sample(\n",
    "            energy, grid, mode='bilinear',\n",
    "            padding_mode='border', align_corners=True\n",
    "        ).squeeze(2).permute(0, 2, 1)  # (B, N, 1)\n",
    "        \n",
    "        # Implicit decoding with energy gating\n",
    "        point_logits = self.fine_head.implicit_decoder(coords, feat_sampled, energy_sampled)\n",
    "        \n",
    "        return point_logits\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Simple Encoder (Fallback for testing)\n",
    "# =============================================================================\n",
    "\n",
    "class SimpleEncoder(nn.Module):\n",
    "    \"\"\"Simple encoder for testing without full HRNet.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 3, base_channels: int = 64,\n",
    "                 num_stages: int = 4):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        channels = in_channels\n",
    "        out_channels = base_channels\n",
    "        \n",
    "        for i in range(num_stages):\n",
    "            layers.append(nn.Conv2d(channels, out_channels, 3, stride=2, padding=1))\n",
    "            layers.append(nn.GroupNorm(min(32, out_channels), out_channels))\n",
    "            layers.append(nn.GELU())\n",
    "            channels = out_channels\n",
    "            out_channels = min(out_channels * 2, 512)\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.out_channels = channels\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EGM-Net Lite (Lightweight version)\n",
    "# =============================================================================\n",
    "\n",
    "class EGMNetLite(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight version of EGM-Net for faster training/inference.\n",
    "    \n",
    "    Reduced channels and stages for lower memory/compute.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 1, num_classes: int = 2,\n",
    "                 img_size: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = EGMNet(\n",
    "            in_channels=in_channels,\n",
    "            num_classes=num_classes,\n",
    "            img_size=img_size,\n",
    "            base_channels=32,\n",
    "            num_stages=3,\n",
    "            use_hrnet=False,  # Use simpler encoder\n",
    "            implicit_hidden=128,\n",
    "            implicit_layers=3,\n",
    "            num_frequencies=32\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, output_size=None):\n",
    "        return self.model(x, output_size)\n",
    "    \n",
    "    def inference(self, x, output_size=None):\n",
    "        return self.model.inference(x, output_size)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Testing\n",
    "# =============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b122fa9",
   "metadata": {},
   "source": [
    "### physics_loss.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc84b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Physics-Inspired Dual Loss Function\n",
    "Combines spatial (Dice + CE) and frequency domain losses for medical image segmentation.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice Loss for binary/multi-class segmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, smooth: float = 1e-5, reduction: str = \"mean\"):\n",
    "        \"\"\"\n",
    "        Initialize Dice Loss.\n",
    "        \n",
    "        Args:\n",
    "            smooth: Smoothing constant to avoid division by zero\n",
    "            reduction: 'mean' or 'sum'\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute Dice Loss.\n",
    "        \n",
    "        Args:\n",
    "            pred: Prediction logits of shape (B, C, H, W)\n",
    "            target: Ground truth labels of shape (B, H, W) or (B, C, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Scalar Dice loss\n",
    "        \"\"\"\n",
    "        # Convert logits to probabilities\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        \n",
    "        # Ensure target has same shape as pred for multi-class\n",
    "        if target.ndim == 3:  # (B, H, W) -> convert to one-hot\n",
    "            target = F.one_hot(target.long(), num_classes=pred.shape[1])\n",
    "            target = target.permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        # Flatten spatial dimensions\n",
    "        pred = pred.view(pred.shape[0], pred.shape[1], -1)\n",
    "        target = target.view(target.shape[0], target.shape[1], -1)\n",
    "        \n",
    "        # Compute Dice score\n",
    "        intersection = torch.sum(pred * target, dim=2)\n",
    "        union = torch.sum(pred, dim=2) + torch.sum(target, dim=2)\n",
    "        \n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        # Return loss (1 - Dice)\n",
    "        loss = 1.0 - dice\n",
    "        \n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 0.25, gamma: float = 2.0,\n",
    "                 reduction: str = \"mean\"):\n",
    "        \"\"\"\n",
    "        Initialize Focal Loss.\n",
    "        \n",
    "        Args:\n",
    "            alpha: Weighting factor for class 1\n",
    "            gamma: Focusing parameter (higher = more focus on hard examples)\n",
    "            reduction: 'mean' or 'sum'\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute Focal Loss.\n",
    "        \n",
    "        Args:\n",
    "            pred: Prediction logits of shape (B, C, H, W)\n",
    "            target: Ground truth labels of shape (B, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Scalar Focal loss\n",
    "        \"\"\"\n",
    "        # Get class probabilities\n",
    "        p = torch.softmax(pred, dim=1)\n",
    "        \n",
    "        # Get class log probabilities\n",
    "        ce = F.cross_entropy(pred, target.long(), reduction='none')\n",
    "        \n",
    "        # Get probability of true class\n",
    "        p_t = torch.gather(p, 1, target.long().unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute focal loss\n",
    "        focal_weight = (1.0 - p_t) ** self.gamma\n",
    "        focal_loss = focal_weight * ce\n",
    "        \n",
    "        if self.reduction == \"mean\":\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "class FrequencyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Frequency Domain Loss for enforcing edge sharpness and detail preservation.\n",
    "    Computes L2 distance between FFT of prediction and ground truth.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, weight: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize FrequencyLoss.\n",
    "        \n",
    "        Args:\n",
    "            weight: Weight factor for this loss component in combined loss\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute frequency domain loss.\n",
    "        \n",
    "        Uses FFT to compare frequency components, emphasizing edge preservation.\n",
    "        \n",
    "        Args:\n",
    "            pred: Prediction tensor of shape (B, C, H, W) or (B, H, W)\n",
    "            target: Ground truth tensor of same shape as pred\n",
    "            \n",
    "        Returns:\n",
    "            Scalar frequency loss\n",
    "        \"\"\"\n",
    "        # Ensure both have batch and channel dimensions\n",
    "        if pred.ndim == 3:\n",
    "            pred = pred.unsqueeze(1)\n",
    "        if target.ndim == 3:\n",
    "            target = target.unsqueeze(1)\n",
    "        \n",
    "        # Flatten to single channel for FFT comparison\n",
    "        if pred.shape[1] > 1:\n",
    "            # For multi-channel, convert to grayscale by averaging\n",
    "            pred = pred.mean(dim=1, keepdim=True)\n",
    "        if target.shape[1] > 1:\n",
    "            target = target.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        # Apply FFT to convert to frequency domain\n",
    "        pred_freq = torch.fft.rfft2(pred, dim=(-2, -1), norm=\"ortho\")\n",
    "        target_freq = torch.fft.rfft2(target, dim=(-2, -1), norm=\"ortho\")\n",
    "        \n",
    "        # Compute L2 distance in frequency domain\n",
    "        # Consider both magnitude and phase information\n",
    "        loss_real = F.mse_loss(pred_freq.real, target_freq.real, reduction='mean')\n",
    "        loss_imag = F.mse_loss(pred_freq.imag, target_freq.imag, reduction='mean')\n",
    "        \n",
    "        return loss_real + loss_imag\n",
    "\n",
    "\n",
    "class SpectralDualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined Spectral Dual Loss.\n",
    "    \n",
    "    Combines:\n",
    "    1. Spatial losses (Dice + Focal CE) - for overall shape and class balance\n",
    "    2. Frequency loss - for edge sharpness and boundary preservation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spatial_weight: float = 1.0, freq_weight: float = 0.1,\n",
    "                 use_dice: bool = True, use_focal: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize SpectralDualLoss.\n",
    "        \n",
    "        Args:\n",
    "            spatial_weight: Weight for spatial losses\n",
    "            freq_weight: Weight for frequency loss\n",
    "            use_dice: Whether to include Dice loss\n",
    "            use_focal: Whether to include Focal loss (else CrossEntropy)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.spatial_weight = spatial_weight\n",
    "        self.freq_weight = freq_weight\n",
    "        self.use_dice = use_dice\n",
    "        self.use_focal = use_focal\n",
    "        \n",
    "        # Spatial losses\n",
    "        if use_dice:\n",
    "            self.dice_loss = DiceLoss(smooth=1e-5)\n",
    "        \n",
    "        if use_focal:\n",
    "            self.focal_loss = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "        else:\n",
    "            self.ce_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Frequency loss\n",
    "        self.freq_loss = FrequencyLoss(weight=freq_weight)\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor,\n",
    "                return_components: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute combined loss.\n",
    "        \n",
    "        Args:\n",
    "            pred: Prediction logits of shape (B, C, H, W)\n",
    "            target: Ground truth labels of shape (B, H, W)\n",
    "            return_components: If True, return dict with individual loss components\n",
    "            \n",
    "        Returns:\n",
    "            Scalar combined loss, or dict if return_components=True\n",
    "        \"\"\"\n",
    "        # Ensure target is on same device as pred\n",
    "        target = target.to(pred.device)\n",
    "        \n",
    "        # Spatial losses\n",
    "        spatial_loss = 0.0\n",
    "        losses_dict = {}\n",
    "        \n",
    "        if self.use_dice:\n",
    "            dice = self.dice_loss(pred, target)\n",
    "            spatial_loss = spatial_loss + dice\n",
    "            losses_dict['dice'] = dice.item()\n",
    "        \n",
    "        if self.use_focal:\n",
    "            focal = self.focal_loss(pred, target)\n",
    "            spatial_loss = spatial_loss + focal\n",
    "            losses_dict['focal'] = focal.item()\n",
    "        else:\n",
    "            ce = self.ce_loss(pred, target)\n",
    "            spatial_loss = spatial_loss + ce\n",
    "            losses_dict['ce'] = ce.item()\n",
    "        \n",
    "        # Frequency loss\n",
    "        # For frequency loss, we need to extract the predicted class (argmax) and compare\n",
    "        pred_probs = torch.softmax(pred, dim=1)\n",
    "        pred_class = torch.argmax(pred_probs, dim=1)  # (B, H, W)\n",
    "        \n",
    "        freq = self.freq_loss(pred_class.float(), target.float())\n",
    "        losses_dict['freq'] = freq.item()\n",
    "        \n",
    "        # Weighted combination\n",
    "        total_loss = (self.spatial_weight * spatial_loss + \n",
    "                     self.freq_weight * freq)\n",
    "        losses_dict['total'] = total_loss.item()\n",
    "        \n",
    "        if return_components:\n",
    "            return total_loss, losses_dict\n",
    "        else:\n",
    "            return total_loss\n",
    "\n",
    "\n",
    "class BoundaryAwareLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Boundary-aware loss that emphasizes pixels near segmentation boundaries.\n",
    "    Useful for improving edge precision.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kernel_size: int = 3, weight: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize BoundaryAwareLoss.\n",
    "        \n",
    "        Args:\n",
    "            kernel_size: Size of kernel for computing boundary gradients\n",
    "            weight: Weight for boundary loss\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.weight = weight\n",
    "    \n",
    "    def _compute_boundaries(self, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute boundary mask using gradient.\n",
    "        \n",
    "        Args:\n",
    "            mask: Binary mask of shape (B, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Boundary map of shape (B, H, W)\n",
    "        \"\"\"\n",
    "        # Convert to float\n",
    "        mask = mask.float().unsqueeze(1)  # (B, 1, H, W)\n",
    "        \n",
    "        # Compute gradients using Sobel-like operation\n",
    "        kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],\n",
    "                                dtype=mask.dtype, device=mask.device)\n",
    "        kernel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]],\n",
    "                                dtype=mask.dtype, device=mask.device)\n",
    "        \n",
    "        kernel_x = kernel_x.view(1, 1, 3, 3)\n",
    "        kernel_y = kernel_y.view(1, 1, 3, 3)\n",
    "        \n",
    "        grad_x = F.conv2d(mask, kernel_x, padding=1)\n",
    "        grad_y = F.conv2d(mask, kernel_y, padding=1)\n",
    "        \n",
    "        # Compute magnitude of gradient\n",
    "        grad_magnitude = torch.sqrt(grad_x ** 2 + grad_y ** 2 + 1e-8)\n",
    "        \n",
    "        # Threshold to get boundary pixels\n",
    "        boundary = (grad_magnitude > 0).float().squeeze(1)\n",
    "        \n",
    "        return boundary\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute boundary-aware loss.\n",
    "        \n",
    "        Args:\n",
    "            pred: Prediction logits of shape (B, C, H, W)\n",
    "            target: Ground truth labels of shape (B, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            Scalar loss emphasizing boundaries\n",
    "        \"\"\"\n",
    "        # Get predicted class\n",
    "        pred_probs = torch.softmax(pred, dim=1)\n",
    "        pred_class = torch.argmax(pred_probs, dim=1)  # (B, H, W)\n",
    "        \n",
    "        # Compute boundary maps\n",
    "        pred_boundary = self._compute_boundaries(pred_class)\n",
    "        target_boundary = self._compute_boundaries(target)\n",
    "        \n",
    "        # Compute cross-entropy loss weighted by boundary\n",
    "        ce_loss = F.cross_entropy(pred, target.long(), reduction='none')\n",
    "        \n",
    "        # Apply boundary weight (higher loss for boundary pixels)\n",
    "        boundary_weight = (pred_boundary + target_boundary).clamp(0, 1)\n",
    "        boundary_weight = 1.0 + boundary_weight  # Weight between 1 and 2\n",
    "        \n",
    "        weighted_loss = ce_loss * boundary_weight\n",
    "        \n",
    "        return weighted_loss.mean()\n",
    "\n",
    "\n",
    "class EyeOpeningLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Eye Opening Loss (Entropy Minimization at Boundaries).\n",
    "    \n",
    "    Based on the Eye Diagram concept from telecommunications:\n",
    "    - Clear \"eye\" opening = decisive predictions (0 or 1)\n",
    "    - Closed \"eye\" = uncertain predictions (~0.5)\n",
    "    \n",
    "    This loss penalizes predictions near 0.5 at boundary regions,\n",
    "    forcing the model to make decisive (sharp) boundary predictions.\n",
    "    \n",
    "    L_eye = 4 Ã— p Ã— (1 - p)\n",
    "    \n",
    "    This reaches maximum (1.0) when p=0.5 and minimum (0.0) when p=0 or p=1.\n",
    "    \n",
    "    Features:\n",
    "    - Warm-up scheduling: Only activates after N epochs\n",
    "    - Annealing: Weight gradually increases to prevent early destabilization\n",
    "    - Boundary focus: Optionally weight by energy/boundary map\n",
    "    \n",
    "    Args:\n",
    "        warmup_epochs: Number of epochs before activating this loss\n",
    "        max_weight: Maximum weight for this loss component\n",
    "        anneal_rate: Rate of weight increase per epoch after warm-up\n",
    "        smooth: Smoothing constant for numerical stability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, warmup_epochs: int = 5, max_weight: float = 0.1,\n",
    "                 anneal_rate: float = 0.02, smooth: float = 1e-7):\n",
    "        super().__init__()\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_weight = max_weight\n",
    "        self.anneal_rate = anneal_rate\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def get_weight(self, epoch: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute loss weight based on current epoch.\n",
    "        \n",
    "        Args:\n",
    "            epoch: Current training epoch (0-indexed)\n",
    "            \n",
    "        Returns:\n",
    "            Loss weight (0 during warm-up, then annealed up to max_weight)\n",
    "        \"\"\"\n",
    "        if epoch < self.warmup_epochs:\n",
    "            return 0.0\n",
    "        return min(self.max_weight, self.anneal_rate * (epoch - self.warmup_epochs))\n",
    "    \n",
    "    def forward(self, logits: torch.Tensor, \n",
    "                energy_map: Optional[torch.Tensor] = None,\n",
    "                epoch: int = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute Eye Opening Loss.\n",
    "        \n",
    "        Args:\n",
    "            logits: Prediction logits of shape (B, C, H, W) or (B, N, C)\n",
    "            energy_map: Optional energy map (B, 1, H, W) to focus on boundaries\n",
    "            epoch: Current training epoch for weight scheduling\n",
    "            \n",
    "        Returns:\n",
    "            Scalar eye opening loss\n",
    "        \"\"\"\n",
    "        # Get weight for current epoch\n",
    "        weight = self.get_weight(epoch)\n",
    "        \n",
    "        if weight <= 0:\n",
    "            return torch.tensor(0.0, device=logits.device, requires_grad=False)\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        if logits.dim() == 4:\n",
    "            # (B, C, H, W) -> softmax over classes\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            # For multi-class, we want high confidence for ANY class\n",
    "            # Maximum probability per pixel (how confident the prediction is)\n",
    "            max_probs = probs.max(dim=1, keepdim=True)[0]  # (B, 1, H, W)\n",
    "        else:\n",
    "            # (B, N, C) -> softmax over classes\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            max_probs = probs.max(dim=-1, keepdim=True)[0]  # (B, N, 1)\n",
    "        \n",
    "        # Eye opening: penalize uncertainty (probs near 0.5)\n",
    "        # L = 4 Ã— p Ã— (1 - p), maximizes at p=0.5\n",
    "        eye_loss = 4 * max_probs * (1 - max_probs)\n",
    "        \n",
    "        # Apply energy weighting (focus on boundaries) if provided\n",
    "        if energy_map is not None:\n",
    "            if energy_map.shape[-2:] != eye_loss.shape[-2:]:\n",
    "                energy_map = F.interpolate(\n",
    "                    energy_map, size=eye_loss.shape[-2:],\n",
    "                    mode='bilinear', align_corners=True\n",
    "                )\n",
    "            eye_loss = eye_loss * energy_map\n",
    "        \n",
    "        # Return weighted mean\n",
    "        return weight * eye_loss.mean()\n",
    "\n",
    "\n",
    "class EGMCombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined Loss for EGM-Net Training.\n",
    "    \n",
    "    Combines:\n",
    "    1. Coarse Loss: Dice + CE for the coarse segmentation\n",
    "    2. Fine Loss: BCE/CE for point-sampled fine predictions\n",
    "    3. Eye Opening Loss: Entropy minimization at boundaries (with warm-up)\n",
    "    4. Consistency Loss: Agreement between coarse and fine branches\n",
    "    \n",
    "    Args:\n",
    "        dice_weight: Weight for Dice loss\n",
    "        ce_weight: Weight for CrossEntropy loss\n",
    "        fine_weight: Weight for fine branch loss\n",
    "        eye_weight: Max weight for Eye Opening loss\n",
    "        consistency_weight: Weight for coarse-fine consistency\n",
    "        eye_warmup: Epochs before activating Eye Opening loss\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dice_weight: float = 1.0, ce_weight: float = 1.0,\n",
    "                 fine_weight: float = 1.0, eye_weight: float = 0.1,\n",
    "                 consistency_weight: float = 0.1, eye_warmup: int = 5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_weight = ce_weight\n",
    "        self.fine_weight = fine_weight\n",
    "        self.eye_weight = eye_weight\n",
    "        self.consistency_weight = consistency_weight\n",
    "        \n",
    "        # Loss components\n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.eye_loss = EyeOpeningLoss(\n",
    "            warmup_epochs=eye_warmup, \n",
    "            max_weight=eye_weight,\n",
    "            anneal_rate=0.02\n",
    "        )\n",
    "    \n",
    "    def forward(self, outputs: dict, target: torch.Tensor,\n",
    "                point_logits: Optional[torch.Tensor] = None,\n",
    "                point_labels: Optional[torch.Tensor] = None,\n",
    "                epoch: int = 0) -> dict:\n",
    "        \"\"\"\n",
    "        Compute combined loss.\n",
    "        \n",
    "        Args:\n",
    "            outputs: Model outputs dict with 'coarse', 'fine', 'energy'\n",
    "            target: Ground truth mask (B, H, W)\n",
    "            point_logits: Optional point predictions (B, N, C)\n",
    "            point_labels: Optional point labels (B, N)\n",
    "            epoch: Current training epoch\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'total' loss and individual components\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "        \n",
    "        # 1. Coarse branch losses\n",
    "        coarse = outputs.get('coarse', outputs.get('output'))\n",
    "        \n",
    "        # Resize target if needed\n",
    "        if coarse.shape[-2:] != target.shape[-2:]:\n",
    "            target_resized = F.interpolate(\n",
    "                target.unsqueeze(1).float(),\n",
    "                size=coarse.shape[-2:],\n",
    "                mode='nearest'\n",
    "            ).squeeze(1).long()\n",
    "        else:\n",
    "            target_resized = target.long()\n",
    "        \n",
    "        dice = self.dice_loss(coarse, target_resized)\n",
    "        ce = self.ce_loss(coarse, target_resized)\n",
    "        \n",
    "        losses['dice'] = dice\n",
    "        losses['ce'] = ce\n",
    "        \n",
    "        coarse_loss = self.dice_weight * dice + self.ce_weight * ce\n",
    "        \n",
    "        # 2. Fine branch loss (point-based if available)\n",
    "        fine_loss = torch.tensor(0.0, device=coarse.device)\n",
    "        if point_logits is not None and point_labels is not None:\n",
    "            # Point-based cross-entropy\n",
    "            B, N, C = point_logits.shape\n",
    "            point_logits_flat = point_logits.view(B * N, C)\n",
    "            point_labels_flat = point_labels.view(B * N)\n",
    "            fine_loss = F.cross_entropy(point_logits_flat, point_labels_flat)\n",
    "            losses['fine'] = fine_loss\n",
    "        elif 'fine' in outputs:\n",
    "            # Use spatial fine output\n",
    "            fine = outputs['fine']\n",
    "            if fine.shape[-2:] != target_resized.shape[-2:]:\n",
    "                target_for_fine = F.interpolate(\n",
    "                    target_resized.unsqueeze(1).float(),\n",
    "                    size=fine.shape[-2:],\n",
    "                    mode='nearest'\n",
    "                ).squeeze(1).long()\n",
    "            else:\n",
    "                target_for_fine = target_resized\n",
    "            fine_loss = self.ce_loss(fine, target_for_fine)\n",
    "            losses['fine'] = fine_loss\n",
    "        \n",
    "        # 3. Eye Opening Loss (with warm-up)\n",
    "        energy_map = outputs.get('energy', None)\n",
    "        if 'fine' in outputs:\n",
    "            eye = self.eye_loss(outputs['fine'], energy_map, epoch)\n",
    "        else:\n",
    "            eye = self.eye_loss(coarse, energy_map, epoch)\n",
    "        losses['eye'] = eye\n",
    "        \n",
    "        # 4. Consistency Loss (coarse and fine should agree)\n",
    "        consistency_loss = torch.tensor(0.0, device=coarse.device)\n",
    "        if 'fine' in outputs and self.consistency_weight > 0:\n",
    "            fine = outputs['fine']\n",
    "            if fine.shape[-2:] != coarse.shape[-2:]:\n",
    "                fine_resized = F.interpolate(\n",
    "                    fine, size=coarse.shape[-2:],\n",
    "                    mode='bilinear', align_corners=True\n",
    "                )\n",
    "            else:\n",
    "                fine_resized = fine\n",
    "            \n",
    "            # KL divergence for consistency\n",
    "            coarse_probs = F.log_softmax(coarse, dim=1)\n",
    "            fine_probs = F.softmax(fine_resized, dim=1)\n",
    "            consistency_loss = F.kl_div(coarse_probs, fine_probs, reduction='batchmean')\n",
    "            losses['consistency'] = consistency_loss\n",
    "        \n",
    "        # Total loss\n",
    "        total = (coarse_loss + \n",
    "                 self.fine_weight * fine_loss + \n",
    "                 eye + \n",
    "                 self.consistency_weight * consistency_loss)\n",
    "        \n",
    "        losses['total'] = total\n",
    "        \n",
    "        return losses\n",
    "\n",
    "\n",
    "\n",
    "    # Test losses\n",
    "    batch_size, num_classes, height, width = 2, 3, 64, 64\n",
    "    \n",
    "    # Create dummy predictions and targets\n",
    "    pred = torch.randn(batch_size, num_classes, height, width)\n",
    "    target = torch.randint(0, num_classes, (batch_size, height, width))\n",
    "    \n",
    "    # Test SpectralDualLoss\n",
    "    loss_fn = SpectralDualLoss(spatial_weight=1.0, freq_weight=0.1)\n",
    "    loss, components = loss_fn(pred, target, return_components=True)\n",
    "    \n",
    "    print(f\"Total Loss: {loss.item():.4f}\")\n",
    "    for name, value in components.items():\n",
    "        print(f\"  {name}: {value:.4f}\")\n",
    "    \n",
    "    # Test BoundaryAwareLoss\n",
    "    boundary_loss_fn = BoundaryAwareLoss()\n",
    "    boundary_loss = boundary_loss_fn(pred, target)\n",
    "    print(f\"\\nBoundary Loss: {boundary_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3dda03",
   "metadata": {},
   "source": [
    "### 2.1 Mamba Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0488aab1",
   "metadata": {},
   "source": [
    "### 2.2 Spectral Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "class SpectralGating(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, height: int, width: int, \n",
    "                 threshold: float = 0.1, complex_init: str = \"kaiming\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # Create learnable complex weights for frequency domain\n",
    "        # Shape: (channels, height, width//2 + 1) for rfft2\n",
    "        self.register_buffer(\n",
    "            \"freq_shape\",\n",
    "            torch.tensor([channels, height, width // 2 + 1], dtype=torch.long)\n",
    "        )\n",
    "        \n",
    "        # Real and Imaginary parts of complex weights\n",
    "        self.weight_real = nn.Parameter(\n",
    "            torch.zeros(channels, height, width // 2 + 1)\n",
    "        )\n",
    "        self.weight_imag = nn.Parameter(\n",
    "            torch.zeros(channels, height, width // 2 + 1)\n",
    "        )\n",
    "        \n",
    "        self._init_weights(complex_init)\n",
    "        \n",
    "    def _init_weights(self, strategy: str = \"kaiming\"):\n",
    "        \n",
    "        if strategy == \"identity\":\n",
    "            # Initialize close to identity (magnitude ~1, phase ~0)\n",
    "            nn.init.ones_(self.weight_real)\n",
    "            nn.init.zeros_(self.weight_imag)\n",
    "        elif strategy == \"kaiming\":\n",
    "            # Kaiming initialization adapted for complex numbers\n",
    "            fan_in = self.height * (self.width // 2 + 1)\n",
    "            std = (2.0 / fan_in) ** 0.5\n",
    "            nn.init.normal_(self.weight_real, 0, std)\n",
    "            nn.init.normal_(self.weight_imag, 0, std)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown init strategy: {strategy}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Apply FFT to convert to frequency domain\n",
    "        # rfft2 returns complex tensor\n",
    "        x_freq = torch.fft.rfft2(x, dim=(-2, -1), norm=\"ortho\")\n",
    "        \n",
    "        # Create complex weight matrix: weight_real + 1j * weight_imag\n",
    "        # Reshape to (1, C, H, W//2+1) for broadcasting\n",
    "        complex_weight = (\n",
    "            self.weight_real.unsqueeze(0) + \n",
    "            1j * self.weight_imag.unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        # Apply channel-wise multiplication in frequency domain\n",
    "        # Shape: (B, C, H, W//2+1) * (1, C, H, W//2+1) -> (B, C, H, W//2+1)\n",
    "        x_filtered = x_freq * complex_weight\n",
    "        \n",
    "        # Optional: Hard thresholding to remove low-amplitude noise\n",
    "        if self.threshold > 0:\n",
    "            magnitude = torch.abs(x_filtered)\n",
    "            mask = magnitude > self.threshold\n",
    "            x_filtered = x_filtered * mask.float()\n",
    "        \n",
    "        # Apply inverse FFT to return to spatial domain\n",
    "        output = torch.fft.irfft2(x_filtered, s=(H, W), dim=(-2, -1), norm=\"ortho\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "class FrequencyLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, weight: float = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Apply FFT\n",
    "        pred_freq = torch.fft.rfft2(pred, dim=(-2, -1), norm=\"ortho\")\n",
    "        target_freq = torch.fft.rfft2(target, dim=(-2, -1), norm=\"ortho\")\n",
    "        \n",
    "        # Compute L2 distance in frequency domain\n",
    "        # Using both magnitude and phase information\n",
    "        loss_real = F.mse_loss(pred_freq.real, target_freq.real)\n",
    "        loss_imag = F.mse_loss(pred_freq.imag, target_freq.imag)\n",
    "        \n",
    "        return loss_real + loss_imag\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test SpectralGating\n",
    "    batch_size, channels, height, width = 2, 64, 64, 64\n",
    "    x = torch.randn(batch_size, channels, height, width)\n",
    "    \n",
    "    spec_gate = SpectralGating(channels, height, width)\n",
    "    output = spec_gate(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Module parameters: {sum(p.numel() for p in spec_gate.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d867eeb",
   "metadata": {},
   "source": [
    "### 2.3 Monogenic Signal Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7daa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class RieszTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, epsilon: float = 1e-8):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Create frequency grid\n",
    "        freq_y = torch.fft.fftfreq(H, device=x.device, dtype=x.dtype)\n",
    "        freq_x = torch.fft.fftfreq(W, device=x.device, dtype=x.dtype)\n",
    "        freq_y, freq_x = torch.meshgrid(freq_y, freq_x, indexing='ij')\n",
    "        \n",
    "        # Compute radial frequency (avoid division by zero)\n",
    "        radius = torch.sqrt(freq_x**2 + freq_y**2 + self.epsilon)\n",
    "        \n",
    "        # Riesz kernels in frequency domain\n",
    "        # H1 = -j * u / |w|, H2 = -j * v / |w|\n",
    "        kernel_x = freq_x / radius\n",
    "        kernel_y = freq_y / radius\n",
    "        \n",
    "        # Set DC component to zero\n",
    "        kernel_x[0, 0] = 0\n",
    "        kernel_y[0, 0] = 0\n",
    "        \n",
    "        # Apply FFT to input\n",
    "        x_fft = torch.fft.fft2(x)\n",
    "        \n",
    "        # Apply Riesz kernels (multiplication by -j in frequency = Hilbert-like)\n",
    "        # -j * X = real(X) * (-j) + imag(X) * (-j) * j = imag(X) - j*real(X)\n",
    "        riesz_x_fft = -1j * x_fft * kernel_x.unsqueeze(0).unsqueeze(0)\n",
    "        riesz_y_fft = -1j * x_fft * kernel_y.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Inverse FFT\n",
    "        riesz_x = torch.fft.ifft2(riesz_x_fft).real\n",
    "        riesz_y = torch.fft.ifft2(riesz_y_fft).real\n",
    "        \n",
    "        return riesz_x, riesz_y\n",
    "\n",
    "class LogGaborFilter(nn.Module):\n",
    "\n",
    "    def __init__(self, num_scales: int = 4, num_orientations: int = 6,\n",
    "                 min_wavelength: float = 3.0, mult: float = 2.1,\n",
    "                 sigma_on_f: float = 0.55):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_scales = num_scales\n",
    "        self.num_orientations = num_orientations\n",
    "        self.min_wavelength = min_wavelength\n",
    "        self.mult = mult\n",
    "        self.sigma_on_f = sigma_on_f\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H, W = x.shape\n",
    "        device = x.device\n",
    "        dtype = x.dtype\n",
    "        \n",
    "        # Create frequency grid\n",
    "        freq_y = torch.fft.fftfreq(H, device=device, dtype=dtype)\n",
    "        freq_x = torch.fft.fftfreq(W, device=device, dtype=dtype)\n",
    "        freq_y, freq_x = torch.meshgrid(freq_y, freq_x, indexing='ij')\n",
    "        \n",
    "        # Polar coordinates\n",
    "        radius = torch.sqrt(freq_x**2 + freq_y**2)\n",
    "        radius[0, 0] = 1  # Avoid log(0)\n",
    "        theta = torch.atan2(freq_y, freq_x)\n",
    "        \n",
    "        # FFT of input\n",
    "        x_fft = torch.fft.fft2(x)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for scale in range(self.num_scales):\n",
    "            wavelength = self.min_wavelength * (self.mult ** scale)\n",
    "            fo = 1.0 / wavelength  # Center frequency\n",
    "            \n",
    "            # Log-Gabor radial component\n",
    "            log_gabor_radial = torch.exp(\n",
    "                -(torch.log(radius / fo) ** 2) / (2 * math.log(self.sigma_on_f) ** 2)\n",
    "            )\n",
    "            log_gabor_radial[0, 0] = 0  # Zero DC\n",
    "            \n",
    "            for orient in range(self.num_orientations):\n",
    "                angle = orient * math.pi / self.num_orientations\n",
    "                \n",
    "                # Angular component\n",
    "                ds = torch.sin(theta - angle)\n",
    "                dc = torch.cos(theta - angle)\n",
    "                dtheta = torch.abs(torch.atan2(ds, dc))\n",
    "                \n",
    "                # Angular spread\n",
    "                angular_spread = torch.exp(\n",
    "                    -(dtheta ** 2) / (2 * (math.pi / self.num_orientations) ** 2)\n",
    "                )\n",
    "                \n",
    "                # Combined filter\n",
    "                log_gabor = log_gabor_radial * angular_spread\n",
    "                \n",
    "                # Apply filter\n",
    "                filtered = torch.fft.ifft2(x_fft * log_gabor.unsqueeze(0).unsqueeze(0))\n",
    "                outputs.append(filtered.abs())\n",
    "        \n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "class MonogenicSignal(nn.Module):\n",
    "\n",
    "    def __init__(self, epsilon: float = 1e-8):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.riesz = RieszTransform(epsilon=epsilon)\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> dict:\n",
    "        \n",
    "        # Get Riesz components\n",
    "        riesz_x, riesz_y = self.riesz(x)\n",
    "        \n",
    "        # Compute amplitude (local energy)\n",
    "        # A = sqrt(f^2 + h1^2 + h2^2)\n",
    "        amplitude = torch.sqrt(x**2 + riesz_x**2 + riesz_y**2 + self.epsilon)\n",
    "        \n",
    "        # Compute orientation\n",
    "        # theta = atan2(h2, h1)\n",
    "        orientation = torch.atan2(riesz_y, riesz_x + self.epsilon)\n",
    "        \n",
    "        # Compute phase\n",
    "        # phi = atan2(sqrt(h1^2 + h2^2), f)\n",
    "        riesz_magnitude = torch.sqrt(riesz_x**2 + riesz_y**2 + self.epsilon)\n",
    "        phase = torch.atan2(riesz_magnitude, x + self.epsilon)\n",
    "        \n",
    "        return {\n",
    "            'amplitude': amplitude,\n",
    "            'phase': phase,\n",
    "            'orientation': orientation,\n",
    "            'riesz_x': riesz_x,\n",
    "            'riesz_y': riesz_y\n",
    "        }\n",
    "\n",
    "class EnergyMap(nn.Module):\n",
    "\n",
    "    def __init__(self, normalize: bool = True, smoothing_sigma: float = 1.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.monogenic = MonogenicSignal()\n",
    "        self.normalize = normalize\n",
    "        self.smoothing_sigma = smoothing_sigma\n",
    "        \n",
    "        # Create Gaussian smoothing kernel\n",
    "        if smoothing_sigma > 0:\n",
    "            kernel_size = int(6 * smoothing_sigma) | 1  # Ensure odd\n",
    "            self.register_buffer('smooth_kernel', self._create_gaussian_kernel(\n",
    "                kernel_size, smoothing_sigma\n",
    "            ))\n",
    "        else:\n",
    "            self.smooth_kernel = None\n",
    "    \n",
    "    def _create_gaussian_kernel(self, kernel_size: int, sigma: float) -> torch.Tensor:\n",
    "        \n",
    "        x = torch.arange(kernel_size) - kernel_size // 2\n",
    "        x = x.float()\n",
    "        gaussian_1d = torch.exp(-x**2 / (2 * sigma**2))\n",
    "        gaussian_2d = gaussian_1d.unsqueeze(0) * gaussian_1d.unsqueeze(1)\n",
    "        gaussian_2d = gaussian_2d / gaussian_2d.sum()\n",
    "        return gaussian_2d.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, dict]:\n",
    "        \n",
    "        # Convert to grayscale if needed\n",
    "        if x.shape[1] > 1:\n",
    "            x = x.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        # Get monogenic decomposition\n",
    "        mono_out = self.monogenic(x)\n",
    "        \n",
    "        # Energy is the amplitude\n",
    "        energy = mono_out['amplitude']\n",
    "        \n",
    "        # Optional smoothing\n",
    "        if self.smooth_kernel is not None:\n",
    "            pad = self.smooth_kernel.shape[-1] // 2\n",
    "            energy = F.conv2d(energy, self.smooth_kernel, padding=pad)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        if self.normalize:\n",
    "            B = energy.shape[0]\n",
    "            energy_flat = energy.view(B, -1)\n",
    "            energy_min = energy_flat.min(dim=1, keepdim=True)[0].view(B, 3, 1, 1)\n",
    "            energy_max = energy_flat.max(dim=1, keepdim=True)[0].view(B, 3, 1, 1)\n",
    "            energy = (energy - energy_min) / (energy_max - energy_min + 1e-8)\n",
    "        \n",
    "        return energy, mono_out\n",
    "\n",
    "class BoundaryDetector(nn.Module):\n",
    "\n",
    "    def __init__(self, num_scales: int = 4, num_orientations: int = 6,\n",
    "                 noise_threshold: float = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.log_gabor = LogGaborFilter(num_scales, num_orientations)\n",
    "        self.num_scales = num_scales\n",
    "        self.num_orientations = num_orientations\n",
    "        self.noise_threshold = noise_threshold\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Get multi-scale responses\n",
    "        responses = self.log_gabor(x)  # (B, S*O, H, W)\n",
    "        \n",
    "        # Sum across orientations to get edge strength per scale\n",
    "        B, _, H, W = responses.shape\n",
    "        responses = responses.view(B, self.num_scales, self.num_orientations, H, W)\n",
    "        \n",
    "        # Max across orientations (strongest edge direction)\n",
    "        edge_strength = responses.max(dim=2)[0]  # (B, S, H, W)\n",
    "        \n",
    "        # Sum across scales\n",
    "        edge_strength = edge_strength.sum(dim=1, keepdim=True)  # (B, 3, H, W)\n",
    "        \n",
    "        # Normalize and threshold\n",
    "        edge_max = edge_strength.view(B, -1).max(dim=1)[0].view(B, 3, 1, 1)\n",
    "        edge_strength = edge_strength / (edge_max + 1e-8)\n",
    "        edge_strength = torch.clamp(edge_strength - self.noise_threshold, min=0)\n",
    "        edge_strength = edge_strength / (1 - self.noise_threshold + 1e-8)\n",
    "        \n",
    "        return edge_strength\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test Monogenic Signal processing\n",
    "    print(\"Testing Monogenic Signal Processing...\")\n",
    "    \n",
    "    # Create test image with edges\n",
    "    H, W = 128, 128\n",
    "    x = torch.zeros(1, 3, H, W)\n",
    "    x[:, :, 32:96, 32:96] = 1.0  # Square\n",
    "    \n",
    "    # Add some noise\n",
    "    x = x + 0.1 * torch.randn_like(x)\n",
    "    \n",
    "    # Test Energy Map\n",
    "    energy_extractor = EnergyMap(normalize=True)\n",
    "    energy, mono = energy_extractor(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Energy map shape: {energy.shape}\")\n",
    "    print(f\"Energy range: [{energy.min():.3f}, {energy.max():.3f}]\")\n",
    "    print(f\"Monogenic components: {list(mono.keys())}\")\n",
    "    \n",
    "    # Test Boundary Detector\n",
    "    boundary_detector = BoundaryDetector()\n",
    "    boundaries = boundary_detector(x)\n",
    "    \n",
    "    print(f\"Boundary map shape: {boundaries.shape}\")\n",
    "    print(f\"Boundary range: [{boundaries.min():.3f}, {boundaries.max():.3f}]\")\n",
    "    \n",
    "    print(\"\\nâœ“ All tests passed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd15abcd",
   "metadata": {},
   "source": [
    "### 2.4 Gabor Implicit Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2b266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "class GaborBasis(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int = 2, num_frequencies: int = 64,\n",
    "                 sigma_range: Tuple[float, float] = (0.1, 2.0),\n",
    "                 freq_range: Tuple[float, float] = (1.0, 10.0),\n",
    "                 learnable: bool = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.output_dim = num_frequencies * 2  # sin and cos components\n",
    "        \n",
    "        # Initialize frequencies uniformly in log space\n",
    "        log_freqs = torch.linspace(\n",
    "            math.log(freq_range[0]), \n",
    "            math.log(freq_range[1]), \n",
    "            num_frequencies\n",
    "        )\n",
    "        freqs = torch.exp(log_freqs)\n",
    "        \n",
    "        # Initialize sigmas (Gaussian envelope widths)\n",
    "        sigmas = torch.linspace(sigma_range[0], sigma_range[1], num_frequencies)\n",
    "        \n",
    "        # Random orientations for 2D\n",
    "        orientations = torch.rand(num_frequencies) * 2 * math.pi\n",
    "        \n",
    "        # Random phases\n",
    "        phases = torch.rand(num_frequencies) * 2 * math.pi\n",
    "        \n",
    "        # Create direction vectors from orientations\n",
    "        directions = torch.stack([\n",
    "            torch.cos(orientations),\n",
    "            torch.sin(orientations)\n",
    "        ], dim=-1)  # (num_freq, 2)\n",
    "        \n",
    "        if learnable:\n",
    "            self.freqs = nn.Parameter(freqs)\n",
    "            self.sigmas = nn.Parameter(sigmas)\n",
    "            self.directions = nn.Parameter(directions)\n",
    "            self.phases = nn.Parameter(phases)\n",
    "        else:\n",
    "            self.register_buffer('freqs', freqs)\n",
    "            self.register_buffer('sigmas', sigmas)\n",
    "            self.register_buffer('directions', directions)\n",
    "            self.register_buffer('phases', phases)\n",
    "    \n",
    "    def forward(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Normalize directions\n",
    "        directions = F.normalize(self.directions, dim=-1)  # (num_freq, 2)\n",
    "        \n",
    "        # Project coordinates onto directions\n",
    "        # coords: (..., 2), directions: (num_freq, 2)\n",
    "        proj = torch.matmul(coords, directions.T)  # (..., num_freq)\n",
    "        \n",
    "        # Compute Gaussian envelope\n",
    "        # exp(-projÂ² / (2ÏƒÂ²))\n",
    "        sigmas = torch.abs(self.sigmas) + 0.01  # Ensure positive\n",
    "        gaussian = torch.exp(-proj**2 / (2 * sigmas**2 + 1e-8))\n",
    "        \n",
    "        # Compute oscillatory component\n",
    "        # cos(2Ï€fÂ·proj + Ï†), sin(2Ï€fÂ·proj + Ï†)\n",
    "        freqs = torch.abs(self.freqs) + 0.1  # Ensure positive\n",
    "        arg = 2 * math.pi * freqs * proj + self.phases\n",
    "        \n",
    "        cos_comp = gaussian * torch.cos(arg)\n",
    "        sin_comp = gaussian * torch.sin(arg)\n",
    "        \n",
    "        # Concatenate sin and cos\n",
    "        gabor_features = torch.cat([cos_comp, sin_comp], dim=-1)\n",
    "        \n",
    "        return gabor_features\n",
    "\n",
    "class FourierFeatures(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int = 2, num_frequencies: int = 64,\n",
    "                 scale: float = 10.0, learnable: bool = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.output_dim = num_frequencies * 2\n",
    "        \n",
    "        # Random frequency matrix\n",
    "        B = torch.randn(input_dim, num_frequencies) * scale\n",
    "        \n",
    "        if learnable:\n",
    "            self.B = nn.Parameter(B)\n",
    "        else:\n",
    "            self.register_buffer('B', B)\n",
    "    \n",
    "    def forward(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Project: coords @ B\n",
    "        proj = 2 * math.pi * torch.matmul(coords, self.B)  # (..., num_freq)\n",
    "        \n",
    "        # Sin and cos\n",
    "        return torch.cat([torch.cos(proj), torch.sin(proj)], dim=-1)\n",
    "\n",
    "class SIRENLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, \n",
    "                 omega_0: float = 30.0, is_first: bool = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.omega_0 = omega_0\n",
    "        self.is_first = is_first\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if self.is_first:\n",
    "                # First layer: uniform in [-1/n, 1/n]\n",
    "                self.linear.weight.uniform_(-1 / self.in_features, \n",
    "                                            1 / self.in_features)\n",
    "            else:\n",
    "                # Other layers: uniform in [-sqrt(6/n)/Ï‰â‚€, sqrt(6/n)/Ï‰â‚€]\n",
    "                bound = math.sqrt(6 / self.in_features) / self.omega_0\n",
    "                self.linear.weight.uniform_(-bound, bound)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sin(self.omega_0 * self.linear(x))\n",
    "\n",
    "class GaborNet(nn.Module):\n",
    "\n",
    "    def __init__(self, coord_dim: int = 2, feature_dim: int = 256,\n",
    "                 hidden_dim: int = 256, output_dim: int = 1,\n",
    "                 num_layers: int = 4, num_frequencies: int = 64,\n",
    "                 use_gabor: bool = True, omega_0: float = 30.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Coordinate encoding\n",
    "        if use_gabor:\n",
    "            self.coord_encoder = GaborBasis(\n",
    "                input_dim=coord_dim,\n",
    "                num_frequencies=num_frequencies,\n",
    "                learnable=True\n",
    "            )\n",
    "        else:\n",
    "            self.coord_encoder = FourierFeatures(\n",
    "                input_dim=coord_dim,\n",
    "                num_frequencies=num_frequencies,\n",
    "                learnable=False\n",
    "            )\n",
    "        \n",
    "        coord_encoded_dim = self.coord_encoder.output_dim\n",
    "        \n",
    "        # Input dimension: encoded coords + features\n",
    "        input_dim = coord_encoded_dim + feature_dim\n",
    "        \n",
    "        # Build SIREN network\n",
    "        layers = []\n",
    "        \n",
    "        # First layer\n",
    "        layers.append(SIRENLayer(input_dim, hidden_dim, omega_0, is_first=True))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(SIRENLayer(hidden_dim, hidden_dim, omega_0, is_first=False))\n",
    "        \n",
    "        # Final layer (linear, no sine activation)\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, coords: torch.Tensor, features: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Encode coordinates\n",
    "        coord_encoded = self.coord_encoder(coords)  # (B, N, coord_encoded_dim)\n",
    "        \n",
    "        # Concatenate with features\n",
    "        x = torch.cat([coord_encoded, features], dim=-1)  # (B, N, input_dim)\n",
    "        \n",
    "        # Pass through network\n",
    "        output = self.network(x)  # (B, N, output_dim)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class ImplicitSegmentationHead(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_channels: int = 64, num_classes: int = 2,\n",
    "                 hidden_dim: int = 256, num_layers: int = 4,\n",
    "                 num_frequencies: int = 64, use_gabor: bool = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_channels = feature_channels\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Feature projector (reduce channel dimension)\n",
    "        self.feature_proj = nn.Sequential(\n",
    "            nn.Conv2d(feature_channels, hidden_dim, kernel_size=1),\n",
    "            nn.GroupNorm(32, hidden_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Implicit decoder\n",
    "        self.implicit_decoder = GaborNet(\n",
    "            coord_dim=2,\n",
    "            feature_dim=hidden_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=num_classes,\n",
    "            num_layers=num_layers,\n",
    "            num_frequencies=num_frequencies,\n",
    "            use_gabor=use_gabor\n",
    "        )\n",
    "    \n",
    "    def sample_features(self, feature_map: torch.Tensor, \n",
    "                       coords: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H, W = feature_map.shape\n",
    "        N = coords.shape[1]\n",
    "        \n",
    "        # Reshape coords for grid_sample: (B, N, 3, 2) -> (B, 3, N, 2)\n",
    "        # grid_sample expects (B, H, W, 2) where last dim is (x, y)\n",
    "        grid = coords.view(B, 3, N, 2)\n",
    "        \n",
    "        # Sample using bilinear interpolation\n",
    "        # feature_map: (B, C, H, W), grid: (B, 3, N, 2)\n",
    "        # output: (B, C, 3, N)\n",
    "        sampled = F.grid_sample(\n",
    "            feature_map, grid,\n",
    "            mode='bilinear',\n",
    "            padding_mode='border',\n",
    "            align_corners=True\n",
    "        )\n",
    "        \n",
    "        # Reshape: (B, C, 3, N) -> (B, N, C)\n",
    "        sampled = sampled.squeeze(2).permute(0, 2, 1)\n",
    "        \n",
    "        return sampled\n",
    "    \n",
    "    def forward(self, feature_map: torch.Tensor, \n",
    "                coords: Optional[torch.Tensor] = None,\n",
    "                output_size: Optional[Tuple[int, int]] = None) -> torch.Tensor:\n",
    "        \n",
    "        B, C, H_feat, W_feat = feature_map.shape\n",
    "        device = feature_map.device\n",
    "        \n",
    "        # Project features\n",
    "        feature_map = self.feature_proj(feature_map)  # (B, hidden_dim, H, W)\n",
    "        \n",
    "        # Generate coordinates if not provided\n",
    "        if coords is None:\n",
    "            if output_size is None:\n",
    "                output_size = (H_feat * 4, W_feat * 4)\n",
    "            \n",
    "            H_out, W_out = output_size\n",
    "            \n",
    "            # Create normalized coordinate grid [-1, 1]\n",
    "            y = torch.linspace(-1, 3, H_out, device=device)\n",
    "            x = torch.linspace(-1, 3, W_out, device=device)\n",
    "            yy, xx = torch.meshgrid(y, x, indexing='ij')\n",
    "            coords = torch.stack([xx, yy], dim=-1)  # (H_out, W_out, 2)\n",
    "            coords = coords.view(1, -1, 2).expand(B, -1, -1)  # (B, H*W, 2)\n",
    "            \n",
    "            reshape_output = True\n",
    "        else:\n",
    "            reshape_output = False\n",
    "            H_out, W_out = None, None\n",
    "        \n",
    "        # Sample features at coordinates\n",
    "        features = self.sample_features(feature_map, coords)  # (B, N, hidden_dim)\n",
    "        \n",
    "        # Implicit decoding\n",
    "        logits = self.implicit_decoder(coords, features)  # (B, N, num_classes)\n",
    "        \n",
    "        # Reshape to image if using grid\n",
    "        if reshape_output:\n",
    "            logits = logits.view(B, H_out, W_out, self.num_classes)\n",
    "            logits = logits.permute(0, 3, 3, 2)  # (B, C, H, W)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing Gabor Implicit Modules...\")\n",
    "    \n",
    "    # Test Gabor Basis\n",
    "    print(\"\\n[1] Testing GaborBasis...\")\n",
    "    gabor = GaborBasis(input_dim=2, num_frequencies=32)\n",
    "    coords = torch.randn(4, 100, 2)  # (B, N, 2)\n",
    "    encoded = gabor(coords)\n",
    "    print(f\"Input coords: {coords.shape}\")\n",
    "    print(f\"Gabor encoded: {encoded.shape}\")\n",
    "    \n",
    "    # Test GaborNet\n",
    "    print(\"\\n[2] Testing GaborNet...\")\n",
    "    net = GaborNet(coord_dim=2, feature_dim=64, hidden_dim=128, \n",
    "                   output_dim=3, num_layers=3, num_frequencies=32)\n",
    "    features = torch.randn(4, 100, 64)\n",
    "    output = net(coords, features)\n",
    "    print(f\"GaborNet output: {output.shape}\")\n",
    "    \n",
    "    # Test ImplicitSegmentationHead\n",
    "    print(\"\\n[3] Testing ImplicitSegmentationHead...\")\n",
    "    head = ImplicitSegmentationHead(\n",
    "        feature_channels=64, num_classes=3,\n",
    "        hidden_dim=128, num_layers=3, num_frequencies=32\n",
    "    )\n",
    "    feature_map = torch.randn(2, 64, 32, 32)\n",
    "    \n",
    "    # Test with automatic grid\n",
    "    seg_output = head(feature_map, output_size=(128, 128))\n",
    "    print(f\"Feature map: {feature_map.shape}\")\n",
    "    print(f\"Segmentation output (grid): {seg_output.shape}\")\n",
    "    \n",
    "    # Test with custom coordinates\n",
    "    custom_coords = torch.rand(2, 500, 2) * 2 - 1  # Random points in [-1, 1]\n",
    "    seg_points = head(feature_map, coords=custom_coords)\n",
    "    print(f\"Segmentation output (points): {seg_points.shape}\")\n",
    "    \n",
    "    print(\"\\nâœ“ All tests passed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14d5fe",
   "metadata": {},
   "source": [
    "### 2.5 Spectral Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1093c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, List\n",
    "\n",
    "class SpectralVSSBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, height: int, width: int,\n",
    "                 depth: int = 2, expansion_ratio: float = 2.0,\n",
    "                 threshold: float = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # Branch A: Spatial path (VSS Blocks)\n",
    "        self.vss_blocks = MambaBlockStack(\n",
    "            channels, depth=depth, \n",
    "            expansion_ratio=expansion_ratio, \n",
    "            scan_dim=min(64, channels)\n",
    "        )\n",
    "        \n",
    "        # Branch B: Spectral path (FFT-based filtering)\n",
    "        self.spectral_gate = SpectralGating(\n",
    "            channels, height, width, \n",
    "            threshold=threshold, \n",
    "            complex_init=\"kaiming\"\n",
    "        )\n",
    "        \n",
    "        # Fusion layer (learnable weighting)\n",
    "        self.fusion_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Branch A: Spatial context (VSS)\n",
    "        spatial_out = self.vss_blocks(x)\n",
    "        \n",
    "        # Branch B: Frequency filtering (Spectral)\n",
    "        spectral_out = self.spectral_gate(x)\n",
    "        \n",
    "        # Learnable fusion with sigmoid weight\n",
    "        weight = torch.sigmoid(self.fusion_weight)\n",
    "        output = weight * spatial_out + (1 - weight) * spectral_out\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 3, out_channels: int = 64, \n",
    "                 patch_size: int = 4):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels,\n",
    "            kernel_size=patch_size, stride=patch_size, bias=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, H, W, C).permute(0, 3, 3, 2)\n",
    "        return x\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, out_channels: Optional[int] = None):\n",
    "        \n",
    "        super().__init__()\n",
    "        out_channels = out_channels or channels * 2\n",
    "        self.conv = nn.Conv2d(channels, out_channels, kernel_size=2, \n",
    "                              stride=2, bias=True)\n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, H, W, C).permute(0, 3, 3, 2)\n",
    "        return x\n",
    "\n",
    "class PatchExpanding(nn.Module):\n",
    "\n",
    "    def __init__(self, channels: int, out_channels: Optional[int] = None):\n",
    "        \n",
    "        super().__init__()\n",
    "        out_channels = out_channels or channels // 2\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', \n",
    "                                    align_corners=True)\n",
    "        self.conv = nn.Conv2d(channels, out_channels, kernel_size=1, bias=True)\n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, H, W, C).permute(0, 3, 3, 2)\n",
    "        return x\n",
    "\n",
    "class SpectralVMUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 1, out_channels: int = 3,\n",
    "                 img_size: int = 256, base_channels: int = 64,\n",
    "                 num_stages: int = 4, depth: int = 2):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.img_size = img_size\n",
    "        self.base_channels = base_channels\n",
    "        self.num_stages = num_stages\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(in_channels, base_channels, patch_size=4)\n",
    "        initial_size = img_size // 4\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_stages):\n",
    "            in_ch = base_channels * (2 ** i)\n",
    "            out_ch = in_ch\n",
    "            h = w = initial_size // (2 ** i)\n",
    "            \n",
    "            # SpectralVSSBlock\n",
    "            block = SpectralVSSBlock(\n",
    "                in_ch, h, w, depth=depth, expansion_ratio=2.0, threshold=0.1\n",
    "            )\n",
    "            self.encoder_blocks.append(block)\n",
    "            \n",
    "            # Downsampling (except after last encoder block)\n",
    "            if i < num_stages - 1:\n",
    "                down = PatchMerging(in_ch, in_ch * 2)\n",
    "                self.downsample_layers.append(down)\n",
    "        \n",
    "        # Bottleneck - uses the last encoder's output channels\n",
    "        # After num_stages-1 downsamplings, channels = base_channels * 2^(num_stages-1)\n",
    "        bottleneck_ch = base_channels * (2 ** (num_stages - 1))\n",
    "        bottleneck_h = bottleneck_w = initial_size // (2 ** (num_stages - 1))\n",
    "        self.bottleneck = SpectralVSSBlock(\n",
    "            bottleneck_ch, bottleneck_h, bottleneck_w,\n",
    "            depth=depth + 1, expansion_ratio=2.0, threshold=0.1\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        # We have num_stages - 1 decoder stages (matching skip connections)\n",
    "        # Each decoder stage: upsample -> concat with skip -> fusion -> SpectralVSSBlock\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        self.upsample_layers = nn.ModuleList()\n",
    "        \n",
    "        num_decoder_stages = num_stages - 1\n",
    "        \n",
    "        for i in range(num_decoder_stages):\n",
    "            # Going from deepest to shallowest\n",
    "            # i=0: from bottleneck (8x8, 512ch) -> upsample to (16x16, 256ch)\n",
    "            # i=1: from 16x16, 256ch -> upsample to (32x32, 128ch)\n",
    "            # i=2: from 32x32, 128ch -> upsample to (64x64, 64ch)\n",
    "            \n",
    "            # Input channels: for i=0, it's bottleneck_ch; else from previous decoder output\n",
    "            if i == 0:\n",
    "                in_ch = bottleneck_ch  # 512 for default\n",
    "            else:\n",
    "                in_ch = base_channels * (2 ** (num_stages - 1 - i))\n",
    "            \n",
    "            # Output channels after upsampling\n",
    "            out_ch = base_channels * (2 ** (num_stages - 2 - i))\n",
    "            \n",
    "            # Upsampling layer\n",
    "            up = PatchExpanding(in_ch, out_ch)\n",
    "            self.upsample_layers.append(up)\n",
    "            \n",
    "            # Skip connection comes from encoder at level (num_decoder_stages - 1 - i)\n",
    "            # which has same spatial size after upsampling\n",
    "            skip_ch = out_ch  # Skip has same channels as upsampled output\n",
    "            \n",
    "            # Spatial size at this level\n",
    "            h = w = initial_size // (2 ** (num_stages - 2 - i))\n",
    "            \n",
    "            # Fusion: concatenate upsampled + skip, then reduce channels\n",
    "            fused_ch = out_ch + skip_ch  # After concatenation\n",
    "            fusion = nn.Sequential(\n",
    "                nn.Conv2d(fused_ch, out_ch, kernel_size=1, bias=True),\n",
    "                nn.GroupNorm(num_groups=min(32, out_ch), num_channels=out_ch, eps=1e-6)\n",
    "            )\n",
    "            self.decoder_blocks.append(fusion)\n",
    "            \n",
    "            # SpectralVSSBlock after fusion\n",
    "            vss = SpectralVSSBlock(\n",
    "                out_ch, h, w, depth=depth, expansion_ratio=2.0, threshold=0.1\n",
    "            )\n",
    "            self.decoder_blocks.append(vss)\n",
    "        \n",
    "        # Segmentation head\n",
    "        self.seg_head = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels // 2, kernel_size=3, \n",
    "                      padding=1, bias=True),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=base_channels // 2, eps=1e-6),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(base_channels // 2, out_channels, kernel_size=1, bias=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Encoder path with skip connections storage\n",
    "        # Skip connections are saved BEFORE downsampling\n",
    "        skips = []\n",
    "        for i in range(self.num_stages):\n",
    "            x = self.encoder_blocks[i](x)\n",
    "            # Save skip connection before downsampling\n",
    "            if i < self.num_stages - 1:\n",
    "                skips.append(x)\n",
    "                x = self.downsample_layers[i](x)\n",
    "        \n",
    "        # The last encoder output goes to bottleneck (no skip for this level)\n",
    "        # skips now contains: [stage0_out, stage1_out, stage2_out] for 4 stages\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        # Decoder stages: num_stages - 1 (since last encoder has no skip)\n",
    "        num_decoder_stages = self.num_stages - 1\n",
    "        \n",
    "        for i in range(num_decoder_stages):\n",
    "            # Upsample\n",
    "            x = self.upsample_layers[i](x)\n",
    "            \n",
    "            # Concatenate skip connection (in reverse order)\n",
    "            # For i=0: skip from encoder stage num_stages-2 (last skip)\n",
    "            # For i=1: skip from encoder stage num_stages-3\n",
    "            skip_idx = num_decoder_stages - 1 - i\n",
    "            skip = skips[skip_idx]\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            \n",
    "            # Fusion and processing\n",
    "            x = self.decoder_blocks[2 * i](x)  # Fusion conv\n",
    "            x = self.decoder_blocks[2 * i + 1](x)  # SpectralVSSBlock\n",
    "        \n",
    "        # Segmentation head\n",
    "        output = self.seg_head(x)\n",
    "        \n",
    "        # Upsample to original resolution (since patch embedding uses stride 4)\n",
    "        output = F.interpolate(output, size=(self.img_size, self.img_size),\n",
    "                               mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the full architecture\n",
    "    batch_size = 2\n",
    "    in_channels = 1\n",
    "    out_channels = 3  # Binary segmentation + background\n",
    "    img_size = 256\n",
    "    \n",
    "    model = SpectralVMUNet(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        img_size=img_size,\n",
    "        base_channels=64,\n",
    "        num_stages=4,\n",
    "        depth=2\n",
    "    )\n",
    "    \n",
    "    # Create dummy input\n",
    "    x = torch.randn(batch_size, in_channels, img_size, img_size)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72355acc",
   "metadata": {},
   "source": [
    "### 2.6 EGM-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b729a20f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39339369",
   "metadata": {},
   "source": [
    "### 2.7 Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6472c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, smooth: float = 1e-5, reduction: str = \"mean\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Convert logits to probabilities\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        \n",
    "        # Ensure target has same shape as pred for multi-class\n",
    "        if target.ndim == 3:  # (B, H, W) -> convert to one-hot\n",
    "            target = F.one_hot(target.long(), num_classes=pred.shape[1])\n",
    "            target = target.permute(0, 3, 3, 2).float()\n",
    "        \n",
    "        # Flatten spatial dimensions\n",
    "        pred = pred.view(pred.shape[0], pred.shape[1], -1)\n",
    "        target = target.view(target.shape[0], target.shape[1], -1)\n",
    "        \n",
    "        # Compute Dice score\n",
    "        intersection = torch.sum(pred * target, dim=2)\n",
    "        union = torch.sum(pred, dim=2) + torch.sum(target, dim=2)\n",
    "        \n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        # Return loss (1 - Dice)\n",
    "        loss = 1.0 - dice\n",
    "        \n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha: float = 0.25, gamma: float = 2.0,\n",
    "                 reduction: str = \"mean\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Get class probabilities\n",
    "        p = torch.softmax(pred, dim=1)\n",
    "        \n",
    "        # Get class log probabilities\n",
    "        ce = F.cross_entropy(pred, target.long(), reduction='none')\n",
    "        \n",
    "        # Get probability of true class\n",
    "        p_t = torch.gather(p, 3, target.long().unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute focal loss\n",
    "        focal_weight = (1.0 - p_t) ** self.gamma\n",
    "        focal_loss = focal_weight * ce\n",
    "        \n",
    "        if self.reduction == \"mean\":\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class FrequencyLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, weight: float = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Ensure both have batch and channel dimensions\n",
    "        if pred.ndim == 3:\n",
    "            pred = pred.unsqueeze(1)\n",
    "        if target.ndim == 3:\n",
    "            target = target.unsqueeze(1)\n",
    "        \n",
    "        # Flatten to single channel for FFT comparison\n",
    "        if pred.shape[1] > 1:\n",
    "            # For multi-channel, convert to grayscale by averaging\n",
    "            pred = pred.mean(dim=1, keepdim=True)\n",
    "        if target.shape[1] > 1:\n",
    "            target = target.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        # Apply FFT to convert to frequency domain\n",
    "        pred_freq = torch.fft.rfft2(pred, dim=(-2, -1), norm=\"ortho\")\n",
    "        target_freq = torch.fft.rfft2(target, dim=(-2, -1), norm=\"ortho\")\n",
    "        \n",
    "        # Compute L2 distance in frequency domain\n",
    "        # Consider both magnitude and phase information\n",
    "        loss_real = F.mse_loss(pred_freq.real, target_freq.real, reduction='mean')\n",
    "        loss_imag = F.mse_loss(pred_freq.imag, target_freq.imag, reduction='mean')\n",
    "        \n",
    "        return loss_real + loss_imag\n",
    "\n",
    "class SpectralDualLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, spatial_weight: float = 1.0, freq_weight: float = 0.1,\n",
    "                 use_dice: bool = True, use_focal: bool = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.spatial_weight = spatial_weight\n",
    "        self.freq_weight = freq_weight\n",
    "        self.use_dice = use_dice\n",
    "        self.use_focal = use_focal\n",
    "        \n",
    "        # Spatial losses\n",
    "        if use_dice:\n",
    "            self.dice_loss = DiceLoss(smooth=1e-5)\n",
    "        \n",
    "        if use_focal:\n",
    "            self.focal_loss = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "        else:\n",
    "            self.ce_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Frequency loss\n",
    "        self.freq_loss = FrequencyLoss(weight=freq_weight)\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor,\n",
    "                return_components: bool = False) -> torch.Tensor:\n",
    "        \n",
    "        # Ensure target is on same device as pred\n",
    "        target = target.to(pred.device)\n",
    "        \n",
    "        # Spatial losses\n",
    "        spatial_loss = 0.0\n",
    "        losses_dict = {}\n",
    "        \n",
    "        if self.use_dice:\n",
    "            dice = self.dice_loss(pred, target)\n",
    "            spatial_loss = spatial_loss + dice\n",
    "            losses_dict['dice'] = dice.item()\n",
    "        \n",
    "        if self.use_focal:\n",
    "            focal = self.focal_loss(pred, target)\n",
    "            spatial_loss = spatial_loss + focal\n",
    "            losses_dict['focal'] = focal.item()\n",
    "        else:\n",
    "            ce = self.ce_loss(pred, target)\n",
    "            spatial_loss = spatial_loss + ce\n",
    "            losses_dict['ce'] = ce.item()\n",
    "        \n",
    "        # Frequency loss\n",
    "        # For frequency loss, we need to extract the predicted class (argmax) and compare\n",
    "        pred_probs = torch.softmax(pred, dim=1)\n",
    "        pred_class = torch.argmax(pred_probs, dim=1)  # (B, H, W)\n",
    "        \n",
    "        freq = self.freq_loss(pred_class.float(), target.float())\n",
    "        losses_dict['freq'] = freq.item()\n",
    "        \n",
    "        # Weighted combination\n",
    "        total_loss = (self.spatial_weight * spatial_loss + \n",
    "                     self.freq_weight * freq)\n",
    "        losses_dict['total'] = total_loss.item()\n",
    "        \n",
    "        if return_components:\n",
    "            return total_loss, losses_dict\n",
    "        else:\n",
    "            return total_loss\n",
    "\n",
    "class BoundaryAwareLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, kernel_size: int = 3, weight: float = 1.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.weight = weight\n",
    "    \n",
    "    def _compute_boundaries(self, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Convert to float\n",
    "        mask = mask.float().unsqueeze(1)  # (B, 3, H, W)\n",
    "        \n",
    "        # Compute gradients using Sobel-like operation\n",
    "        kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],\n",
    "                                dtype=mask.dtype, device=mask.device)\n",
    "        kernel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]],\n",
    "                                dtype=mask.dtype, device=mask.device)\n",
    "        \n",
    "        kernel_x = kernel_x.view(1, 3, 3, 3)\n",
    "        kernel_y = kernel_y.view(1, 3, 3, 3)\n",
    "        \n",
    "        grad_x = F.conv2d(mask, kernel_x, padding=1)\n",
    "        grad_y = F.conv2d(mask, kernel_y, padding=1)\n",
    "        \n",
    "        # Compute magnitude of gradient\n",
    "        grad_magnitude = torch.sqrt(grad_x ** 2 + grad_y ** 2 + 1e-8)\n",
    "        \n",
    "        # Threshold to get boundary pixels\n",
    "        boundary = (grad_magnitude > 0).float().squeeze(1)\n",
    "        \n",
    "        return boundary\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Get predicted class\n",
    "        pred_probs = torch.softmax(pred, dim=1)\n",
    "        pred_class = torch.argmax(pred_probs, dim=1)  # (B, H, W)\n",
    "        \n",
    "        # Compute boundary maps\n",
    "        pred_boundary = self._compute_boundaries(pred_class)\n",
    "        target_boundary = self._compute_boundaries(target)\n",
    "        \n",
    "        # Compute cross-entropy loss weighted by boundary\n",
    "        ce_loss = F.cross_entropy(pred, target.long(), reduction='none')\n",
    "        \n",
    "        # Apply boundary weight (higher loss for boundary pixels)\n",
    "        boundary_weight = (pred_boundary + target_boundary).clamp(0, 1)\n",
    "        boundary_weight = 1.0 + boundary_weight  # Weight between 1 and 2\n",
    "        \n",
    "        weighted_loss = ce_loss * boundary_weight\n",
    "        \n",
    "        return weighted_loss.mean()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test losses\n",
    "    batch_size, num_classes, height, width = 2, 3, 64, 64\n",
    "    \n",
    "    # Create dummy predictions and targets\n",
    "    pred = torch.randn(batch_size, num_classes, height, width)\n",
    "    target = torch.randint(0, num_classes, (batch_size, height, width))\n",
    "    \n",
    "    # Test SpectralDualLoss\n",
    "    loss_fn = SpectralDualLoss(spatial_weight=1.0, freq_weight=0.1)\n",
    "    loss, components = loss_fn(pred, target, return_components=True)\n",
    "    \n",
    "    print(f\"Total Loss: {loss.item():.4f}\")\n",
    "    for name, value in components.items():\n",
    "        print(f\"  {name}: {value:.4f}\")\n",
    "    \n",
    "    # Test BoundaryAwareLoss\n",
    "    boundary_loss_fn = BoundaryAwareLoss()\n",
    "    boundary_loss = boundary_loss_fn(pred, target)\n",
    "    print(f\"\\nBoundary Loss: {boundary_loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a78b3b",
   "metadata": {},
   "source": [
    "### 2.8 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad17bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from monai.metrics import compute_hausdorff_distance\n",
    "\n",
    "def count_parameters(model):\n",
    "    \n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class SegmentationMetrics:\n",
    "    def __init__(self, num_classes, device):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.batches = 0\n",
    "        self.total_correct_pixels = 0\n",
    "        self.total_pixels = 0\n",
    "        \n",
    "        # Aggregated stats for Precision/Recall/F1 (Global)\n",
    "        self.tp = torch.zeros(self.num_classes, device=self.device)\n",
    "        self.fp = torch.zeros(self.num_classes, device=self.device)\n",
    "        self.fn = torch.zeros(self.num_classes, device=self.device)\n",
    "        \n",
    "        # Accumulators for averaging Batch-wise metrics\n",
    "        self.dice_sum = torch.zeros(self.num_classes, device=self.device)\n",
    "        self.iou_sum = torch.zeros(self.num_classes, device=self.device)\n",
    "        self.hd95_sum = torch.zeros(self.num_classes, device=self.device)\n",
    "        \n",
    "        # Track valid batches for HD95 (it can be NaN if class is missing)\n",
    "        self.hd95_counts = torch.zeros(self.num_classes, device=self.device)\n",
    "\n",
    "    def update(self, preds, targets):\n",
    "        \n",
    "        self.batches += 1\n",
    "        \n",
    "        # Accuracy\n",
    "        self.total_correct_pixels += (preds == targets).sum().item()\n",
    "        self.total_pixels += targets.numel()\n",
    "        \n",
    "        # Create one-hot for HD95 and Dice\n",
    "        # preds_oh: (B, C, H, W)\n",
    "        preds_oh = F.one_hot(preds, num_classes=self.num_classes).permute(0, 3, 1, 2).float()\n",
    "        targets_oh = F.one_hot(targets, num_classes=self.num_classes).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        # Helper for Dice/IoU/TP/FP/FN\n",
    "        for c in range(self.num_classes):\n",
    "            p_flat = preds_oh[:, c].reshape(-1)\n",
    "            t_flat = targets_oh[:, c].reshape(-1)\n",
    "            \n",
    "            intersection = (p_flat * t_flat).sum()\n",
    "            union = p_flat.sum() + t_flat.sum()\n",
    "            \n",
    "            # Global TP/FP/FN accumulation\n",
    "            self.tp[c] += intersection\n",
    "            self.fp[c] += (p_flat.sum() - intersection)\n",
    "            self.fn[c] += (t_flat.sum() - intersection)\n",
    "            \n",
    "            # Batch-wise Dice/IoU accumulation\n",
    "            dice = (2. * intersection + 1e-6) / (union + 1e-6)\n",
    "            iou = (intersection + 1e-6) / (union - intersection + 1e-6)\n",
    "            \n",
    "            self.dice_sum[c] += dice\n",
    "            self.iou_sum[c] += iou\n",
    "            \n",
    "        # HD95 Compliance (MONAI)\n",
    "        # compute_hausdorff_distance expects (B, C, spatial...)\n",
    "        # include_background=True usually, but we iterate.\n",
    "        # We can compute all classes at once.\n",
    "        try:\n",
    "            import warnings\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                \n",
    "                # percentile=95\n",
    "                hd95_batch = compute_hausdorff_distance(\n",
    "                    y_pred=preds_oh, \n",
    "                    y=targets_oh, \n",
    "                    include_background=True, \n",
    "                    percentile=95.0,\n",
    "                    spacing=None  # Pixel space\n",
    "                )\n",
    "            # hd95_batch is (B, C)\n",
    "            \n",
    "            for c in range(self.num_classes):\n",
    "                # Filter NaNs/Infs (happens if class missing in both pred and target, or just one)\n",
    "                # MONAI returns NaN if one is empty. We mostly care if target exists.\n",
    "                # Common practice: if target is empty, skip. If target exists but pred empty, HD is high (inf).\n",
    "                # MONAI behavior: Nan if both empty. Inf if one empty.\n",
    "                \n",
    "                valid_vals = hd95_batch[:, c]\n",
    "                valid_mask = ~torch.isnan(valid_vals) & ~torch.isinf(valid_vals)\n",
    "                \n",
    "                if valid_mask.any():\n",
    "                    self.hd95_sum[c] += valid_vals[valid_mask].sum()\n",
    "                    self.hd95_counts[c] += valid_mask.sum()\n",
    "                    \n",
    "        except Exception as e:\n",
    "            # Fallback or strict error? \n",
    "            # Often happens if shapes are weird or empty batch.\n",
    "            pass\n",
    "\n",
    "    def compute(self):\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Global Accuracy\n",
    "        metrics['accuracy'] = self.total_correct_pixels / max(self.total_pixels, 1)\n",
    "        \n",
    "        # Per-class metrics\n",
    "        dice_scores = []\n",
    "        iou_scores = []\n",
    "        hd95_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for c in range(self.num_classes):\n",
    "            # Batch-averaged Dice/IoU\n",
    "            dice_scores.append((self.dice_sum[c] / max(self.batches, 1)).item())\n",
    "            iou_scores.append((self.iou_sum[c] / max(self.batches, 1)).item())\n",
    "            \n",
    "            # Batch-averaged HD95\n",
    "            if self.hd95_counts[c] > 0:\n",
    "                hd95_scores.append((self.hd95_sum[c] / self.hd95_counts[c]).item())\n",
    "            else:\n",
    "                hd95_scores.append(float('nan')) # Or 0.0 or inf\n",
    "            \n",
    "            # Global-based Precision/Recall/F1\n",
    "            p = (self.tp[c] / (self.tp[c] + self.fp[c] + 1e-6)).item()\n",
    "            r = (self.tp[c] / (self.tp[c] + self.fn[c] + 1e-6)).item()\n",
    "            f1 = 2 * p * r / (p + r + 1e-6) if (p + r) > 0 else 0.0\n",
    "            \n",
    "            precision_scores.append(p)\n",
    "            recall_scores.append(r)\n",
    "            f1_scores.append(f1)\n",
    "            \n",
    "        metrics['dice_scores'] = dice_scores\n",
    "        metrics['iou'] = iou_scores\n",
    "        metrics['hd95'] = hd95_scores\n",
    "        metrics['precision'] = precision_scores\n",
    "        metrics['recall'] = recall_scores\n",
    "        metrics['f1_score'] = f1_scores\n",
    "        \n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('âœ… All modules loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0373d79",
   "metadata": {},
   "source": [
    "---\n",
    "## 3ï¸âƒ£ Dataset\n",
    "\n",
    "Download and preprocess ACDC cardiac MRI dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74be26ee",
   "metadata": {},
   "source": [
    "### 3.1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f804fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'ACDC'\n",
    "DRIVE_FOLDER_ID = '1EelzBVjIoDQ4uzt0_2JzmF_PuUHsD93e'\n",
    "RAW_DATA_DIR = f'./data/{DATASET}'\n",
    "PREPROCESSED_DIR = f'./preprocessed_data/{DATASET}'\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 4\n",
    "CLASS_NAMES = ['Background', 'RV', 'Myocardium', 'LV']\n",
    "\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "print(f\"ðŸ“Š Dataset: {DATASET}\")\n",
    "print(f\"   Classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9206647",
   "metadata": {},
   "source": [
    "### 3.2 Download from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0565e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "acdc_path = kagglehub.dataset_download('samdazel/automated-cardiac-diagnosis-challenge-miccai17')\n",
    "\n",
    "print('Data source import complete.')\n",
    "print(f'Data path: {acdc_path}')\n",
    "# Xem cáº¥u trÃºc thÆ° má»¥c\n",
    "import os\n",
    "for item in os.listdir(acdc_path):\n",
    "    print(f'  {item}')\n",
    "\n",
    "\n",
    "RAW_DATA_DIR = os.path.join(acdc_path, 'database')\n",
    "print(f'RAW_DATA_DIR: {RAW_DATA_DIR}')\n",
    "# Xem cáº¥u trÃºc\n",
    "for item in os.listdir(RAW_DATA_DIR):\n",
    "    print(f'  {item}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa89e59",
   "metadata": {},
   "source": [
    "### 3.3 Preprocess (NIfTI â†’ NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd8c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import configparser\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Add project root to path\n",
    "\n",
    "def normalize_intensity(image):\n",
    "    \n",
    "    # Clip outliers\n",
    "    p05 = np.percentile(image, 0.5)\n",
    "    p995 = np.percentile(image, 99.5)\n",
    "    image = np.clip(image, p05, p995)\n",
    "    \n",
    "    # Z-score\n",
    "    mean = np.mean(image)\n",
    "    std = np.std(image)\n",
    "    if std > 0:\n",
    "        return (image - mean) / std\n",
    "    return image\n",
    "\n",
    "def preprocess_single_patient_acdc(patient_path, output_dir, target_size=(224, 224)):\n",
    "    \n",
    "    patient_folder = os.path.basename(patient_path)\n",
    "    info_cfg_path = os.path.join(patient_path, 'Info.cfg')\n",
    "    \n",
    "    # Táº¡o folder output cho slice\n",
    "    img_save_dir = os.path.join(output_dir, 'images')\n",
    "    mask_save_dir = os.path.join(output_dir, 'masks')\n",
    "    os.makedirs(img_save_dir, exist_ok=True)\n",
    "    os.makedirs(mask_save_dir, exist_ok=True)\n",
    "    \n",
    "    # Äá»c config Ä‘á»ƒ biáº¿t frame nÃ o lÃ  ED, frame nÃ o lÃ  ES\n",
    "    if not os.path.exists(info_cfg_path):\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        parser = configparser.ConfigParser()\n",
    "        with open(info_cfg_path, 'r') as f:\n",
    "            config_string = '[DEFAULT]\\n' + f.read()\n",
    "        parser.read_string(config_string)\n",
    "        ed_frame = int(parser['DEFAULT']['ED'])\n",
    "        es_frame = int(parser['DEFAULT']['ES'])\n",
    "    except Exception as e:\n",
    "        print(f\"  Error reading Info.cfg for {patient_folder}: {e}\")\n",
    "        return 0\n",
    "    \n",
    "    slices_saved = 0\n",
    "    \n",
    "    for frame_num, frame_name in [(ed_frame, 'ED'), (es_frame, 'ES')]:\n",
    "        img_filename = f'{patient_folder}_frame{frame_num:02d}.nii.gz'\n",
    "        mask_filename = f'{patient_folder}_frame{frame_num:02d}_gt.nii.gz'\n",
    "        \n",
    "        # TÃ¬m file (support cáº£ .nii vÃ  .nii.gz)\n",
    "        img_path = None\n",
    "        mask_path = None\n",
    "        for suffix in ['.gz', '']:\n",
    "            test_img = os.path.join(patient_path, img_filename.replace('.gz', '') if suffix == '' else img_filename)\n",
    "            test_mask = os.path.join(patient_path, mask_filename.replace('.gz', '') if suffix == '' else mask_filename)\n",
    "            if os.path.exists(test_img):\n",
    "                img_path = test_img\n",
    "                mask_path = test_mask\n",
    "                break\n",
    "        \n",
    "        if img_path is None or not os.path.exists(img_path):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Load NIfTI\n",
    "            img_nii = nib.load(img_path)\n",
    "            img_data = img_nii.get_fdata() # (H, W, D)\n",
    "            \n",
    "            mask_data = None\n",
    "            if os.path.exists(mask_path):\n",
    "                mask_data = nib.load(mask_path).get_fdata()\n",
    "            else:\n",
    "                continue # Bá» qua náº¿u khÃ´ng cÃ³ mask\n",
    "            \n",
    "            # 1. Normalize Intensity TRÆ¯á»šC khi resize (tÃ­nh trÃªn toÃ n volume 3D)\n",
    "            img_data = normalize_intensity(img_data)\n",
    "            \n",
    "            num_slices = img_data.shape[2]\n",
    "            \n",
    "            # 2. Xá»­ lÃ½ tá»«ng slice vÃ  lÆ°u ngay láº­p tá»©c\n",
    "            for i in range(num_slices):\n",
    "                slice_img = img_data[:, :, i]\n",
    "                slice_mask = mask_data[:, :, i]\n",
    "                \n",
    "                # Bá» qua slice Ä‘en thui (khÃ´ng cÃ³ thÃ´ng tin) Ä‘á»ƒ trÃ¡nh nhiá»…u training\n",
    "                if np.sum(slice_img) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Resize (LÆ°u Ã½: resize cá»§a skimage range input=output, Ä‘Ã£ normalize thÃ¬ váº«n giá»¯ range)\n",
    "                slice_img_resized = resize(\n",
    "                    slice_img, target_size, order=1, preserve_range=True, anti_aliasing=True, mode='reflect'\n",
    "                ).astype(np.float32)\n",
    "                \n",
    "                slice_mask_resized = resize(\n",
    "                    slice_mask, target_size, order=0, preserve_range=True, anti_aliasing=False, mode='reflect'\n",
    "                ).astype(np.uint8) # Mask pháº£i lÃ  int\n",
    "                \n",
    "                # Táº¡o tÃªn file: patient001_ED_slice005.npy\n",
    "                file_id = f\"{patient_folder}_{frame_name}_slice{i:03d}\"\n",
    "                \n",
    "                np.save(os.path.join(img_save_dir, f\"{file_id}.npy\"), slice_img_resized)\n",
    "                np.save(os.path.join(mask_save_dir, f\"{file_id}.npy\"), slice_mask_resized)\n",
    "                \n",
    "                slices_saved += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {patient_folder} frame {frame_num}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    return slices_saved\n",
    "\n",
    "def preprocess_acdc_dataset(input_dir, output_dir, target_size=(224, 224)):\n",
    "    \n",
    "    # Láº¥y danh sÃ¡ch patient\n",
    "    patient_folders = sorted([\n",
    "        os.path.join(input_dir, d) \n",
    "        for d in os.listdir(input_dir) \n",
    "        if os.path.isdir(os.path.join(input_dir, d)) and d.startswith('patient')\n",
    "    ])\n",
    "    \n",
    "    print(f\"Found {len(patient_folders)} patients. Outputting 2D slices to {output_dir}\")\n",
    "    \n",
    "    total_slices = 0\n",
    "    \n",
    "    for patient_path in tqdm(patient_folders, desc=\"Processing ACDC\"):\n",
    "        slices = preprocess_single_patient_acdc(patient_path, output_dir, target_size)\n",
    "        total_slices += slices\n",
    "        \n",
    "    print(f\"\\nCompleted! Saved {total_slices} slices total.\")\n",
    "    print(f\"Images: {os.path.join(output_dir, 'images')}\")\n",
    "    print(f\"Masks:  {os.path.join(output_dir, 'masks')}\")\n",
    "\n",
    "\n",
    "training_dir = os.path.join(acdc_path, 'database', 'training')\n",
    "output_dir = './preprocessed_data/ACDC'\n",
    "print(f\"ðŸ“‚ Input: {training_dir}\")\n",
    "print(f\"ðŸ“‚ Output: {output_dir}\")\n",
    "preprocess_acdc_dataset(training_dir, output_dir, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7e6cd",
   "metadata": {},
   "source": [
    "### 3.4 Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8876d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "\n",
    "class ACDCDataset2D(Dataset):\n",
    "    \"\"\"Fast memmap-based dataset for preprocessed 2D slices.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, split='train'):\n",
    "        img_dir = os.path.join(data_dir, 'images')\n",
    "        all_files = sorted(glob.glob(os.path.join(img_dir, '*.npy')))\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        indices = np.random.permutation(len(all_files))\n",
    "        split_idx = int(0.8 * len(all_files))\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.files = [all_files[i] for i in indices[:split_idx]]\n",
    "        else:\n",
    "            self.files = [all_files[i] for i in indices[split_idx:]]\n",
    "        \n",
    "        self.mask_dir = os.path.join(data_dir, 'masks')\n",
    "        print(f\"   {split.upper()}: {len(self.files)} 2D slices\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        mask_path = os.path.join(self.mask_dir, os.path.basename(img_path))\n",
    "        \n",
    "        # memmap for fast disk access\n",
    "        img = np.load(img_path, mmap_mode='r').copy()\n",
    "        seg = np.load(mask_path, mmap_mode='r').copy()\n",
    "        \n",
    "        # Already 2D (H, W) -> add channel dim -> (1, H, W)\n",
    "        return torch.from_numpy(img).unsqueeze(0).float(), torch.from_numpy(seg).long()\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "train_ds = ACDCDataset2D(PREPROCESSED_DIR, 'train')\n",
    "val_ds = ACDCDataset2D(PREPROCESSED_DIR, 'val')\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "print(f\"âœ… Memmap DataLoader ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d238cbc",
   "metadata": {},
   "source": [
    "---\n",
    "## 4ï¸âƒ£ Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9b806",
   "metadata": {},
   "source": [
    "### 4.1 Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EGMNet(use_hrnet=True, in_channels=3, num_classes=NUM_CLASSES, img_size=IMG_SIZE).to(device)\n",
    "print(f\"âœ… EGM-Net: {sum(p.numel() for p in model.parameters()):,} parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b160f",
   "metadata": {},
   "source": [
    "### 4.2 Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d48bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SpectralDualLoss(spatial_weight=1.0, freq_weight=0.1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a9d3f",
   "metadata": {},
   "source": [
    "### 4.3 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45942c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to restart the Kernel. \n",
      "\u001b[1;31mInvalid response: 404 Not Found. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# === TRAINING ===\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "EARLY_STOP_PATIENCE = 20\n",
    "NUM_CLASSES = 4\n",
    "CLASS_NAMES = {0: 'BG', 1: 'RV', 2: 'MYO', 3: 'LV'}\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "model = EGMNet(use_hrnet=True, in_channels=3, num_classes=NUM_CLASSES, img_size=IMG_SIZE).to(device)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "criterion = SpectralDualLoss(spatial_weight=1.0, freq_weight=0.1).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10)\n",
    "scaler = GradScaler()\n",
    "\n",
    "best_dice = 0.0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "            loss = criterion(pred, masks)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model.eval()\n",
    "    val_dice_sum = 0\n",
    "    val_count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "            pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "            pred_cls = pred.argmax(dim=1)\n",
    "            for c in range(1, NUM_CLASSES):\n",
    "                inter = ((pred_cls == c) & (masks == c)).sum()\n",
    "                union = (pred_cls == c).sum() + (masks == c).sum()\n",
    "                val_dice_sum += (2 * inter / (union + 1e-5)).item()\n",
    "            val_count += 1\n",
    "    \n",
    "    avg_dice = val_dice_sum / (val_count * (NUM_CLASSES - 1))\n",
    "    scheduler.step(avg_dice)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss={train_loss/len(train_loader):.4f}, Dice={avg_dice:.4f}\")\n",
    "    \n",
    "    if avg_dice > best_dice:\n",
    "        best_dice = avg_dice\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"   âœ… Best model saved!\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "    \n",
    "    if epochs_no_improve >= EARLY_STOP_PATIENCE:\n",
    "        print(f\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "print(f\"Training complete! Best Dice: {best_dice:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EVALUATE FUNCTION ===\n",
    "def evaluate_metrics(model, dataloader, device, num_classes):\n",
    "    \"\"\"Evaluate model on a dataloader and return metrics.\"\"\"\n",
    "    model.eval()\n",
    "    metrics_tracker = SegmentationMetrics(num_classes=num_classes, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "            pred_cls = pred.argmax(dim=1)\n",
    "            metrics_tracker.update(pred_cls, masks)\n",
    "    \n",
    "    return metrics_tracker.compute()\n",
    "\n",
    "# === FINAL TEST EVALUATION ===\n",
    "def final_evaluation(model, test_loader, device, num_classes, class_names):\n",
    "    \"\"\"Run final evaluation on test set.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š FINAL TEST EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load best model\n",
    "    checkpoint = torch.load('best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "    \n",
    "    metrics = evaluate_metrics(model, test_loader, device, num_classes)\n",
    "    \n",
    "    print(\"\\n--- Per-Class Results ---\")\n",
    "    for c in range(num_classes):\n",
    "        print(f\"   {class_names[c]:<5}: Dice={metrics['dice_scores'][c]:.4f}, \"\n",
    "              f\"IoU={metrics['iou'][c]:.4f}, HD95={metrics['hd95'][c]:.2f}\")\n",
    "    \n",
    "    avg_dice = np.mean(metrics['dice_scores'][1:])\n",
    "    avg_iou = np.mean(metrics['iou'][1:])\n",
    "    avg_hd95 = np.nanmean(metrics['hd95'][1:])\n",
    "    \n",
    "    print(\"\\n--- Summary (Foreground Only) ---\")\n",
    "    print(f\"   Avg Dice: {avg_dice:.4f}\")\n",
    "    print(f\"   Avg IoU:  {avg_iou:.4f}\")\n",
    "    print(f\"   Avg HD95: {avg_hd95:.2f}\")\n",
    "    print(f\"   Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "final_metrics = final_evaluation(model, val_loader, device, NUM_CLASSES, CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4614a502",
   "metadata": {},
   "source": [
    "---\n",
    "## 5ï¸âƒ£ Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "images, masks = next(iter(val_loader))\n",
    "images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    pred = outputs['output'] if isinstance(outputs, dict) else outputs\n",
    "    pred_cls = pred.argmax(dim=1)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "for i in range(4):\n",
    "    axes[0, i].imshow(images[i, 0].cpu(), cmap='gray')\n",
    "    axes[0, i].set_title('Input')\n",
    "    axes[1, i].imshow(masks[i].cpu(), cmap='jet', vmin=0, vmax=NUM_CLASSES-1)\n",
    "    axes[1, i].set_title('Ground Truth')\n",
    "    axes[2, i].imshow(pred_cls[i].cpu(), cmap='jet', vmin=0, vmax=NUM_CLASSES-1)\n",
    "    axes[2, i].set_title('Prediction')\n",
    "for ax in axes.flatten():\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
